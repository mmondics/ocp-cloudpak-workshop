{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop \u00b6 Welcome to the Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE workshop. Below you can find the workshop agenda, presentations, and lab documentation. Agenda \u00b6 Syntax Duration Presentation 1 30-45 minutes Short Break 10-15 minutes Presentation 2 ~ 1 hour Connect to environment as a group 10-15 minutes Hands-on, self-paced labs Remainder of day Note The lab environments will be available the day following the workshop. For example, If the workshop is on a Thursday, the environments will be available until 5PM EST Friday. Presentations \u00b6 Presentation 1 - High Level Overview of Red Hat OpenShift & IBM Cloud Paks on IBM Z Presentation 2 - Technical Deep Dive, Installation & Configuration, Lessons Learned Labs \u00b6 Note The labs are designed so that you can pick and choose you would like to complete. The labs are not designed for you to get through them all in one day. Labs are non-sequential and have no dependencies on one another. Lab 001 - Exploring the OpenShift Console Lab 002 - Using the OpenShift Command Line (oc) Lab 003 - Using the z/OS Cloud Broker with OCP Lab 004 - Deploying an Application from Source Code Lab 005 - Monitoring, Metering, and Metrics with OCP Lab 006 - Using Persistent Storage - MongoDB and NodeJS Lab 007 - Deploying an Application with the Open Liberty Operator Lab 008 - Deploying an Application with Quarkus Red Hat Runtime Lab 009 - Using OpenShift Pipelines Lab 010 - OpenShift Service Mesh Workshop Owners \u00b6 Matt Mondics Paul Novak","title":"Home"},{"location":"#red-hat-openshift-and-ibm-cloud-paks-on-ibm-z-and-linuxone-workshop","text":"Welcome to the Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE workshop. Below you can find the workshop agenda, presentations, and lab documentation.","title":"Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop"},{"location":"#agenda","text":"Syntax Duration Presentation 1 30-45 minutes Short Break 10-15 minutes Presentation 2 ~ 1 hour Connect to environment as a group 10-15 minutes Hands-on, self-paced labs Remainder of day Note The lab environments will be available the day following the workshop. For example, If the workshop is on a Thursday, the environments will be available until 5PM EST Friday.","title":"Agenda"},{"location":"#presentations","text":"Presentation 1 - High Level Overview of Red Hat OpenShift & IBM Cloud Paks on IBM Z Presentation 2 - Technical Deep Dive, Installation & Configuration, Lessons Learned","title":"Presentations"},{"location":"#labs","text":"Note The labs are designed so that you can pick and choose you would like to complete. The labs are not designed for you to get through them all in one day. Labs are non-sequential and have no dependencies on one another. Lab 001 - Exploring the OpenShift Console Lab 002 - Using the OpenShift Command Line (oc) Lab 003 - Using the z/OS Cloud Broker with OCP Lab 004 - Deploying an Application from Source Code Lab 005 - Monitoring, Metering, and Metrics with OCP Lab 006 - Using Persistent Storage - MongoDB and NodeJS Lab 007 - Deploying an Application with the Open Liberty Operator Lab 008 - Deploying an Application with Quarkus Red Hat Runtime Lab 009 - Using OpenShift Pipelines Lab 010 - OpenShift Service Mesh","title":"Labs"},{"location":"#workshop-owners","text":"Matt Mondics Paul Novak","title":"Workshop Owners"},{"location":"lab-assignments/","text":"Lab Assignments \u00b6 Name Team Number Skytap URL Skytap Password OpenShift Username OpenShift Password TBD 01 user01 p@ssw0rd TBD 02 user02 p@ssw0rd TBD 03 user03 p@ssw0rd TBD 04 user04 p@ssw0rd TBD 05 user05 p@ssw0rd TBD 06 user06 p@ssw0rd TBD 07 user07 p@ssw0rd TBD 08 user08 p@ssw0rd TBD 09 user09 p@ssw0rd TBD 10 user10 p@ssw0rd TBD 11 user11 p@ssw0rd TBD 12 user12 p@ssw0rd TBD 13 user13 p@ssw0rd TBD 14 user14 p@ssw0rd TBD 15 user15 p@ssw0rd TBD 16 user16 p@ssw0rd TBD 17 user17 p@ssw0rd TBD 18 user18 p@ssw0rd TBD 19 user19 p@ssw0rd TBD 20 user20 p@ssw0rd","title":"Lab Assignments"},{"location":"lab-assignments/#lab-assignments","text":"Name Team Number Skytap URL Skytap Password OpenShift Username OpenShift Password TBD 01 user01 p@ssw0rd TBD 02 user02 p@ssw0rd TBD 03 user03 p@ssw0rd TBD 04 user04 p@ssw0rd TBD 05 user05 p@ssw0rd TBD 06 user06 p@ssw0rd TBD 07 user07 p@ssw0rd TBD 08 user08 p@ssw0rd TBD 09 user09 p@ssw0rd TBD 10 user10 p@ssw0rd TBD 11 user11 p@ssw0rd TBD 12 user12 p@ssw0rd TBD 13 user13 p@ssw0rd TBD 14 user14 p@ssw0rd TBD 15 user15 p@ssw0rd TBD 16 user16 p@ssw0rd TBD 17 user17 p@ssw0rd TBD 18 user18 p@ssw0rd TBD 19 user19 p@ssw0rd TBD 20 user20 p@ssw0rd","title":"Lab Assignments"},{"location":"lab001-1/","text":"Lab 001 - Exploring the OpenShift Console \u00b6 Important Work In Progress","title":"Introduction"},{"location":"lab001-1/#lab-001-exploring-the-openshift-console","text":"Important Work In Progress","title":"Lab 001 - Exploring the OpenShift Console"},{"location":"lab001-2/","text":"Connect to OCP and Authenticate \u00b6 Important Work in Progress In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP"},{"location":"lab001-2/#connect-to-ocp-and-authenticate","text":"Important Work in Progress In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP and Authenticate"},{"location":"lab001-3/","text":"The Administrator Perspective \u00b6 Important Work in Progress Take a moment to notice the following elements in the navigation bar: Note These buttons display on each page of the OpenShift console. Note that the Applications button might be missing from your screen, depending on your credentials. By default, the menu on the left side of the page should be activated and displaying the cluster menu. In the left-side menu, select the Administrator perspective if it isn't already showing. With the administrator menu showing, you are provided with a broad range of options to manage the OpenShift cluster and the applications running on it. Expand to Learn More About the Different Views Developer / Administrator toggle . This lets you flip between which of the two perspectives you want to use. Home : Provides overview of projects, resources, and events in the scope of your credentials. Operators : Provides access to the OperatorHub to install new operators and also lets you view operators that are already installed. Workloads : Expands to provide access to many Kubernetes and OpenShift objects, such as pods, deployments, secrets, jobs and more. Networking : Provides access to services, routes, and ingresses required for external access to the cluster. Storage : Provides access to storage objects in the OpenShift cluster, such as PersistentVolumeClaims. Builds : View and create Build objects \u2013 use to transform input parameters into resulting objects. Pipelines : View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. This will be missing if not installed in your OpenShift cluster. Monitoring : Access cluster resource Monitoring, Metrics, and Alerting. Compute : Access cluster infrastructure \u2013 Control & Compute Nodes, Machines, and more. User Management : Access and manage Users, Groups, Roles, RoleBindings, Service Accounts, and more. Administration : View and edit cluster settings. The Administrator perspective is the default view for the OpenShift console for users who have an administrative access level. This perspective provides visibility into options related to cluster administration, as well as a broader view of the projects associated with the currently logged-in user. In the Menu, click Home -> Projects . The rest of the page is populated by projects. A project has been created for you to work in named userNN-project (where NN is your user number). Note Any project starting with openshift- or kube- contain the workloads running the OpenShift platform itself. Click the userNN-project hyperlink (where NN is your user number). Tip With so many Projects displayed, you can use the search bar to find yours more easily. You will now see the Dashboard for your project. Scroll down the Overview tab of your project . This displays information about what\u2019s going on in your project, such as CPU and memory usage, any alerts or crashlooping pods, an inventory of all the Kubernetes resources deployed in the project, and more. You won\u2019t see much information yet, as no workloads should be running in this project. Click the Workloads tab to the right of YAML. This page displays all of the workloads in your project, so it\u2019s empty for now. Note All objects in OpenShift are generated using YAML files. YAML (standing for Yet Another Markup Language) is meant to be a human-readable language for configuration files. Any OpenShift object such as Deployments, Services, Routes, and nearly everything else can be modified by directly editing their YAML file in either the console or command line. Workloads are typically created by developers, so in the next section, you will swap to the developer perspective to deploy a an application. You will return to the administrator perspective later in this lab.","title":"The Administrator Persepctive"},{"location":"lab001-3/#the-administrator-perspective","text":"Important Work in Progress Take a moment to notice the following elements in the navigation bar: Note These buttons display on each page of the OpenShift console. Note that the Applications button might be missing from your screen, depending on your credentials. By default, the menu on the left side of the page should be activated and displaying the cluster menu. In the left-side menu, select the Administrator perspective if it isn't already showing. With the administrator menu showing, you are provided with a broad range of options to manage the OpenShift cluster and the applications running on it. Expand to Learn More About the Different Views Developer / Administrator toggle . This lets you flip between which of the two perspectives you want to use. Home : Provides overview of projects, resources, and events in the scope of your credentials. Operators : Provides access to the OperatorHub to install new operators and also lets you view operators that are already installed. Workloads : Expands to provide access to many Kubernetes and OpenShift objects, such as pods, deployments, secrets, jobs and more. Networking : Provides access to services, routes, and ingresses required for external access to the cluster. Storage : Provides access to storage objects in the OpenShift cluster, such as PersistentVolumeClaims. Builds : View and create Build objects \u2013 use to transform input parameters into resulting objects. Pipelines : View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. This will be missing if not installed in your OpenShift cluster. Monitoring : Access cluster resource Monitoring, Metrics, and Alerting. Compute : Access cluster infrastructure \u2013 Control & Compute Nodes, Machines, and more. User Management : Access and manage Users, Groups, Roles, RoleBindings, Service Accounts, and more. Administration : View and edit cluster settings. The Administrator perspective is the default view for the OpenShift console for users who have an administrative access level. This perspective provides visibility into options related to cluster administration, as well as a broader view of the projects associated with the currently logged-in user. In the Menu, click Home -> Projects . The rest of the page is populated by projects. A project has been created for you to work in named userNN-project (where NN is your user number). Note Any project starting with openshift- or kube- contain the workloads running the OpenShift platform itself. Click the userNN-project hyperlink (where NN is your user number). Tip With so many Projects displayed, you can use the search bar to find yours more easily. You will now see the Dashboard for your project. Scroll down the Overview tab of your project . This displays information about what\u2019s going on in your project, such as CPU and memory usage, any alerts or crashlooping pods, an inventory of all the Kubernetes resources deployed in the project, and more. You won\u2019t see much information yet, as no workloads should be running in this project. Click the Workloads tab to the right of YAML. This page displays all of the workloads in your project, so it\u2019s empty for now. Note All objects in OpenShift are generated using YAML files. YAML (standing for Yet Another Markup Language) is meant to be a human-readable language for configuration files. Any OpenShift object such as Deployments, Services, Routes, and nearly everything else can be modified by directly editing their YAML file in either the console or command line. Workloads are typically created by developers, so in the next section, you will swap to the developer perspective to deploy a an application. You will return to the administrator perspective later in this lab.","title":"The Administrator Perspective"},{"location":"lab001-4/","text":"The Developer Perspective \u00b6 Important Work in Progress In the left-side Menu, click the Administrator dropdown, and select Developer . The Developer perspective provides views and workflows specific to developer use cases, while hiding many of the cluster management options typically used by administrators. This perspective provides developers with a streamlined view of the options they typically use. Expand to Learn More About the Different Views +Add : Clicking on this will open a prompt letting you add a workload to your current project. Topology : Displays all of the deployed workloads in the currently selected project. Monitoring : Lets you view the monitoring dashboard for just this project. Search : Used to search for any type of API resource present in this project, provided you have access to that resource type. Builds : This will let you view or create Build Configurations in the currently selected project. Pipelines : View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. Helm : Displays the Helm releases in this project, or prompts you to install one from the catalog if none are present. Project : Takes you to your project overview page, the project inventory, events, utilization, and more. Config Maps : Displays Config Maps for your project, which store non-confidential data in key-value pairs. Secrets : Displays Secrets for your project. Used to store sensitive, confidential data in key-value pairs, tokens, or passwords. Switching to the Developer perspective takes you to the Topology view. If no workloads are deployed in the selected project, options to deploy a workload are displayed. If this isn't the case, click the +Add button in the menu . Expand to learn about Deployment Methods There are multiple methods of deploying workloads from the OpenShift web browser. Samples : Red Hat provides sample applications in various languages. Use these to see what a pre-made application running in OpenShift can look like. From Git : Use this option to import an existing codebase in a Git repository to create, build, and deploy an application. From Devfile : Similar to From Git, use this option to import a Devfile from your Git repository to build and deploy an application. Container Image : Use existing images from an image stream or registry to deploy it. From Catalog : Explore the Developer Catalog to select the required applications, services, or source to image builders and add it to your project. From Dockerfile : Import a dockerfile from your Git repository to build and deploy an application. YAML : Use the editor to add YAML or JSON definitions to create and modify resources. Database : Filters the Developer Catalog to display only the databases it contains. Operator Backed : Deploy applications that are managed by Operators. Many of these will come from the OperatorHub. Helm Chart : Deploy applications defined by Helm Charts, which provide simple installations, upgrades, rollbacks, and generally reduced complexity. Pipeline : Create a Tekton-based Pipeline to automate application creation and delivery using OpenShift\u2019s built-in CI/CD capabilities. In the next section, you will deploy an application from the OpenShift Developer Catalog.","title":"The Developer Perspective"},{"location":"lab001-4/#the-developer-perspective","text":"Important Work in Progress In the left-side Menu, click the Administrator dropdown, and select Developer . The Developer perspective provides views and workflows specific to developer use cases, while hiding many of the cluster management options typically used by administrators. This perspective provides developers with a streamlined view of the options they typically use. Expand to Learn More About the Different Views +Add : Clicking on this will open a prompt letting you add a workload to your current project. Topology : Displays all of the deployed workloads in the currently selected project. Monitoring : Lets you view the monitoring dashboard for just this project. Search : Used to search for any type of API resource present in this project, provided you have access to that resource type. Builds : This will let you view or create Build Configurations in the currently selected project. Pipelines : View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. Helm : Displays the Helm releases in this project, or prompts you to install one from the catalog if none are present. Project : Takes you to your project overview page, the project inventory, events, utilization, and more. Config Maps : Displays Config Maps for your project, which store non-confidential data in key-value pairs. Secrets : Displays Secrets for your project. Used to store sensitive, confidential data in key-value pairs, tokens, or passwords. Switching to the Developer perspective takes you to the Topology view. If no workloads are deployed in the selected project, options to deploy a workload are displayed. If this isn't the case, click the +Add button in the menu . Expand to learn about Deployment Methods There are multiple methods of deploying workloads from the OpenShift web browser. Samples : Red Hat provides sample applications in various languages. Use these to see what a pre-made application running in OpenShift can look like. From Git : Use this option to import an existing codebase in a Git repository to create, build, and deploy an application. From Devfile : Similar to From Git, use this option to import a Devfile from your Git repository to build and deploy an application. Container Image : Use existing images from an image stream or registry to deploy it. From Catalog : Explore the Developer Catalog to select the required applications, services, or source to image builders and add it to your project. From Dockerfile : Import a dockerfile from your Git repository to build and deploy an application. YAML : Use the editor to add YAML or JSON definitions to create and modify resources. Database : Filters the Developer Catalog to display only the databases it contains. Operator Backed : Deploy applications that are managed by Operators. Many of these will come from the OperatorHub. Helm Chart : Deploy applications defined by Helm Charts, which provide simple installations, upgrades, rollbacks, and generally reduced complexity. Pipeline : Create a Tekton-based Pipeline to automate application creation and delivery using OpenShift\u2019s built-in CI/CD capabilities. In the next section, you will deploy an application from the OpenShift Developer Catalog.","title":"The Developer Perspective"},{"location":"lab001-5/","text":"Deploy from the Developer Catalog \u00b6 Important Work in Progress In this section, you will be building a sample application from a template. The template will create two pods: A Ruby on Rails blogging application from source code in GitHub A PostgreSQL database from a container image Info A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the OpenShift internal registry, the Red Hat registry, Docker Hub, or a private registry of your own. Click the From Catalog option from the Add page. This brings up the OpenShift Developer catalog containing all types of applications you can deploy including Operators, Helm Charts, Templates, and more. Find and click the Rails + PostgreSQL (Ephemeral) tile . Tip You can make this easier on yourself by searching for Rails + PostgreSQL (Ephemeral ) in the search bar. Click Instantiate Template on the next screen that appears. You are brought to a page full of configurable parameters that you can edit if so desired. Notice that all of the required fields on this page automatically populate. You can read through all of the options, but there is no need to edit any of them. Click the Create button at the bottom of the page. You will now be taken to the topology view, where you will see two icons \u2013 one for each of the two workload pods that the template will create. If you don\u2019t see the icons right away, you may need to refresh your browser window. ??+ Info The Ruby on Rails application will take a few minutes to fully deploy, while the PostgreSQL application will deploy in just a few seconds. The reason for this difference is that the Ruby application is being built (containerized) from Ruby source code located in the GitHub repository located here: https://github.com/sclorg/rails-ex.git into a container image, and then deployed. If you would like to watch the steps that OpenShift is taking to build the containerized application, click the circle labeled rails-postgresql-example, click the Resources tab, and click View Logs in the Builds section . The PostgreSQL application, on the other hand, is deployed from a pre-built container image hosted in quay.io, so it takes much less time to start up. You will know that both applications are successfully deployed and running when each has a solid blue circle. Click the icon for the rails-postgresql-example application . This will bring up a window on the right side of the screen with information about your DeploymentConfig. Click the Details tab if it is not already selected. Here you\u2019ll see information about your DeploymentConfig. Notice that many of the fields such as Labels, Update Strategy, and more have been populated with default values. These can be modified. Click the Actions dropdown . Many application configurations can be modified from this menu, along with other tasks such as starting or pausing a rollout, or deleting the deployment configuration. Click the up arrow next to the blue circle. This scales your application from one pod to two pods. Note This is a simple demonstration of horizontal scaling with Kubernetes. You now have two instances of your pod running in the OpenShift cluster. Traffic to the Rails application will now be distributed to each pod, and if for some reason a pod is lost, that traffic will be redistributed to the remaining pods until a Kubernetes starts another. If a whole compute node is lost, Kubernetes will move the pods to different compute nodes. OpenShift and Kubernetes also support autoscaling of pods based on CPU or memory consumption, but that is outside the scope of this lab. Click the Resources tab . Notice the two pods associated with your Rails application. On this page, you\u2019ll see more information about your pods, any build configurations currently running or completed, and the services/ports associated with the pod. Click the route address a the bottom of the resources tab . Expand for a Tip You could also access this route by clicking on the external link icon associated with your Rails pod on the Topology view. If you see the page above, your Rails application is up and running. You just deployed a Ruby on Rails application from source code residing in GitHub, and connected it to a PostgreSQL container deployed from a container image pulled from quay.io into OpenShift running on an IBM Z server. Feel free to read through the Rails application homepage to learn more about what this application can do. Add /articles to the end of the Rails homepage URL . This will result in a URL like the following: http://rails-postgresql-example-userNN-project.apps.atsocppa.dmz/articles Where NN is your user number. You are now interacting with the blogging application that\u2019s shipped with the Rails source code. If you create a new article, the contents for the Title and Body are stored in the PostgreSQL database in the other pod that makes up this application. In the next section you will navigate back to the Administrator perspective to see the overview of your project with a workload running.","title":"Deploy from the Developer Catalog"},{"location":"lab001-5/#deploy-from-the-developer-catalog","text":"Important Work in Progress In this section, you will be building a sample application from a template. The template will create two pods: A Ruby on Rails blogging application from source code in GitHub A PostgreSQL database from a container image Info A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the OpenShift internal registry, the Red Hat registry, Docker Hub, or a private registry of your own. Click the From Catalog option from the Add page. This brings up the OpenShift Developer catalog containing all types of applications you can deploy including Operators, Helm Charts, Templates, and more. Find and click the Rails + PostgreSQL (Ephemeral) tile . Tip You can make this easier on yourself by searching for Rails + PostgreSQL (Ephemeral ) in the search bar. Click Instantiate Template on the next screen that appears. You are brought to a page full of configurable parameters that you can edit if so desired. Notice that all of the required fields on this page automatically populate. You can read through all of the options, but there is no need to edit any of them. Click the Create button at the bottom of the page. You will now be taken to the topology view, where you will see two icons \u2013 one for each of the two workload pods that the template will create. If you don\u2019t see the icons right away, you may need to refresh your browser window. ??+ Info The Ruby on Rails application will take a few minutes to fully deploy, while the PostgreSQL application will deploy in just a few seconds. The reason for this difference is that the Ruby application is being built (containerized) from Ruby source code located in the GitHub repository located here: https://github.com/sclorg/rails-ex.git into a container image, and then deployed. If you would like to watch the steps that OpenShift is taking to build the containerized application, click the circle labeled rails-postgresql-example, click the Resources tab, and click View Logs in the Builds section . The PostgreSQL application, on the other hand, is deployed from a pre-built container image hosted in quay.io, so it takes much less time to start up. You will know that both applications are successfully deployed and running when each has a solid blue circle. Click the icon for the rails-postgresql-example application . This will bring up a window on the right side of the screen with information about your DeploymentConfig. Click the Details tab if it is not already selected. Here you\u2019ll see information about your DeploymentConfig. Notice that many of the fields such as Labels, Update Strategy, and more have been populated with default values. These can be modified. Click the Actions dropdown . Many application configurations can be modified from this menu, along with other tasks such as starting or pausing a rollout, or deleting the deployment configuration. Click the up arrow next to the blue circle. This scales your application from one pod to two pods. Note This is a simple demonstration of horizontal scaling with Kubernetes. You now have two instances of your pod running in the OpenShift cluster. Traffic to the Rails application will now be distributed to each pod, and if for some reason a pod is lost, that traffic will be redistributed to the remaining pods until a Kubernetes starts another. If a whole compute node is lost, Kubernetes will move the pods to different compute nodes. OpenShift and Kubernetes also support autoscaling of pods based on CPU or memory consumption, but that is outside the scope of this lab. Click the Resources tab . Notice the two pods associated with your Rails application. On this page, you\u2019ll see more information about your pods, any build configurations currently running or completed, and the services/ports associated with the pod. Click the route address a the bottom of the resources tab . Expand for a Tip You could also access this route by clicking on the external link icon associated with your Rails pod on the Topology view. If you see the page above, your Rails application is up and running. You just deployed a Ruby on Rails application from source code residing in GitHub, and connected it to a PostgreSQL container deployed from a container image pulled from quay.io into OpenShift running on an IBM Z server. Feel free to read through the Rails application homepage to learn more about what this application can do. Add /articles to the end of the Rails homepage URL . This will result in a URL like the following: http://rails-postgresql-example-userNN-project.apps.atsocppa.dmz/articles Where NN is your user number. You are now interacting with the blogging application that\u2019s shipped with the Rails source code. If you create a new article, the contents for the Title and Body are stored in the PostgreSQL database in the other pod that makes up this application. In the next section you will navigate back to the Administrator perspective to see the overview of your project with a workload running.","title":"Deploy from the Developer Catalog"},{"location":"lab001-6/","text":"View Workload from the Administrator Perspective \u00b6 Important Work in Progress In the left-side menu, select the Administrator perspective . Navigate back to your project by clicking Menu -> Home -> Projects -> userNN-project The overview page now displays data about the CPU and Memory Usage, new objects in your project inventory, and new activity in the events panel. Click View Events under the right-side panel. This page is populated with all of the events associated with your project, including errors, container creation messages, pod scaling and deletion, and much more. You can filter by type, category, or by searching for keywords. Note Feel free to click through a few more pages from the left-side main menu. You\u2019ll notice a few of them have objects created as a part of the Rails-PostgreSQL application, such as Workloads \uf0e0 Pods, Networking \uf0e0 Services and Routes, Builds \uf0e0 Image Streams. These were all created as part of the template package. Navigate back your project as in the previous step (or by clicking your browser\u2019s back button). Find the Inventory on the project page which lists all of the objects created as part of your application","title":"View Workload from the Administrator Perspective"},{"location":"lab001-6/#view-workload-from-the-administrator-perspective","text":"Important Work in Progress In the left-side menu, select the Administrator perspective . Navigate back to your project by clicking Menu -> Home -> Projects -> userNN-project The overview page now displays data about the CPU and Memory Usage, new objects in your project inventory, and new activity in the events panel. Click View Events under the right-side panel. This page is populated with all of the events associated with your project, including errors, container creation messages, pod scaling and deletion, and much more. You can filter by type, category, or by searching for keywords. Note Feel free to click through a few more pages from the left-side main menu. You\u2019ll notice a few of them have objects created as a part of the Rails-PostgreSQL application, such as Workloads \uf0e0 Pods, Networking \uf0e0 Services and Routes, Builds \uf0e0 Image Streams. These were all created as part of the template package. Navigate back your project as in the previous step (or by clicking your browser\u2019s back button). Find the Inventory on the project page which lists all of the objects created as part of your application","title":"View Workload from the Administrator Perspective"},{"location":"lab001-7/","text":"Cleaning Up \u00b6 Important Work in Progress Navigate back your project as in the previous section (or by clicking your browser\u2019s back button). Find the Inventory on the project page which lists all of the objects created as part of your application Click the Deployment Configs hyperlink . For both of the 2 Deployment Configs that appear click the three dots on the right side of the screen, and then click Delete Deployment Config. This will delete some, but not all of the resources created by the application template. The running pods will be stopped and deleted, but some other components will remain. This is not a problem in the case of our labs.","title":"Cleaning Up"},{"location":"lab001-7/#cleaning-up","text":"Important Work in Progress Navigate back your project as in the previous section (or by clicking your browser\u2019s back button). Find the Inventory on the project page which lists all of the objects created as part of your application Click the Deployment Configs hyperlink . For both of the 2 Deployment Configs that appear click the three dots on the right side of the screen, and then click Delete Deployment Config. This will delete some, but not all of the resources created by the application template. The running pods will be stopped and deleted, but some other components will remain. This is not a problem in the case of our labs.","title":"Cleaning Up"},{"location":"lab002-1/","text":"Lab 002 - Using the OpenShift Command Line (oc) \u00b6 Important Work In Progress","title":"Introduction"},{"location":"lab002-1/#lab-002-using-the-openshift-command-line-oc","text":"Important Work In Progress","title":"Lab 002 - Using the OpenShift Command Line (oc)"},{"location":"lab002-2/","text":"Log into OpenShift Using the CLI \u00b6 Important Work In Progress In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server: ssh userNN@192.168.176.61 (where NN is your user number). When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab002-2/#log-into-openshift-using-the-cli","text":"Important Work In Progress In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server: ssh userNN@192.168.176.61 (where NN is your user number). When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab002-3/","text":"Overview of the OpenShift CLI \u00b6 Important Work In Progress The OpenShift command line oc is a command line tool that can be used to create applications and manage OpenShift projects. oc is ideal in situations where you: Work directly with project source code. Script OpenShift Container Platform operations. Are restricted by bandwidth resources and cannot use the web console. Furthermore, many people familiar with Linux and/or Kubernetes tend to find the oc command line an easier and more efficient method of performing tasks, rather than the web-based console. In your terminal, enter the command: oc --help Example Output user01@lab061:~$ oc --help OpenShift Client This client helps you develop, build, deploy, and run your applications on any OpenShift or Kubernetes cluster. It also includes the administrative commands for managing a cluster under the 'adm' subcommand. Usage: oc [flags] Basic Commands: login Log in to a server new-project Request a new project new-app Create a new application status Show an overview of the current project project Switch to another project projects Display existing projects explain Documentation of resources Build and Deploy Commands: rollout Manage a Kubernetes deployment or OpenShift deployment config rollback Revert part of an application back to a previous deployment new-build Create a new build configuration start-build Start a new build The --help flag will display all of the available options the oc CLI. Enter the command oc new-app --help Example Output user01@lab061:~$ oc new-app --help Create a new application by specifying source code, templates, and/or images This command will try to build up the components of an application using images, templates, or code that has a public repository. It will lookup the images on the local Docker installation (if available), a container image registry, an integrated image stream, or stored templates. If you specify a source code URL, it will set up a build that takes your source code and converts it into an image that can run inside of a pod. Local source must be in a git repository that has a remote repository that the server can see. The images will be deployed via a deployment configuration, and a service will be connected to the first public port of the app. You may either specify components using the various existing flags or let new-app autodetect what kind of components you have provided. If you provide source code, a new build will be automatically triggered. You can use 'oc status' to check the progress. Usage: oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [flags] Examples: # List all local templates and image streams that can be used to create an app oc new-app --list # Create an application based on the source code in the current git repository (with a public remote) and a Docker image oc new-app . --docker-image=repo/langimage The --help flag now displays all of the available options for the oc new-app command. If you get confused about any of the commands we use in this workshop, or just want more information, using this flag is a good first step.","title":"Overview of the OpenShift CLI"},{"location":"lab002-3/#overview-of-the-openshift-cli","text":"Important Work In Progress The OpenShift command line oc is a command line tool that can be used to create applications and manage OpenShift projects. oc is ideal in situations where you: Work directly with project source code. Script OpenShift Container Platform operations. Are restricted by bandwidth resources and cannot use the web console. Furthermore, many people familiar with Linux and/or Kubernetes tend to find the oc command line an easier and more efficient method of performing tasks, rather than the web-based console. In your terminal, enter the command: oc --help Example Output user01@lab061:~$ oc --help OpenShift Client This client helps you develop, build, deploy, and run your applications on any OpenShift or Kubernetes cluster. It also includes the administrative commands for managing a cluster under the 'adm' subcommand. Usage: oc [flags] Basic Commands: login Log in to a server new-project Request a new project new-app Create a new application status Show an overview of the current project project Switch to another project projects Display existing projects explain Documentation of resources Build and Deploy Commands: rollout Manage a Kubernetes deployment or OpenShift deployment config rollback Revert part of an application back to a previous deployment new-build Create a new build configuration start-build Start a new build The --help flag will display all of the available options the oc CLI. Enter the command oc new-app --help Example Output user01@lab061:~$ oc new-app --help Create a new application by specifying source code, templates, and/or images This command will try to build up the components of an application using images, templates, or code that has a public repository. It will lookup the images on the local Docker installation (if available), a container image registry, an integrated image stream, or stored templates. If you specify a source code URL, it will set up a build that takes your source code and converts it into an image that can run inside of a pod. Local source must be in a git repository that has a remote repository that the server can see. The images will be deployed via a deployment configuration, and a service will be connected to the first public port of the app. You may either specify components using the various existing flags or let new-app autodetect what kind of components you have provided. If you provide source code, a new build will be automatically triggered. You can use 'oc status' to check the progress. Usage: oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [flags] Examples: # List all local templates and image streams that can be used to create an app oc new-app --list # Create an application based on the source code in the current git repository (with a public remote) and a Docker image oc new-app . --docker-image=repo/langimage The --help flag now displays all of the available options for the oc new-app command. If you get confused about any of the commands we use in this workshop, or just want more information, using this flag is a good first step.","title":"Overview of the OpenShift CLI"},{"location":"lab002-4/","text":"Deploy Container Image from the CLI \u00b6 Important Work In Progress oc new-app is a powerful and commonly used command in the OpenShift CLI. It has the ability to deploy applications from components that include: Source or binary code Container images Templates The set of objects created by oc new-app depends on the artifacts passed as an input. Run the following command to start a MongoDB deployment from a template: oc new-app --template=mongodb-ephemeral Example Output user01@lab061:~$ oc new-app --template=mongodb-ephemeral --> Deploying template \"openshift/mongodb-ephemeral\" to project user01-project MongoDB (Ephemeral) --------- MongoDB database service, without persistent storage. For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container. WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing The following service(s) have been created in your project: mongodb. Username: userFUX Password: AXGgm5dnKY44Byuk Database Name: sampledb Connection URL: mongodb://userFUX:AXGgm5dnKY44Byuk@mongodb/sampledb For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container. * With parameters: * Memory Limit=512Mi * Namespace=openshift * Database Service Name=mongodb * MongoDB Connection Username=userFUX # generated * MongoDB Connection Password=AXGgm5dnKY44Byuk # generated * MongoDB Database Name=sampledb * MongoDB Admin Password=JibwnlSwiow18owJ # generated * Version of MongoDB Image=3.6 --> Creating resources ... secret \"mongodb\" created service \"mongodb\" created deploymentconfig.apps.openshift.io \"mongodb\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/mongodb' Run 'oc status' to view your app. Note Notice a few things: OpenShift went out and found a template that matches your desired deployment \u2013 MongoDB-ephemeral. You\u2019re told what exactly is going to be created and what it will be named. Those objects are then created within your project space. You\u2019re told that the application was successfully deployed, but it is not yet exposed. This means that it\u2019s running, but it\u2019s not accessible from outside the cluster. Run the following command to view the app in your project space: oc status Example Output user01@lab061:~$ oc status In project user01-project on server https://api.atsocppa.dmz:6443 svc/mongodb - 172.30.94.118:27017 dc/mongodb deploys istag/mongodb:latest deployment #1 deployed 3 minutes ago - 1 pod View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'. Now run the following command to see all of the objects that were built: oc get all Example Output user01@lab061:~$ oc get all NAME READY STATUS RESTARTS AGE pod/mongodb-1-deploy 0/1 Completed 0 5m30s pod/mongodb-1-sj6mk 1/1 Running 0 5m22s NAME DESIRED CURRENT READY AGE replicationcontroller/mongodb-1 1 1 1 5m30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mongodb ClusterIP 172.30.94.118 <none> 27017/TCP 5m32s NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/mongodb 1 1 1 config,image(mongodb:3.6) These are the objects that OpenShift told us would be created, and they all work together to run the application. While they\u2019re all important pieces of this puzzle, pods are where the application code is actually running. Let\u2019s narrow down on our pods. Note You might also have objects left over from other labs if they were not completely cleaned out. This is okay and the objects for different applications will not interfere with one another due to their use of labels . Run the command oc get pods Example Output user01@lab061:~$ oc get pods NAME READY STATUS RESTARTS AGE mongodb-1-deploy 0/1 Completed 0 28s mongodb-1-r8dpw 1/1 Running 0 19s The oc new-app command created two pods. One ending with \u201cdeploy\u201d, and the other ending with a randomly-generated string of 5 characters (r8dpw in the screenshot above). They are both associated with your mongo deployment, but one is in a Completed status, and one is Running . The Completed pod had one simple job \u2013 scale the other pod to its desired count of 1. Run the following command to see the logs for the deploy pod oc logs pod/mongodb-1-deploy Example Output user01@lab061:~$ oc logs pod/mongodb-1-deploy --> Scaling mongodb-1 to 1 --> Success That\u2019s a pretty simple responsibility. The second pod, ending in the randomly generated string of characters, has a much more complicated job. This is the pod where the MongoDB application code is actually running. Run the following command to see the logs for the MongoDB deployment: oc logs pod/mongodb-1-XXXXX where XXXXX is your unique string of characters that you saw in the oc get pods output. Example Output user01@lab061:~$ oc logs pod/mongodb-1-r8dpw 2020-04-15T16:56:12.344+0000 I CONTROL [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none' 2020-04-15T16:56:12.346+0000 W ASIO [main] No TransportLayer configured during NetworkInterface startup 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=mongo-1-r8dpw 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] db version v4.2.5 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] git version: 2261279b51ea13df08ae708ff278f0679c59dc32 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.1.1 11 Sep 2018 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] allocator: tcmalloc 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] modules: none 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] build environment: 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] distmod: ubuntu1804 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] distarch: s390x 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] target_arch: s390x This is obviously a much busier pod. One of the first lines in its log tells you which version of MongoDB is running. In the next section, we will connect to the pod and see that it is actually running MongoDB.","title":"Deploy Container Image from the CLI"},{"location":"lab002-4/#deploy-container-image-from-the-cli","text":"Important Work In Progress oc new-app is a powerful and commonly used command in the OpenShift CLI. It has the ability to deploy applications from components that include: Source or binary code Container images Templates The set of objects created by oc new-app depends on the artifacts passed as an input. Run the following command to start a MongoDB deployment from a template: oc new-app --template=mongodb-ephemeral Example Output user01@lab061:~$ oc new-app --template=mongodb-ephemeral --> Deploying template \"openshift/mongodb-ephemeral\" to project user01-project MongoDB (Ephemeral) --------- MongoDB database service, without persistent storage. For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container. WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing The following service(s) have been created in your project: mongodb. Username: userFUX Password: AXGgm5dnKY44Byuk Database Name: sampledb Connection URL: mongodb://userFUX:AXGgm5dnKY44Byuk@mongodb/sampledb For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container. * With parameters: * Memory Limit=512Mi * Namespace=openshift * Database Service Name=mongodb * MongoDB Connection Username=userFUX # generated * MongoDB Connection Password=AXGgm5dnKY44Byuk # generated * MongoDB Database Name=sampledb * MongoDB Admin Password=JibwnlSwiow18owJ # generated * Version of MongoDB Image=3.6 --> Creating resources ... secret \"mongodb\" created service \"mongodb\" created deploymentconfig.apps.openshift.io \"mongodb\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/mongodb' Run 'oc status' to view your app. Note Notice a few things: OpenShift went out and found a template that matches your desired deployment \u2013 MongoDB-ephemeral. You\u2019re told what exactly is going to be created and what it will be named. Those objects are then created within your project space. You\u2019re told that the application was successfully deployed, but it is not yet exposed. This means that it\u2019s running, but it\u2019s not accessible from outside the cluster. Run the following command to view the app in your project space: oc status Example Output user01@lab061:~$ oc status In project user01-project on server https://api.atsocppa.dmz:6443 svc/mongodb - 172.30.94.118:27017 dc/mongodb deploys istag/mongodb:latest deployment #1 deployed 3 minutes ago - 1 pod View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'. Now run the following command to see all of the objects that were built: oc get all Example Output user01@lab061:~$ oc get all NAME READY STATUS RESTARTS AGE pod/mongodb-1-deploy 0/1 Completed 0 5m30s pod/mongodb-1-sj6mk 1/1 Running 0 5m22s NAME DESIRED CURRENT READY AGE replicationcontroller/mongodb-1 1 1 1 5m30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mongodb ClusterIP 172.30.94.118 <none> 27017/TCP 5m32s NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/mongodb 1 1 1 config,image(mongodb:3.6) These are the objects that OpenShift told us would be created, and they all work together to run the application. While they\u2019re all important pieces of this puzzle, pods are where the application code is actually running. Let\u2019s narrow down on our pods. Note You might also have objects left over from other labs if they were not completely cleaned out. This is okay and the objects for different applications will not interfere with one another due to their use of labels . Run the command oc get pods Example Output user01@lab061:~$ oc get pods NAME READY STATUS RESTARTS AGE mongodb-1-deploy 0/1 Completed 0 28s mongodb-1-r8dpw 1/1 Running 0 19s The oc new-app command created two pods. One ending with \u201cdeploy\u201d, and the other ending with a randomly-generated string of 5 characters (r8dpw in the screenshot above). They are both associated with your mongo deployment, but one is in a Completed status, and one is Running . The Completed pod had one simple job \u2013 scale the other pod to its desired count of 1. Run the following command to see the logs for the deploy pod oc logs pod/mongodb-1-deploy Example Output user01@lab061:~$ oc logs pod/mongodb-1-deploy --> Scaling mongodb-1 to 1 --> Success That\u2019s a pretty simple responsibility. The second pod, ending in the randomly generated string of characters, has a much more complicated job. This is the pod where the MongoDB application code is actually running. Run the following command to see the logs for the MongoDB deployment: oc logs pod/mongodb-1-XXXXX where XXXXX is your unique string of characters that you saw in the oc get pods output. Example Output user01@lab061:~$ oc logs pod/mongodb-1-r8dpw 2020-04-15T16:56:12.344+0000 I CONTROL [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none' 2020-04-15T16:56:12.346+0000 W ASIO [main] No TransportLayer configured during NetworkInterface startup 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=mongo-1-r8dpw 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] db version v4.2.5 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] git version: 2261279b51ea13df08ae708ff278f0679c59dc32 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.1.1 11 Sep 2018 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] allocator: tcmalloc 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] modules: none 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] build environment: 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] distmod: ubuntu1804 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] distarch: s390x 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] target_arch: s390x This is obviously a much busier pod. One of the first lines in its log tells you which version of MongoDB is running. In the next section, we will connect to the pod and see that it is actually running MongoDB.","title":"Deploy Container Image from the CLI"},{"location":"lab002-5/","text":"Open a Remote Shell Session into the MongoDB Pod \u00b6 Important Work In Progress OpenShift provides Remote Shell capabilities from both the command line and from the web console. With the oc rsh command, you can issue commands as if you are inside the container and perform local operations like monitoring, debugging, and using CLI commands specific to what is running in the container. Information For example, if you open a remote shell session into a MySQL container, you can count the number of records in the database by invoking the mysql command, then using the prompt to type in the SELECT command. You can also use commands like ps(1) and ls(1) for validation. With the MongoDB application you deployed, you can rsh into the MongoDB pod to run mongo CLI commands. Enter the following command to rsh into the container: oc rsh mongodb-1-XXXXX where XXXXX is your unique string of 5 characters Example Output user01@lab061:~$ oc rsh mongodb-1-r8dpw $ This new line that does not start with userNN@lab061 indicates that you are now in the remote shell session for the pod Important If you wait too long to interact with the remote shell (about a minute), it will automatically time-out and you will have to re-connect. You can tell that this happened if the prompt reappears. In the remote session, issue the command mongo Example Output $ mongo MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"2320e01b-168e-41d0-a132-af0c9243d29c\") } MongoDB server version: 4.2.5 Welcome to the MongoDB shell. For interactive help, type \"help\". For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user mongo is the shell command for MongoDB. Issuing the mongo command without any options or flags connects you to a MongoDB instance running on your localhost with port 27017. If you see this message, MongoDB is up and running in the container. Exit the MongoDB shell by entering the command exit Exit the remote shell session by entering exit once again. You should be back in the userNN@lab061 command line.","title":"Open a Remote Shell Session into the MongoDB Pod"},{"location":"lab002-5/#open-a-remote-shell-session-into-the-mongodb-pod","text":"Important Work In Progress OpenShift provides Remote Shell capabilities from both the command line and from the web console. With the oc rsh command, you can issue commands as if you are inside the container and perform local operations like monitoring, debugging, and using CLI commands specific to what is running in the container. Information For example, if you open a remote shell session into a MySQL container, you can count the number of records in the database by invoking the mysql command, then using the prompt to type in the SELECT command. You can also use commands like ps(1) and ls(1) for validation. With the MongoDB application you deployed, you can rsh into the MongoDB pod to run mongo CLI commands. Enter the following command to rsh into the container: oc rsh mongodb-1-XXXXX where XXXXX is your unique string of 5 characters Example Output user01@lab061:~$ oc rsh mongodb-1-r8dpw $ This new line that does not start with userNN@lab061 indicates that you are now in the remote shell session for the pod Important If you wait too long to interact with the remote shell (about a minute), it will automatically time-out and you will have to re-connect. You can tell that this happened if the prompt reappears. In the remote session, issue the command mongo Example Output $ mongo MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"2320e01b-168e-41d0-a132-af0c9243d29c\") } MongoDB server version: 4.2.5 Welcome to the MongoDB shell. For interactive help, type \"help\". For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user mongo is the shell command for MongoDB. Issuing the mongo command without any options or flags connects you to a MongoDB instance running on your localhost with port 27017. If you see this message, MongoDB is up and running in the container. Exit the MongoDB shell by entering the command exit Exit the remote shell session by entering exit once again. You should be back in the userNN@lab061 command line.","title":"Open a Remote Shell Session into the MongoDB Pod"},{"location":"lab002-6/","text":"Working with Pods \u00b6 Important Work In Progress One of the main benefits of using containers and Kubernetes-based cloud platforms like OpenShift is the ability to scale pods horizontally \u2013 rapidly duplicating or deleting pods to meet a desired state. Information One of the core concepts of Kubernetes is the Declarative State . Users declare what resources they want, and Kubernetes does whatever it can to make that happen. Scaling is one example of this. Scaling essentially creates copies of the application in order to distribute traffic to multiple instances and/or compute nodes for high availability and load balancing. Enter the command oc get dc to get the name of your MongoDB deploymentconfig (dc) Example Output user01@lab061:~$ oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY mongodb 1 1 1 config,image(mongodb:3.6) Your deploymentconfig named mongo has a count desired = current = 1. Scale the mongo deployment to 3 replicas : oc scale dc/mongodb --replicas=3 Example Output user01@lab061:~$ oc scale dc/mongodb --replicas=3 deploymentconfig.apps.openshift.io/mongodb scaled Enter oc get dc again to see the scaled application. Example Output user01@lab061:~$ oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY mongodb 1 3 3 config,image(mongodb:3.6) This output is telling you that OpenShift knows that you want three copies (pods) of MongoDB, and it is successfully meeting that declared state. Enter oc get pods again to see your three pods . Example Output user01@lab061:~$ oc get pods NAME READY STATUS RESTARTS AGE mongodb-1-5nmjn 1/1 Running 0 2m6s mongodb-1-deploy 0/1 Completed 0 20m mongodb-1-dh49x 1/1 Running 0 2m6s mongodb-1-r8dpw 1/1 Running 0 19m Two of the pods will have a shorter age than the original one \u2013 these are the two new pods that were just created when you scaled the application. Dig into the pods a little bit further by entering the following command : oc describe pod/mongodb-1-XXXXX where XXXXX is one of your unique strings of characters. Example Output user01@lab061:~$ oc describe pod/mongodb-1-5nmjn Name: mongodb-1-5nmjn Namespace: user01-project Priority: 0 PriorityClassName: <none> Node: worker-0.atsocppa.dmz/192.168.176.175 Start Time: Wed, 15 Apr 2020 13:13:53 -0400 Labels: app=mongodb deployment=mongodb-1 deploymentconfig=mongodb This command gives you all kinds of information about your pod. Notice the Node: field that begins with worker-#. Run the same command again, but on a different pod this time : oc describe pod/mongodb-1-YYYYY where YYYYY is one of your other unique strings of characters. Pick one different than the previous step. Example Output user01@lab061:~$ oc describe pod/mongodb-1-r8dpw Name: mongodb-1-r8dpw Namespace: user01-project Priority: 0 PriorityClassName: <none> Node: worker-2.atsocppa.dmz/192.168.176.177 Start Time: Wed, 15 Apr 2020 12:56:03 -0400 Labels: app=mongodb deployment=mongodb-1 deploymentconfig=mongodb Notice that this pod has been placed on a different compute node than the first pod you described. The reason for this is that you have three compute nodes in this OpenShift cluster, and Kubernetes has balanced the load for this application across multiple nodes.","title":"Working with Pods"},{"location":"lab002-6/#working-with-pods","text":"Important Work In Progress One of the main benefits of using containers and Kubernetes-based cloud platforms like OpenShift is the ability to scale pods horizontally \u2013 rapidly duplicating or deleting pods to meet a desired state. Information One of the core concepts of Kubernetes is the Declarative State . Users declare what resources they want, and Kubernetes does whatever it can to make that happen. Scaling is one example of this. Scaling essentially creates copies of the application in order to distribute traffic to multiple instances and/or compute nodes for high availability and load balancing. Enter the command oc get dc to get the name of your MongoDB deploymentconfig (dc) Example Output user01@lab061:~$ oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY mongodb 1 1 1 config,image(mongodb:3.6) Your deploymentconfig named mongo has a count desired = current = 1. Scale the mongo deployment to 3 replicas : oc scale dc/mongodb --replicas=3 Example Output user01@lab061:~$ oc scale dc/mongodb --replicas=3 deploymentconfig.apps.openshift.io/mongodb scaled Enter oc get dc again to see the scaled application. Example Output user01@lab061:~$ oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY mongodb 1 3 3 config,image(mongodb:3.6) This output is telling you that OpenShift knows that you want three copies (pods) of MongoDB, and it is successfully meeting that declared state. Enter oc get pods again to see your three pods . Example Output user01@lab061:~$ oc get pods NAME READY STATUS RESTARTS AGE mongodb-1-5nmjn 1/1 Running 0 2m6s mongodb-1-deploy 0/1 Completed 0 20m mongodb-1-dh49x 1/1 Running 0 2m6s mongodb-1-r8dpw 1/1 Running 0 19m Two of the pods will have a shorter age than the original one \u2013 these are the two new pods that were just created when you scaled the application. Dig into the pods a little bit further by entering the following command : oc describe pod/mongodb-1-XXXXX where XXXXX is one of your unique strings of characters. Example Output user01@lab061:~$ oc describe pod/mongodb-1-5nmjn Name: mongodb-1-5nmjn Namespace: user01-project Priority: 0 PriorityClassName: <none> Node: worker-0.atsocppa.dmz/192.168.176.175 Start Time: Wed, 15 Apr 2020 13:13:53 -0400 Labels: app=mongodb deployment=mongodb-1 deploymentconfig=mongodb This command gives you all kinds of information about your pod. Notice the Node: field that begins with worker-#. Run the same command again, but on a different pod this time : oc describe pod/mongodb-1-YYYYY where YYYYY is one of your other unique strings of characters. Pick one different than the previous step. Example Output user01@lab061:~$ oc describe pod/mongodb-1-r8dpw Name: mongodb-1-r8dpw Namespace: user01-project Priority: 0 PriorityClassName: <none> Node: worker-2.atsocppa.dmz/192.168.176.177 Start Time: Wed, 15 Apr 2020 12:56:03 -0400 Labels: app=mongodb deployment=mongodb-1 deploymentconfig=mongodb Notice that this pod has been placed on a different compute node than the first pod you described. The reason for this is that you have three compute nodes in this OpenShift cluster, and Kubernetes has balanced the load for this application across multiple nodes.","title":"Working with Pods"},{"location":"lab002-7/","text":"Administrative CLI Commands \u00b6 Important Work in Progress If you\u2019ve already completed Lab 001 \u2013 Exploring the OpenShift Console , you\u2019ll remember that there are both developer and administrator perspectives. The same is true in the OpenShift CLI. The oc adm command gives cluster administrators the ability to check logs, manage users, groups, policies, certificates, and many other tasks usually associated with administrative roles. Issue the oc adm --help command to see all of the OpenShift administrator commands. Example Output user01@lab061:~$ oc adm --help Administrative Commands Actions for administering an OpenShift cluster are exposed here. Usage: oc adm [flags] Cluster Management: upgrade Upgrade a cluster top Show usage statistics of resources on the server must-gather Launch a new instance of a pod for gathering debug information Node Management: drain Drain node in preparation for maintenance cordon Mark node as unschedulable uncordon Mark node as schedulable taint Update the taints on one or more nodes node-logs Display and filter node logs Security and Policy: new-project Create a new project policy Manage cluster authorization and security policy groups Manage groups certificate Approve or reject certificate requests pod-network Manage pod network Maintenance: prune Remove older versions of resources from the server migrate Migrate data in the cluster Configuration: create-kubeconfig Create a basic .kubeconfig file from client certs create-bootstrap-project-template Create a bootstrap project template create-login-template Create a login template create-provider-selection-template Create a provider selection template create-error-template Create an error page template Other Commands: build-chain Output the inputs and dependencies of your builds completion Output shell completion code for the specified shell (bash or zsh) config Change configuration files for the client verify-image-signature Verify the image identity contained in the image signature Note Your userNN credential has the privileges required to run some, but not all of these commands. Run the command oc adm top pods to show see usage statistics for pods in your project. Example Output user01@lab061:~$ oc adm top pods NAME CPU(cores) MEMORY(bytes) mongodb-1-5nmjn 3m 83Mi mongodb-1-dh49x 3m 83Mi mongodb-1-r8dpw 3m 85Mi As OpenShift clusters grow in production, administrative commands like this one become more and more essential to keep everything running smoothly.","title":"Administrative CLI Commands"},{"location":"lab002-7/#administrative-cli-commands","text":"Important Work in Progress If you\u2019ve already completed Lab 001 \u2013 Exploring the OpenShift Console , you\u2019ll remember that there are both developer and administrator perspectives. The same is true in the OpenShift CLI. The oc adm command gives cluster administrators the ability to check logs, manage users, groups, policies, certificates, and many other tasks usually associated with administrative roles. Issue the oc adm --help command to see all of the OpenShift administrator commands. Example Output user01@lab061:~$ oc adm --help Administrative Commands Actions for administering an OpenShift cluster are exposed here. Usage: oc adm [flags] Cluster Management: upgrade Upgrade a cluster top Show usage statistics of resources on the server must-gather Launch a new instance of a pod for gathering debug information Node Management: drain Drain node in preparation for maintenance cordon Mark node as unschedulable uncordon Mark node as schedulable taint Update the taints on one or more nodes node-logs Display and filter node logs Security and Policy: new-project Create a new project policy Manage cluster authorization and security policy groups Manage groups certificate Approve or reject certificate requests pod-network Manage pod network Maintenance: prune Remove older versions of resources from the server migrate Migrate data in the cluster Configuration: create-kubeconfig Create a basic .kubeconfig file from client certs create-bootstrap-project-template Create a bootstrap project template create-login-template Create a login template create-provider-selection-template Create a provider selection template create-error-template Create an error page template Other Commands: build-chain Output the inputs and dependencies of your builds completion Output shell completion code for the specified shell (bash or zsh) config Change configuration files for the client verify-image-signature Verify the image identity contained in the image signature Note Your userNN credential has the privileges required to run some, but not all of these commands. Run the command oc adm top pods to show see usage statistics for pods in your project. Example Output user01@lab061:~$ oc adm top pods NAME CPU(cores) MEMORY(bytes) mongodb-1-5nmjn 3m 83Mi mongodb-1-dh49x 3m 83Mi mongodb-1-r8dpw 3m 85Mi As OpenShift clusters grow in production, administrative commands like this one become more and more essential to keep everything running smoothly.","title":"Administrative CLI Commands"},{"location":"lab002-8/","text":"Cleaning Up \u00b6 Important Work In Progress Double check that you are in your own userNN-project by issuing the command oc project Example Output user01@lab061:~$ oc project Using project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\". Once you\u2019re sure you\u2019re in your own project, **issue the following command to delete all objects associated with your application labeled mongodb-ephemeral. oc delete all --selector app=mongodb-ephemeral -o name ** Example Output user01@lab061:~$ oc delete all --selector app=mongodb-ephemeral -o name replicationcontroller/mongodb-1 service/mongodb deploymentconfig.apps.openshift.io/mongodb To check that all of your mongo application resources were deleted, run the command oc get all Example Output user01@lab061:~$ oc get all No resources found. user00@lab061:~$ (Optional) If there are leftover resources from other labs that you would like to delete, run the command oc delete all --all . Example Output user01@lab061:~$ oc delete all --all pod \"rails-postgresql-example-1-build\" deleted service \"postgresql\" deleted service \"rails-postgresql-example\" deleted buildconfig.build.openshift.io \"rails-postgresql-example\" deleted build.build.openshift.io \"rails-postgresql-example-1\" deleted imagestream.image.openshift.io \"rails-postgresql-example\" deleted route.route.openshift.io \"rails-postgresql-example\" deleted","title":"Cleaning Up"},{"location":"lab002-8/#cleaning-up","text":"Important Work In Progress Double check that you are in your own userNN-project by issuing the command oc project Example Output user01@lab061:~$ oc project Using project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\". Once you\u2019re sure you\u2019re in your own project, **issue the following command to delete all objects associated with your application labeled mongodb-ephemeral. oc delete all --selector app=mongodb-ephemeral -o name ** Example Output user01@lab061:~$ oc delete all --selector app=mongodb-ephemeral -o name replicationcontroller/mongodb-1 service/mongodb deploymentconfig.apps.openshift.io/mongodb To check that all of your mongo application resources were deleted, run the command oc get all Example Output user01@lab061:~$ oc get all No resources found. user00@lab061:~$ (Optional) If there are leftover resources from other labs that you would like to delete, run the command oc delete all --all . Example Output user01@lab061:~$ oc delete all --all pod \"rails-postgresql-example-1-build\" deleted service \"postgresql\" deleted service \"rails-postgresql-example\" deleted buildconfig.build.openshift.io \"rails-postgresql-example\" deleted build.build.openshift.io \"rails-postgresql-example-1\" deleted imagestream.image.openshift.io \"rails-postgresql-example\" deleted route.route.openshift.io \"rails-postgresql-example\" deleted","title":"Cleaning Up"},{"location":"lab003-1/","text":"Lab 003 - Using the z/OS Cloud Broker - Introduction \u00b6 Important Work In Progress The IBM z/OS Cloud Broker is an offering that connects z/OS services running on an IBM Z backend to a frontend private cloud platform providing self-service access and consumption of these services to developers. This allows developers to provision their own z/OS resources directly from the OpenShift console \u2013 without the need for z/OS skills or direct access. The services available for the broker to expose into OpenShift are: z/OS Connect EE Db2 CICS IMS MQ WLP Provision / deprovision z/OS Connect Servers. Start/Stop z/OS Connect Servers Provision / deprovision Db2 subsystems, schemas, and databases + snapshot / restore Provision / deprovision CICS regions Provision / deprovision IMS TM/DB systems Provision / deprovision MQ Queue Manager subsystem WebSphere Liberty Profile server provisioning, start/stop server In this lab, you will be provisioning a WebSphere Liberty Profile (WLP) server on z/OS using the z/OS Cloud Broker on OpenShift.","title":"Introduction"},{"location":"lab003-1/#lab-003-using-the-zos-cloud-broker-introduction","text":"Important Work In Progress The IBM z/OS Cloud Broker is an offering that connects z/OS services running on an IBM Z backend to a frontend private cloud platform providing self-service access and consumption of these services to developers. This allows developers to provision their own z/OS resources directly from the OpenShift console \u2013 without the need for z/OS skills or direct access. The services available for the broker to expose into OpenShift are: z/OS Connect EE Db2 CICS IMS MQ WLP Provision / deprovision z/OS Connect Servers. Start/Stop z/OS Connect Servers Provision / deprovision Db2 subsystems, schemas, and databases + snapshot / restore Provision / deprovision CICS regions Provision / deprovision IMS TM/DB systems Provision / deprovision MQ Queue Manager subsystem WebSphere Liberty Profile server provisioning, start/stop server In this lab, you will be provisioning a WebSphere Liberty Profile (WLP) server on z/OS using the z/OS Cloud Broker on OpenShift.","title":"Lab 003 - Using the z/OS Cloud Broker - Introduction"},{"location":"lab003-2/","text":"Connect to OpenShift and Authenticate \u00b6 Important Work In Progress In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocpd1.dmz/ . Important This is a different OpenShift cluster than the one used in other labs . Make sure you follow this correct link (atsocpd1) or you will not be able to find the z/OS Cloud Broker. Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP"},{"location":"lab003-2/#connect-to-openshift-and-authenticate","text":"Important Work In Progress In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocpd1.dmz/ . Important This is a different OpenShift cluster than the one used in other labs . Make sure you follow this correct link (atsocpd1) or you will not be able to find the z/OS Cloud Broker. Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OpenShift and Authenticate"},{"location":"lab003-3/","text":"Deploy Liberty for z/OS Using the z/OS Cloud Broker \u00b6 Important Work In Progress With the z/OS Cloud Broker and OpenShift, provisioning z/OS resources is as easy as clicking on a tile in the OpenShift Developer Catalog. Enter the Developer Perspective , if you aren\u2019t there already. Make sure that you\u2019re working under the z/OS Cloud Broker project atg-zoscb . Important Unlike other labs, this lab uses a shared project for all lab attendees. Please pay close attention to naming conventions so you do not end up deleting other attendees\u2019 provisioned services . Click the +Add button in the left-side menu . Click the From Catalog tile . Search the catalog for Liberty for z/OS and click on it . Click the Create button at the bottom of the page. Information This Liberty service does not live inside this OpenShift cluster. It is, in fact, a template for a z/OS Liberty instance that z/OSMF has found and displayed. When you provision an instance, it will spin up the service in a completely different z/OS LPAR separate from the Linux LPAR where this OpenShift cluster is running. All of the required fields will automatically populate for you, but rename the wlp service to userNN-wlp where NN is your user number. Important Please double check that you have correctly typed your user number for userNN. Remember that you are using a shared project for this lab, and nothing is stopping you from interfering with another lab participant's provisioned service if you use the wrong name. Click the create button . You will be brought to the topology page. After you click create, you will need to navigate to the service instance page: Switch to the Administrator Perspective -> Operators in the menu bar -> Installed Operators -> Liberty for z/OS -> Liberty for z/OS tab -> Click on the instance with your user number NN . You will end up on a screen that looks like the following: Notice two things: Depending on how quickly you navigated to this page and how long the WLP instance takes to provision, your status will be either Pending or Succeeded . Once it\u2019s Succeeded , you will have a link to your Dashboard. OpenShift is telling you that the service is either provisioned in z/OS, or in the process of being provisioned. While you don\u2019t have access to z/OSMF, the following is what you would see over in the z/OSMF console: If you want to look at the z/OSMF console, ask an instructor and they will give you a tour. This service will take a minute or two to provision. Wait until you see the following messages on the service instance page: Over on z/OSMF page again, this is what one would see: And in z/OS itself, the following task is started: You have just successfully provisioned a Liberty instance on z/OS, without leaving the OpenShift console. From the OpenShift WLP instance page, click the Dashboard URL hyperlink . Click the Log in with OpenShift button . You might get a security challenge here. If you do, make sure that both of the two checkboxes are checked, and click Allow Selected Permissions . You will be taken to the dashboard for your z/OS Liberty instance. This page will be referred to as the Dashboard tab. The right side of the page contains information about your WLP service and the z/OS system it\u2019s running on. The left side of the page contains buttons you can use to perform various actions. You will use a few of them shortly. Scroll to the bottom of the right-hand column, and locate the IP_ADDRESS variable . Hint It's 192.168.176.154 . That\u2019s the IP address of the z/OS system on which z/OSMF and Liberty for z/OS are hosted. Scroll up a bit and locate the HTTP_PORT variable . It\u2019s just about in the middle of the column. Hint It's something like 9XXX , where XXX will be unique for each user. Keeping the Dashboard tab open, open a new browser tab . In the new tab, navigate to : Hint It will look something like 192.168.176.154:9XXX , where the XXX is unique for each user. You should see the default Liberty homepage. This is the Liberty service you just provisioned on z/OS. Staying in this \u201cWelcome to Liberty\u201d tab, add the following string to the end of the URL : /CloudTestServlet Press enter . That will take you to a sample application that was deployed into the Liberty z/OS instance you provisioned. You will see something like this: Note the date and timestamp. It should be the current time (in U.S. Eastern time format). Reload the browser tab . You should see the time-stamp change. Do not close this tab . Return to the Dashboard tab , which had all the information about the provisioned instance in it.","title":"Deploy Liberty for z/OS Using the z/OS Cloud Broker"},{"location":"lab003-3/#deploy-liberty-for-zos-using-the-zos-cloud-broker","text":"Important Work In Progress With the z/OS Cloud Broker and OpenShift, provisioning z/OS resources is as easy as clicking on a tile in the OpenShift Developer Catalog. Enter the Developer Perspective , if you aren\u2019t there already. Make sure that you\u2019re working under the z/OS Cloud Broker project atg-zoscb . Important Unlike other labs, this lab uses a shared project for all lab attendees. Please pay close attention to naming conventions so you do not end up deleting other attendees\u2019 provisioned services . Click the +Add button in the left-side menu . Click the From Catalog tile . Search the catalog for Liberty for z/OS and click on it . Click the Create button at the bottom of the page. Information This Liberty service does not live inside this OpenShift cluster. It is, in fact, a template for a z/OS Liberty instance that z/OSMF has found and displayed. When you provision an instance, it will spin up the service in a completely different z/OS LPAR separate from the Linux LPAR where this OpenShift cluster is running. All of the required fields will automatically populate for you, but rename the wlp service to userNN-wlp where NN is your user number. Important Please double check that you have correctly typed your user number for userNN. Remember that you are using a shared project for this lab, and nothing is stopping you from interfering with another lab participant's provisioned service if you use the wrong name. Click the create button . You will be brought to the topology page. After you click create, you will need to navigate to the service instance page: Switch to the Administrator Perspective -> Operators in the menu bar -> Installed Operators -> Liberty for z/OS -> Liberty for z/OS tab -> Click on the instance with your user number NN . You will end up on a screen that looks like the following: Notice two things: Depending on how quickly you navigated to this page and how long the WLP instance takes to provision, your status will be either Pending or Succeeded . Once it\u2019s Succeeded , you will have a link to your Dashboard. OpenShift is telling you that the service is either provisioned in z/OS, or in the process of being provisioned. While you don\u2019t have access to z/OSMF, the following is what you would see over in the z/OSMF console: If you want to look at the z/OSMF console, ask an instructor and they will give you a tour. This service will take a minute or two to provision. Wait until you see the following messages on the service instance page: Over on z/OSMF page again, this is what one would see: And in z/OS itself, the following task is started: You have just successfully provisioned a Liberty instance on z/OS, without leaving the OpenShift console. From the OpenShift WLP instance page, click the Dashboard URL hyperlink . Click the Log in with OpenShift button . You might get a security challenge here. If you do, make sure that both of the two checkboxes are checked, and click Allow Selected Permissions . You will be taken to the dashboard for your z/OS Liberty instance. This page will be referred to as the Dashboard tab. The right side of the page contains information about your WLP service and the z/OS system it\u2019s running on. The left side of the page contains buttons you can use to perform various actions. You will use a few of them shortly. Scroll to the bottom of the right-hand column, and locate the IP_ADDRESS variable . Hint It's 192.168.176.154 . That\u2019s the IP address of the z/OS system on which z/OSMF and Liberty for z/OS are hosted. Scroll up a bit and locate the HTTP_PORT variable . It\u2019s just about in the middle of the column. Hint It's something like 9XXX , where XXX will be unique for each user. Keeping the Dashboard tab open, open a new browser tab . In the new tab, navigate to : Hint It will look something like 192.168.176.154:9XXX , where the XXX is unique for each user. You should see the default Liberty homepage. This is the Liberty service you just provisioned on z/OS. Staying in this \u201cWelcome to Liberty\u201d tab, add the following string to the end of the URL : /CloudTestServlet Press enter . That will take you to a sample application that was deployed into the Liberty z/OS instance you provisioned. You will see something like this: Note the date and timestamp. It should be the current time (in U.S. Eastern time format). Reload the browser tab . You should see the time-stamp change. Do not close this tab . Return to the Dashboard tab , which had all the information about the provisioned instance in it.","title":"Deploy Liberty for z/OS Using the z/OS Cloud Broker"},{"location":"lab003-4/","text":"Stop and Restart Liberty for z/OS from OCP \u00b6 Important Work In Progress In the Dashboard tab, you should see the following on the left side of the screen: Click the \"Run\" button that's associated with \"Stop\" . Click the \u201cAction History\u201d button above . Depending on how quickly you click on this button, you\u2019ll see either: If the stop is in progress, or: if the stop completed before you looked at the Action History. Over in z/OSMF, one would see: Once you see in the Action History that the stop has completed, go back to the tab where the timestamp application was ( 192.168.176.154:9XXX , if you accidentally closed it). Reload this page . You will see the following: (Or whatever your flavor of browser indicates when a server is no longer listening.) Go back to the Dashboard tab and click the \u201cRun\u201d button that\u2019s associated with \u201cStart\u201d . This will trigger a workflow over in z/OSMF to start the server. Click \u201cAction History\u201d and refresh until you see that the Start workflow is complete . Go back to the tab with the timestamp application and reload the page . You should see the time and date with the current time shown: This indicates that the server is back up and serving pages","title":"Stop and Restart Liberty for z/OS from OCP"},{"location":"lab003-4/#stop-and-restart-liberty-for-zos-from-ocp","text":"Important Work In Progress In the Dashboard tab, you should see the following on the left side of the screen: Click the \"Run\" button that's associated with \"Stop\" . Click the \u201cAction History\u201d button above . Depending on how quickly you click on this button, you\u2019ll see either: If the stop is in progress, or: if the stop completed before you looked at the Action History. Over in z/OSMF, one would see: Once you see in the Action History that the stop has completed, go back to the tab where the timestamp application was ( 192.168.176.154:9XXX , if you accidentally closed it). Reload this page . You will see the following: (Or whatever your flavor of browser indicates when a server is no longer listening.) Go back to the Dashboard tab and click the \u201cRun\u201d button that\u2019s associated with \u201cStart\u201d . This will trigger a workflow over in z/OSMF to start the server. Click \u201cAction History\u201d and refresh until you see that the Start workflow is complete . Go back to the tab with the timestamp application and reload the page . You should see the time and date with the current time shown: This indicates that the server is back up and serving pages","title":"Stop and Restart Liberty for z/OS from OCP"},{"location":"lab003-5/","text":"Cleaning Up \u00b6 Important Work In Progress Close the tab with the timestamp application . Close the Dashboard tab . Navigate back to your userNN-wlp instance Hint Administrator -> Operators -> Installed Operators -> Liberty for z/OS -> Liberty for z/OS tab Click the three dots to the far right of your provisioned service and click Delete WLP . Over in z/OSMF, that will trigger a de-provision operation: When the operation is complete, you will see On z/OSMF, the Liberty z/OS server instance has been de-provisioned, which means it was stopped and the file system location for the server instance removed.","title":"Cleaning Up"},{"location":"lab003-5/#cleaning-up","text":"Important Work In Progress Close the tab with the timestamp application . Close the Dashboard tab . Navigate back to your userNN-wlp instance Hint Administrator -> Operators -> Installed Operators -> Liberty for z/OS -> Liberty for z/OS tab Click the three dots to the far right of your provisioned service and click Delete WLP . Over in z/OSMF, that will trigger a de-provision operation: When the operation is complete, you will see On z/OSMF, the Liberty z/OS server instance has been de-provisioned, which means it was stopped and the file system location for the server instance removed.","title":"Cleaning Up"},{"location":"lab004-1/","text":"Source-to-Image (S2I) Overview \u00b6 Important Work In Progress","title":"Source-to-Image (S2I) Overview"},{"location":"lab004-1/#source-to-image-s2i-overview","text":"Important Work In Progress","title":"Source-to-Image (S2I) Overview"},{"location":"lab004-2/","text":"Source-to-Image (S2I) Overview \u00b6 Important Work In Progress","title":"Exploring GitHub and the Example Health Source Code"},{"location":"lab004-2/#source-to-image-s2i-overview","text":"Important Work In Progress","title":"Source-to-Image (S2I) Overview"},{"location":"lab004-3/","text":"Source-to-Image (S2I) Overview \u00b6 Important Work In Progress","title":"Connect to OCP and Authenticate"},{"location":"lab004-3/#source-to-image-s2i-overview","text":"Important Work In Progress","title":"Source-to-Image (S2I) Overview"},{"location":"lab004-4/","text":"Source-to-Image (S2I) Overview \u00b6 Important Work In Progress","title":"Edit the Source Code and Push an Update"},{"location":"lab004-4/#source-to-image-s2i-overview","text":"Important Work In Progress","title":"Source-to-Image (S2I) Overview"},{"location":"lab004-5/","text":"Cleaning Up \u00b6 Important Work In Progress","title":"Cleaning Up"},{"location":"lab004-5/#cleaning-up","text":"Important Work In Progress","title":"Cleaning Up"},{"location":"lab005-1/","text":"Lab 005 - Monitoring, Metering, and Metrics with OCP \u00b6 Important Work In Progress","title":"Lab 005"},{"location":"lab005-1/#lab-005-monitoring-metering-and-metrics-with-ocp","text":"Important Work In Progress","title":"Lab 005 - Monitoring, Metering, and Metrics with OCP"},{"location":"lab006-1/","text":"Lab 006 - Using Persistent Storage - MongoDB and NodeJS \u00b6 Important Work In Progress","title":"Lab 006"},{"location":"lab006-1/#lab-006-using-persistent-storage-mongodb-and-nodejs","text":"Important Work In Progress","title":"Lab 006 - Using Persistent Storage - MongoDB and NodeJS"},{"location":"lab007-1/","text":"Lab 007 - Deploying an Application with the Open Liberty Operator \u00b6 Important Work In Progress","title":"Lab 007"},{"location":"lab007-1/#lab-007-deploying-an-application-with-the-open-liberty-operator","text":"Important Work In Progress","title":"Lab 007 - Deploying an Application with the Open Liberty Operator"},{"location":"lab008-1/","text":"Lab 008 - Deploying an Application with Quarkus Red Hat Runtime \u00b6 Important Work In Progress","title":"Lab 008"},{"location":"lab008-1/#lab-008-deploying-an-application-with-quarkus-red-hat-runtime","text":"Important Work In Progress","title":"Lab 008 - Deploying an Application with Quarkus Red Hat Runtime"},{"location":"lab009-1/","text":"Lab 010 - OpenShift Service Mesh \u00b6 Important Work In Progress","title":"Lab 009"},{"location":"lab009-1/#lab-010-openshift-service-mesh","text":"Important Work In Progress","title":"Lab 010 - OpenShift Service Mesh"},{"location":"lab010-1/","text":"Lab 010 - OpenShift Service Mesh \u00b6 Important Work In Progress","title":"Lab 010"},{"location":"lab010-1/#lab-010-openshift-service-mesh","text":"Important Work In Progress","title":"Lab 010 - OpenShift Service Mesh"}]}