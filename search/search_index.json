{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop \u00b6 Welcome to the Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE workshop. Below you can find the workshop agenda, presentations, and lab documentation. Agenda \u00b6 Activity Duration Presentation 1 - High level overview of OpenShift, Cloud Paks, and running them on IBM Z 30-45 minutes Presentation 2 - Technical Deep Dive ~ 1 hour Connect to environment as a group 5-10 minutes Hands-on, self-paced labs Remainder of day Note The lab environments will be available the day following the workshop. For example, If the workshop is on a Thursday, the environments will be available until 5PM EST Friday. Presentations \u00b6 Presentation 1 - High Level Overview of Red Hat OpenShift & IBM Cloud Paks on IBM Z Presentation 2 - Technical Deep Dive, Installation & Configuration, Lessons Learned Labs \u00b6 Note The labs are designed so that you can pick and choose which you would like to complete. The labs are not designed for you to get through them all in one day. Labs are non-sequential and have no dependencies on one another. Introductory Labs \u00b6 Exploring the OpenShift Console Using the OpenShift Command Line (oc) OpenShift Capability Labs \u00b6 Deploying an Application from Source Code OpenShift Pipelines Monitoring, Metering, and Metrics Using Persistent Storage - MongoDB and NodeJS Extended Capability Labs \u00b6 Deploying an Application with the Open Liberty Operator Deploying an Application with Quarkus Red Hat Runtime Workshop Environment Architecture \u00b6 Please visit this page to see the architecture of the workshop's lab environment. Workshop Owners \u00b6 Matt Mondics Paul Novak","title":"Home"},{"location":"#red-hat-openshift-and-ibm-cloud-paks-on-ibm-z-and-linuxone-workshop","text":"Welcome to the Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE workshop. Below you can find the workshop agenda, presentations, and lab documentation.","title":"Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop"},{"location":"#agenda","text":"Activity Duration Presentation 1 - High level overview of OpenShift, Cloud Paks, and running them on IBM Z 30-45 minutes Presentation 2 - Technical Deep Dive ~ 1 hour Connect to environment as a group 5-10 minutes Hands-on, self-paced labs Remainder of day Note The lab environments will be available the day following the workshop. For example, If the workshop is on a Thursday, the environments will be available until 5PM EST Friday.","title":"Agenda"},{"location":"#presentations","text":"Presentation 1 - High Level Overview of Red Hat OpenShift & IBM Cloud Paks on IBM Z Presentation 2 - Technical Deep Dive, Installation & Configuration, Lessons Learned","title":"Presentations"},{"location":"#labs","text":"Note The labs are designed so that you can pick and choose which you would like to complete. The labs are not designed for you to get through them all in one day. Labs are non-sequential and have no dependencies on one another.","title":"Labs"},{"location":"#introductory-labs","text":"Exploring the OpenShift Console Using the OpenShift Command Line (oc)","title":"Introductory Labs"},{"location":"#openshift-capability-labs","text":"Deploying an Application from Source Code OpenShift Pipelines Monitoring, Metering, and Metrics Using Persistent Storage - MongoDB and NodeJS","title":"OpenShift Capability Labs"},{"location":"#extended-capability-labs","text":"Deploying an Application with the Open Liberty Operator Deploying an Application with Quarkus Red Hat Runtime","title":"Extended Capability Labs"},{"location":"#workshop-environment-architecture","text":"Please visit this page to see the architecture of the workshop's lab environment.","title":"Workshop Environment Architecture"},{"location":"#workshop-owners","text":"Matt Mondics Paul Novak","title":"Workshop Owners"},{"location":"lab-assignments/","text":"Lab Assignments \u00b6 There are connection instructions below the table on this page. Virtual Machine & OpenShift Logins \u00b6 Name Virtual Machine Password workshop-user Password User Number OpenShift Username OpenShift Password Bodie zyt1wvld p@ssw0rd 01 user01 p@ssw0rd Compton p4z4shdw p@ssw0rd 02 user02 p@ssw0rd Emery 9to5o0rp p@ssw0rd 03 user03 p@ssw0rd Fortin gllu1tom p@ssw0rd 04 user04 p@ssw0rd Fowlkes t08c7yma p@ssw0rd 05 user05 p@ssw0rd Frawley mw04xicr p@ssw0rd 06 user06 p@ssw0rd Gailey i4tr4f86 p@ssw0rd 07 user07 p@ssw0rd Harris m9fvfmsk p@ssw0rd 08 user08 p@ssw0rd Higgins dbdpoxu4 p@ssw0rd 09 user09 p@ssw0rd Johnson 39wrj896 p@ssw0rd 10 user10 p@ssw0rd Keller 2hzda34e p@ssw0rd 11 user11 p@ssw0rd Le 5j0cwpb8 p@ssw0rd 12 user12 p@ssw0rd McNeil szo7w7uo p@ssw0rd 13 user13 p@ssw0rd Sapp 2u0g64ra p@ssw0rd 14 user14 p@ssw0rd Smith pbmn9lej p@ssw0rd 15 user15 p@ssw0rd Thacker oo3uzexr p@ssw0rd 16 user16 p@ssw0rd Connecting to your RHEL Virtual Desktop \u00b6 Click the link to your personal virtual machine and enter the Virtual Machine Password provided on the table . Click the box for the RHEL desktop that should be green and running. Log into the RHEL desktop with the password: p@ssw0rd . Important Each virtual machine has a 3 hour inactivity timer. If you exceed this timeout, you can restart the virtual machine, but this will log you off of the VPN. If this happens, reach out to an instructor and they will log back into the VPN for you.","title":"Lab Assignments"},{"location":"lab-assignments/#lab-assignments","text":"There are connection instructions below the table on this page.","title":"Lab Assignments"},{"location":"lab-assignments/#virtual-machine-openshift-logins","text":"Name Virtual Machine Password workshop-user Password User Number OpenShift Username OpenShift Password Bodie zyt1wvld p@ssw0rd 01 user01 p@ssw0rd Compton p4z4shdw p@ssw0rd 02 user02 p@ssw0rd Emery 9to5o0rp p@ssw0rd 03 user03 p@ssw0rd Fortin gllu1tom p@ssw0rd 04 user04 p@ssw0rd Fowlkes t08c7yma p@ssw0rd 05 user05 p@ssw0rd Frawley mw04xicr p@ssw0rd 06 user06 p@ssw0rd Gailey i4tr4f86 p@ssw0rd 07 user07 p@ssw0rd Harris m9fvfmsk p@ssw0rd 08 user08 p@ssw0rd Higgins dbdpoxu4 p@ssw0rd 09 user09 p@ssw0rd Johnson 39wrj896 p@ssw0rd 10 user10 p@ssw0rd Keller 2hzda34e p@ssw0rd 11 user11 p@ssw0rd Le 5j0cwpb8 p@ssw0rd 12 user12 p@ssw0rd McNeil szo7w7uo p@ssw0rd 13 user13 p@ssw0rd Sapp 2u0g64ra p@ssw0rd 14 user14 p@ssw0rd Smith pbmn9lej p@ssw0rd 15 user15 p@ssw0rd Thacker oo3uzexr p@ssw0rd 16 user16 p@ssw0rd","title":"Virtual Machine &amp; OpenShift Logins"},{"location":"lab-assignments/#connecting-to-your-rhel-virtual-desktop","text":"Click the link to your personal virtual machine and enter the Virtual Machine Password provided on the table . Click the box for the RHEL desktop that should be green and running. Log into the RHEL desktop with the password: p@ssw0rd . Important Each virtual machine has a 3 hour inactivity timer. If you exceed this timeout, you can restart the virtual machine, but this will log you off of the VPN. If this happens, reach out to an instructor and they will log back into the VPN for you.","title":"Connecting to your RHEL Virtual Desktop"},{"location":"prerequisites/","text":"Prerequisites \u00b6 GitHub Account \u00b6 If you wish to complete Deploying an Application from Source Code , you must have your own GitHub account. You can create one create one by clicking the Sign Up button on the GitHub homepage .","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"prerequisites/#github-account","text":"If you wish to complete Deploying an Application from Source Code , you must have your own GitHub account. You can create one create one by clicking the Sign Up button on the GitHub homepage .","title":"GitHub Account"},{"location":"workshop-architecture/","text":"Workshop Architecture Diagram \u00b6 The OpenShift (OCP) on IBM Z environment used in this workshop is detailed in the diagram above. Note that this is not the recommended OpenShift architecture for high availablity or production. For OCP on Z reference architectures navigate to this link . The entire lab environment is behing the Washington Systems Center VPN. You are given a RHEL virtual machine with the Cisco AnyConnect VPN client installed and running which provides access to the WSC environment. There are 3 OpenShift clusters you access during the labs. Because of the wide variety of lab material that requires different operators , each with their own resource and version requirements, it is simpler to divide labs on to multiple clusters. Each OCP cluster is made up of 3 Control Planes and 3 Compute Nodes as shown in OCP Cluster 1 in the diagram. The Control Planes and Compute Nodes have a minimum of the resources shown for OCP cluster 1, although some clusters have more than the resources listed because of some more intensive applications running on them (IBM Cloud Paks & Foundational Services). All clusters are running on a single z/VM 7.1 instance on a single LPAR of an IBM z14 (again, not recommended outside of POC/demo). There are various other support servers running as Linux guests that you use during these labs. These are outside of the OCP cluster itself, but take care of tasks such as LDAP, NFS storage, and a server with the oc command line installed that will let you connect to the three OpenShift clusters.","title":"Lab Architecture"},{"location":"workshop-architecture/#workshop-architecture-diagram","text":"The OpenShift (OCP) on IBM Z environment used in this workshop is detailed in the diagram above. Note that this is not the recommended OpenShift architecture for high availablity or production. For OCP on Z reference architectures navigate to this link . The entire lab environment is behing the Washington Systems Center VPN. You are given a RHEL virtual machine with the Cisco AnyConnect VPN client installed and running which provides access to the WSC environment. There are 3 OpenShift clusters you access during the labs. Because of the wide variety of lab material that requires different operators , each with their own resource and version requirements, it is simpler to divide labs on to multiple clusters. Each OCP cluster is made up of 3 Control Planes and 3 Compute Nodes as shown in OCP Cluster 1 in the diagram. The Control Planes and Compute Nodes have a minimum of the resources shown for OCP cluster 1, although some clusters have more than the resources listed because of some more intensive applications running on them (IBM Cloud Paks & Foundational Services). All clusters are running on a single z/VM 7.1 instance on a single LPAR of an IBM z14 (again, not recommended outside of POC/demo). There are various other support servers running as Linux guests that you use during these labs. These are outside of the OCP cluster itself, but take care of tasks such as LDAP, NFS storage, and a server with the oc command line installed that will let you connect to the three OpenShift clusters.","title":"Workshop Architecture Diagram"},{"location":"lab001/lab001-1/","text":"Exploring the OpenShift Console \u00b6 The OpenShift Container Platform web console is a user interface accessible from a web browser. Developers can use the web console to visualize, browse, and manage the contents of projects . Administrators can use the web console to monitor the status of the applications running on the cluster, along with the cluster itself . The web console can be customized to suit an organization's needs, and when you log into the web console, you will only see the cluster resources that are available to you as allowed by the OpenShift Role Based Access Control (RBAC). The web console runs as a group of pods on the control plane nodes in the openshift-console project, along with a service exposed as a route.","title":"Introduction"},{"location":"lab001/lab001-1/#exploring-the-openshift-console","text":"The OpenShift Container Platform web console is a user interface accessible from a web browser. Developers can use the web console to visualize, browse, and manage the contents of projects . Administrators can use the web console to monitor the status of the applications running on the cluster, along with the cluster itself . The web console can be customized to suit an organization's needs, and when you log into the web console, you will only see the cluster resources that are available to you as allowed by the OpenShift Role Based Access Control (RBAC). The web console runs as a group of pods on the control plane nodes in the openshift-console project, along with a service exposed as a route.","title":"Exploring the OpenShift Console"},{"location":"lab001/lab001-2/","text":"Connect to OCP and Authenticate \u00b6 In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP"},{"location":"lab001/lab001-2/#connect-to-ocp-and-authenticate","text":"In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP and Authenticate"},{"location":"lab001/lab001-3/","text":"The Administrator Perspective \u00b6 Take a moment to notice the following elements in the navigation bar: Note These buttons display on each page of the OpenShift console. Note that the Applications button might be missing from your screen, depending on your credentials. By default, the menu on the left side of the page should be activated and displaying the cluster menu. In the left-side menu, select the Administrator perspective if it isn't already showing. With the administrator menu showing, you are provided with a broad range of options to manage the OpenShift cluster and the applications running on it. Expand to Learn More About the Different Views Developer / Administrator toggle . This lets you flip between which of the two perspectives you want to use. Home : Provides overview of projects, resources, and events in the scope of your credentials. Operators : Provides access to the OperatorHub to install new operators and also lets you view operators that are already installed. Workloads : Expands to provide access to many Kubernetes and OpenShift objects, such as pods, deployments, secrets, jobs and more. Networking : Provides access to services, routes, and ingresses required for external access to the cluster. Storage : Provides access to storage objects in the OpenShift cluster, such as PersistentVolumeClaims. Builds : View and create Build objects \u2013 use to transform input parameters into resulting objects. Pipelines : View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. This will be missing if not installed in your OpenShift cluster. Monitoring : Access cluster resource Monitoring, Metrics, and Alerting. Compute : Access cluster infrastructure \u2013 Control & Compute Nodes, Machines, and more. User Management : Access and manage Users, Groups, Roles, RoleBindings, Service Accounts, and more. Administration : View and edit cluster settings. The Administrator perspective is the default view for the OpenShift console for users who have an administrative access level. This perspective provides visibility into options related to cluster administration, as well as a broader view of the projects associated with the currently logged-in user. In the Menu, click Home -> Projects . The rest of the page is populated by projects. A project has been created for you to work in named userNN-project (where NN is your user number). Note Any project starting with openshift- or kube- contain the workloads running the OpenShift platform itself. Click the userNN-project hyperlink (where NN is your user number). Tip With so many Projects displayed, you can use the search bar to find yours more easily. You will now see the Dashboard for your project. Scroll down the Overview tab of your project . This displays information about what\u2019s going on in your project, such as CPU and memory usage, any alerts or crashlooping pods, an inventory of all the Kubernetes resources deployed in the project, and more. You won\u2019t see much information yet, as no workloads should be running in this project. Click the Workloads tab to the right of YAML. This page displays all of the workloads in your project, so it\u2019s empty for now. Note All objects in OpenShift are generated using YAML files. YAML (standing for Yet Another Markup Language) is meant to be a human-readable language for configuration files. Any OpenShift object such as Deployments, Services, Routes, and nearly everything else can be modified by directly editing their YAML file in either the console or command line. Workloads are typically created by developers, so in the next section, you will swap to the developer perspective to deploy a an application. You will return to the administrator perspective later in this lab.","title":"The Administrator Persepctive"},{"location":"lab001/lab001-3/#the-administrator-perspective","text":"Take a moment to notice the following elements in the navigation bar: Note These buttons display on each page of the OpenShift console. Note that the Applications button might be missing from your screen, depending on your credentials. By default, the menu on the left side of the page should be activated and displaying the cluster menu. In the left-side menu, select the Administrator perspective if it isn't already showing. With the administrator menu showing, you are provided with a broad range of options to manage the OpenShift cluster and the applications running on it. Expand to Learn More About the Different Views Developer / Administrator toggle . This lets you flip between which of the two perspectives you want to use. Home : Provides overview of projects, resources, and events in the scope of your credentials. Operators : Provides access to the OperatorHub to install new operators and also lets you view operators that are already installed. Workloads : Expands to provide access to many Kubernetes and OpenShift objects, such as pods, deployments, secrets, jobs and more. Networking : Provides access to services, routes, and ingresses required for external access to the cluster. Storage : Provides access to storage objects in the OpenShift cluster, such as PersistentVolumeClaims. Builds : View and create Build objects \u2013 use to transform input parameters into resulting objects. Pipelines : View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. This will be missing if not installed in your OpenShift cluster. Monitoring : Access cluster resource Monitoring, Metrics, and Alerting. Compute : Access cluster infrastructure \u2013 Control & Compute Nodes, Machines, and more. User Management : Access and manage Users, Groups, Roles, RoleBindings, Service Accounts, and more. Administration : View and edit cluster settings. The Administrator perspective is the default view for the OpenShift console for users who have an administrative access level. This perspective provides visibility into options related to cluster administration, as well as a broader view of the projects associated with the currently logged-in user. In the Menu, click Home -> Projects . The rest of the page is populated by projects. A project has been created for you to work in named userNN-project (where NN is your user number). Note Any project starting with openshift- or kube- contain the workloads running the OpenShift platform itself. Click the userNN-project hyperlink (where NN is your user number). Tip With so many Projects displayed, you can use the search bar to find yours more easily. You will now see the Dashboard for your project. Scroll down the Overview tab of your project . This displays information about what\u2019s going on in your project, such as CPU and memory usage, any alerts or crashlooping pods, an inventory of all the Kubernetes resources deployed in the project, and more. You won\u2019t see much information yet, as no workloads should be running in this project. Click the Workloads tab to the right of YAML. This page displays all of the workloads in your project, so it\u2019s empty for now. Note All objects in OpenShift are generated using YAML files. YAML (standing for Yet Another Markup Language) is meant to be a human-readable language for configuration files. Any OpenShift object such as Deployments, Services, Routes, and nearly everything else can be modified by directly editing their YAML file in either the console or command line. Workloads are typically created by developers, so in the next section, you will swap to the developer perspective to deploy a an application. You will return to the administrator perspective later in this lab.","title":"The Administrator Perspective"},{"location":"lab001/lab001-4/","text":"The Developer Perspective \u00b6 In the left-side Menu, click the Administrator dropdown, and select Developer . The Developer perspective provides views and workflows specific to developer use cases, while hiding many of the cluster management options typically used by administrators. This perspective provides developers with a streamlined view of the options they typically use. Expand to Learn More About the Different Views +Add : Clicking on this will open a prompt letting you add a workload to your current project. Topology : Displays all of the deployed workloads in the currently selected project. Monitoring : Lets you view the monitoring dashboard for just this project. Search : Used to search for any type of API resource present in this project, provided you have access to that resource type. Builds : This will let you view or create Build Configurations in the currently selected project. Pipelines : View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. Helm : Displays the Helm releases in this project, or prompts you to install one from the catalog if none are present. Project : Takes you to your project overview page, the project inventory, events, utilization, and more. Config Maps : Displays Config Maps for your project, which store non-confidential data in key-value pairs. Secrets : Displays Secrets for your project. Used to store sensitive, confidential data in key-value pairs, tokens, or passwords. Switching to the Developer perspective takes you to the Topology view. If no workloads are deployed in the selected project, options to start building an application or visit the +Add page or are displayed. If you ended up on a page other than Topology, continue with step 1 below anyways. Click the +Add button in the menu . Expand to learn about Deployment Methods There are multiple methods of deploying workloads from the OpenShift web browser. Samples : Red Hat provides sample applications in various languages. Use these to see what a pre-made application running in OpenShift can look like. From Git : Use this option to import an existing codebase in a Git repository to create, build, and deploy an application. From Devfile : Similar to From Git, use this option to import a Devfile from your Git repository to build and deploy an application. Container Image : Use existing images from an image stream or registry to deploy it. From Catalog : Explore the Developer Catalog to select the required applications, services, or source to image builders and add it to your project. From Dockerfile : Import a dockerfile from your Git repository to build and deploy an application. YAML : Use the editor to add YAML or JSON definitions to create and modify resources. Database : Filters the Developer Catalog to display only the databases it contains. Operator Backed : Deploy applications that are managed by Operators. Many of these will come from the OperatorHub. Helm Chart : Deploy applications defined by Helm Charts, which provide simple installations, upgrades, rollbacks, and generally reduced complexity. Pipeline : Create a Tekton-based Pipeline to automate application creation and delivery using OpenShift\u2019s built-in CI/CD capabilities. In the next section, you will deploy an application from the OpenShift Developer Catalog.","title":"The Developer Perspective"},{"location":"lab001/lab001-4/#the-developer-perspective","text":"In the left-side Menu, click the Administrator dropdown, and select Developer . The Developer perspective provides views and workflows specific to developer use cases, while hiding many of the cluster management options typically used by administrators. This perspective provides developers with a streamlined view of the options they typically use. Expand to Learn More About the Different Views +Add : Clicking on this will open a prompt letting you add a workload to your current project. Topology : Displays all of the deployed workloads in the currently selected project. Monitoring : Lets you view the monitoring dashboard for just this project. Search : Used to search for any type of API resource present in this project, provided you have access to that resource type. Builds : This will let you view or create Build Configurations in the currently selected project. Pipelines : View and create Pipelines \u2013 Tekton-based CI/CD processes and objects. Helm : Displays the Helm releases in this project, or prompts you to install one from the catalog if none are present. Project : Takes you to your project overview page, the project inventory, events, utilization, and more. Config Maps : Displays Config Maps for your project, which store non-confidential data in key-value pairs. Secrets : Displays Secrets for your project. Used to store sensitive, confidential data in key-value pairs, tokens, or passwords. Switching to the Developer perspective takes you to the Topology view. If no workloads are deployed in the selected project, options to start building an application or visit the +Add page or are displayed. If you ended up on a page other than Topology, continue with step 1 below anyways. Click the +Add button in the menu . Expand to learn about Deployment Methods There are multiple methods of deploying workloads from the OpenShift web browser. Samples : Red Hat provides sample applications in various languages. Use these to see what a pre-made application running in OpenShift can look like. From Git : Use this option to import an existing codebase in a Git repository to create, build, and deploy an application. From Devfile : Similar to From Git, use this option to import a Devfile from your Git repository to build and deploy an application. Container Image : Use existing images from an image stream or registry to deploy it. From Catalog : Explore the Developer Catalog to select the required applications, services, or source to image builders and add it to your project. From Dockerfile : Import a dockerfile from your Git repository to build and deploy an application. YAML : Use the editor to add YAML or JSON definitions to create and modify resources. Database : Filters the Developer Catalog to display only the databases it contains. Operator Backed : Deploy applications that are managed by Operators. Many of these will come from the OperatorHub. Helm Chart : Deploy applications defined by Helm Charts, which provide simple installations, upgrades, rollbacks, and generally reduced complexity. Pipeline : Create a Tekton-based Pipeline to automate application creation and delivery using OpenShift\u2019s built-in CI/CD capabilities. In the next section, you will deploy an application from the OpenShift Developer Catalog.","title":"The Developer Perspective"},{"location":"lab001/lab001-5/","text":"Deploy from the Developer Catalog \u00b6 In this section, you will be building a sample application from a template. The template will create two pods: A Ruby on Rails blogging application from source code in GitHub A PostgreSQL database from a container image Info A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the OpenShift internal registry, the Red Hat registry, Docker Hub, or a private registry of your own. Click the All Services option in the Developer Catalog section on the +Add page. This brings up the OpenShift Developer catalog containing all types of applications you can deploy including Operators, Helm Charts, Templates, and more. Find and click the Rails + PostgreSQL (Ephemeral) tile . Tip You can search for Rails + PostgreSQL (Ephemeral ) in the search bar. Click Instantiate Template on the next screen that appears. You are brought to a page full of configurable parameters that you can edit if so desired. Notice that all of the required fields on this page automatically populate. You can read through all of the options, but there is no need to edit any of them. Click the Create button at the bottom of the page. You will now be taken to the topology view, where you will see two icons \u2013 one for each of the two workload pods that the template will create. If you don\u2019t see the icons right away, you may need to refresh your browser window. Info The Ruby on Rails application will take a few minutes to fully deploy, while the PostgreSQL application will deploy in just a few seconds. The reason for this difference is that the Ruby application is being built (containerized) from Ruby source code located in the GitHub repository located here: https://github.com/sclorg/rails-ex.git into a container image, and then deployed. If you would like to watch the steps that OpenShift is taking to build the containerized application, click the circle labeled rails-postgresql-example, click the Resources tab, and click View Logs in the Builds section . The PostgreSQL application, on the other hand, is deployed from a pre-built container image hosted in quay.io, so it takes much less time to start up. You will know that both applications are successfully deployed and running when each icon has a solid blue circle. Click the icon for the rails-postgresql-example application . This will bring up a window on the right side of the screen with information about your DeploymentConfig. Click the Details tab if it is not already selected. Here you\u2019ll see information about your DeploymentConfig. Notice that many of the fields such as Labels, Update Strategy, and more have been populated with default values. These can be modified. Click the Actions dropdown . Many application configurations can be modified from this menu, along with other tasks such as starting or pausing a rollout, or deleting the deployment configuration. Click the up arrow next to the blue circle. This scales your application from one pod to two pods. Note This is a simple demonstration of horizontal scaling with Kubernetes. You now have two instances of your pod running in the OpenShift cluster. Traffic to the Rails application will now be distributed to each pod, and if for some reason a pod is lost, that traffic will be redistributed to the remaining pods until a Kubernetes starts another. If a whole compute node is lost, Kubernetes will move the pods to different compute nodes. OpenShift and Kubernetes also support autoscaling of pods based on CPU or memory consumption, but that is outside the scope of this lab. Click the Resources tab . Notice the two pods associated with your Rails application. On this page, you\u2019ll see more information about your pods, any build configurations currently running or completed, and the services/ports associated with the pod. Click the route address at the bottom of the resources tab . Expand for a Tip You could also access this route by clicking on the external link icon associated with your Rails pod on the Topology view. If you see the page above, your Rails application is up and running. You just deployed a Ruby on Rails application from source code residing in GitHub, and connected it to a PostgreSQL container deployed from a container image pulled from quay.io into OpenShift running on an IBM Z server. Feel free to read through the Rails application homepage to learn more about what this application can do. Add /articles to the end of the Rails homepage URL . This will result in a URL like the following: http://rails-postgresql-example-userNN-project.apps.atsocppa.dmz/articles Where NN is your user number. You are now interacting with the blogging application that\u2019s shipped with the Rails source code. If you create a new article, the contents for the Title and Body are stored in the PostgreSQL database in the other pod that makes up this application. In the next section you will navigate back to the Administrator perspective to see the overview of your project with a workload running.","title":"Deploy from the Developer Catalog"},{"location":"lab001/lab001-5/#deploy-from-the-developer-catalog","text":"In this section, you will be building a sample application from a template. The template will create two pods: A Ruby on Rails blogging application from source code in GitHub A PostgreSQL database from a container image Info A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the OpenShift internal registry, the Red Hat registry, Docker Hub, or a private registry of your own. Click the All Services option in the Developer Catalog section on the +Add page. This brings up the OpenShift Developer catalog containing all types of applications you can deploy including Operators, Helm Charts, Templates, and more. Find and click the Rails + PostgreSQL (Ephemeral) tile . Tip You can search for Rails + PostgreSQL (Ephemeral ) in the search bar. Click Instantiate Template on the next screen that appears. You are brought to a page full of configurable parameters that you can edit if so desired. Notice that all of the required fields on this page automatically populate. You can read through all of the options, but there is no need to edit any of them. Click the Create button at the bottom of the page. You will now be taken to the topology view, where you will see two icons \u2013 one for each of the two workload pods that the template will create. If you don\u2019t see the icons right away, you may need to refresh your browser window. Info The Ruby on Rails application will take a few minutes to fully deploy, while the PostgreSQL application will deploy in just a few seconds. The reason for this difference is that the Ruby application is being built (containerized) from Ruby source code located in the GitHub repository located here: https://github.com/sclorg/rails-ex.git into a container image, and then deployed. If you would like to watch the steps that OpenShift is taking to build the containerized application, click the circle labeled rails-postgresql-example, click the Resources tab, and click View Logs in the Builds section . The PostgreSQL application, on the other hand, is deployed from a pre-built container image hosted in quay.io, so it takes much less time to start up. You will know that both applications are successfully deployed and running when each icon has a solid blue circle. Click the icon for the rails-postgresql-example application . This will bring up a window on the right side of the screen with information about your DeploymentConfig. Click the Details tab if it is not already selected. Here you\u2019ll see information about your DeploymentConfig. Notice that many of the fields such as Labels, Update Strategy, and more have been populated with default values. These can be modified. Click the Actions dropdown . Many application configurations can be modified from this menu, along with other tasks such as starting or pausing a rollout, or deleting the deployment configuration. Click the up arrow next to the blue circle. This scales your application from one pod to two pods. Note This is a simple demonstration of horizontal scaling with Kubernetes. You now have two instances of your pod running in the OpenShift cluster. Traffic to the Rails application will now be distributed to each pod, and if for some reason a pod is lost, that traffic will be redistributed to the remaining pods until a Kubernetes starts another. If a whole compute node is lost, Kubernetes will move the pods to different compute nodes. OpenShift and Kubernetes also support autoscaling of pods based on CPU or memory consumption, but that is outside the scope of this lab. Click the Resources tab . Notice the two pods associated with your Rails application. On this page, you\u2019ll see more information about your pods, any build configurations currently running or completed, and the services/ports associated with the pod. Click the route address at the bottom of the resources tab . Expand for a Tip You could also access this route by clicking on the external link icon associated with your Rails pod on the Topology view. If you see the page above, your Rails application is up and running. You just deployed a Ruby on Rails application from source code residing in GitHub, and connected it to a PostgreSQL container deployed from a container image pulled from quay.io into OpenShift running on an IBM Z server. Feel free to read through the Rails application homepage to learn more about what this application can do. Add /articles to the end of the Rails homepage URL . This will result in a URL like the following: http://rails-postgresql-example-userNN-project.apps.atsocppa.dmz/articles Where NN is your user number. You are now interacting with the blogging application that\u2019s shipped with the Rails source code. If you create a new article, the contents for the Title and Body are stored in the PostgreSQL database in the other pod that makes up this application. In the next section you will navigate back to the Administrator perspective to see the overview of your project with a workload running.","title":"Deploy from the Developer Catalog"},{"location":"lab001/lab001-6/","text":"View Workload from the Administrator Perspective \u00b6 In the left-side menu, select the Administrator perspective . Navigate back to your project by clicking Menu -> Home -> Projects -> userNN-project . The overview page now displays data about the CPU and Memory Usage, new objects in your project inventory, and new activity in the events panel. Click View Events under the right-side panel. This page is populated with all of the events associated with your project, including errors, container creation messages, pod scaling and deletion, and much more. You can filter by type, category, or by searching for keywords. Note Feel free to click through a few more pages from the left-side main menu. You\u2019ll notice a few of them have objects created as a part of the Rails-PostgreSQL application, such as Workloads \uf0e0 Pods, Networking \uf0e0 Services and Routes, Builds \uf0e0 Image Streams. These were all created as part of the template package. Navigate back your project as in the previous step (or by clicking your browser\u2019s back button). Find the Inventory on the project page which lists all of the objects created as part of your application","title":"View Workload from the Administrator Perspective"},{"location":"lab001/lab001-6/#view-workload-from-the-administrator-perspective","text":"In the left-side menu, select the Administrator perspective . Navigate back to your project by clicking Menu -> Home -> Projects -> userNN-project . The overview page now displays data about the CPU and Memory Usage, new objects in your project inventory, and new activity in the events panel. Click View Events under the right-side panel. This page is populated with all of the events associated with your project, including errors, container creation messages, pod scaling and deletion, and much more. You can filter by type, category, or by searching for keywords. Note Feel free to click through a few more pages from the left-side main menu. You\u2019ll notice a few of them have objects created as a part of the Rails-PostgreSQL application, such as Workloads \uf0e0 Pods, Networking \uf0e0 Services and Routes, Builds \uf0e0 Image Streams. These were all created as part of the template package. Navigate back your project as in the previous step (or by clicking your browser\u2019s back button). Find the Inventory on the project page which lists all of the objects created as part of your application","title":"View Workload from the Administrator Perspective"},{"location":"lab001/lab001-7/","text":"Cleaning Up \u00b6 Navigate back your project as in the previous section (or by clicking your browser\u2019s back button). Find the Inventory on the project page which lists all of the objects created as part of your application Click the Deployment Configs hyperlink . For both of the 2 Deployment Configs that appear click the three dots on the right side of the screen, and then click Delete Deployment Config. This will delete some, but not all of the resources created by the application template. The running pods will be stopped and deleted, but some other components will remain. This is not a problem in the case of these labs.","title":"Cleaning Up"},{"location":"lab001/lab001-7/#cleaning-up","text":"Navigate back your project as in the previous section (or by clicking your browser\u2019s back button). Find the Inventory on the project page which lists all of the objects created as part of your application Click the Deployment Configs hyperlink . For both of the 2 Deployment Configs that appear click the three dots on the right side of the screen, and then click Delete Deployment Config. This will delete some, but not all of the resources created by the application template. The running pods will be stopped and deleted, but some other components will remain. This is not a problem in the case of these labs.","title":"Cleaning Up"},{"location":"lab002/lab002-1/","text":"The OpenShift Command Line (oc) \u00b6 The OpenShift command line oc is a command line tool that can be used to create applications and manage OpenShift projects. oc is ideal in situations where you: Work directly with project source code. Script OpenShift Container Platform operations. Are restricted by bandwidth resources and cannot use the web console. Furthermore, many people familiar with Linux and/or Kubernetes tend to find the oc command line an easier and more efficient method of performing tasks, rather than the web-based console. Like with the OpenShift web console, the OpenShift command line includes functions both for developers and for administrators .","title":"Introduction"},{"location":"lab002/lab002-1/#the-openshift-command-line-oc","text":"The OpenShift command line oc is a command line tool that can be used to create applications and manage OpenShift projects. oc is ideal in situations where you: Work directly with project source code. Script OpenShift Container Platform operations. Are restricted by bandwidth resources and cannot use the web console. Furthermore, many people familiar with Linux and/or Kubernetes tend to find the oc command line an easier and more efficient method of performing tasks, rather than the web-based console. Like with the OpenShift web console, the OpenShift command line includes functions both for developers and for administrators .","title":"The OpenShift Command Line (oc)"},{"location":"lab002/lab002-2/","text":"Log into OpenShift Using the CLI \u00b6 In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server : ssh userNN@192.168.176.61 (where NN is your user number). When prompted, enter your password : p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab002/lab002-2/#log-into-openshift-using-the-cli","text":"In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server : ssh userNN@192.168.176.61 (where NN is your user number). When prompted, enter your password : p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab002/lab002-3/","text":"Overview of the OpenShift CLI \u00b6 In your terminal, enter the command : oc --help Example Output user01@lab061:~$ oc --help OpenShift Client This client helps you develop, build, deploy, and run your applications on any OpenShift or Kubernetes cluster. It also includes the administrative commands for managing a cluster under the 'adm' subcommand. Usage: oc [flags] Basic Commands: login Log in to a server new-project Request a new project new-app Create a new application status Show an overview of the current project project Switch to another project projects Display existing projects explain Documentation of resources Build and Deploy Commands: rollout Manage a Kubernetes deployment or OpenShift deployment config rollback Revert part of an application back to a previous deployment new-build Create a new build configuration start-build Start a new build The --help flag will display all of the available options the oc CLI. Enter the following command oc new-app --help Example Output user01@lab061:~$ oc new-app --help Create a new application by specifying source code, templates, and/or images This command will try to build up the components of an application using images, templates, or code that has a public repository. It will lookup the images on the local Docker installation (if available), a container image registry, an integrated image stream, or stored templates. If you specify a source code URL, it will set up a build that takes your source code and converts it into an image that can run inside of a pod. Local source must be in a git repository that has a remote repository that the server can see. The images will be deployed via a deployment configuration, and a service will be connected to the first public port of the app. You may either specify components using the various existing flags or let new-app autodetect what kind of components you have provided. If you provide source code, a new build will be automatically triggered. You can use 'oc status' to check the progress. Usage: oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [flags] Examples: # List all local templates and image streams that can be used to create an app oc new-app --list # Create an application based on the source code in the current git repository (with a public remote) and a Docker image oc new-app . --docker-image=repo/langimage The --help flag now displays all of the available options for the oc new-app command. If you get confused about any of the commands we use in this workshop, or just want more information, using this flag is a good first step.","title":"Overview of the OpenShift CLI"},{"location":"lab002/lab002-3/#overview-of-the-openshift-cli","text":"In your terminal, enter the command : oc --help Example Output user01@lab061:~$ oc --help OpenShift Client This client helps you develop, build, deploy, and run your applications on any OpenShift or Kubernetes cluster. It also includes the administrative commands for managing a cluster under the 'adm' subcommand. Usage: oc [flags] Basic Commands: login Log in to a server new-project Request a new project new-app Create a new application status Show an overview of the current project project Switch to another project projects Display existing projects explain Documentation of resources Build and Deploy Commands: rollout Manage a Kubernetes deployment or OpenShift deployment config rollback Revert part of an application back to a previous deployment new-build Create a new build configuration start-build Start a new build The --help flag will display all of the available options the oc CLI. Enter the following command oc new-app --help Example Output user01@lab061:~$ oc new-app --help Create a new application by specifying source code, templates, and/or images This command will try to build up the components of an application using images, templates, or code that has a public repository. It will lookup the images on the local Docker installation (if available), a container image registry, an integrated image stream, or stored templates. If you specify a source code URL, it will set up a build that takes your source code and converts it into an image that can run inside of a pod. Local source must be in a git repository that has a remote repository that the server can see. The images will be deployed via a deployment configuration, and a service will be connected to the first public port of the app. You may either specify components using the various existing flags or let new-app autodetect what kind of components you have provided. If you provide source code, a new build will be automatically triggered. You can use 'oc status' to check the progress. Usage: oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [flags] Examples: # List all local templates and image streams that can be used to create an app oc new-app --list # Create an application based on the source code in the current git repository (with a public remote) and a Docker image oc new-app . --docker-image=repo/langimage The --help flag now displays all of the available options for the oc new-app command. If you get confused about any of the commands we use in this workshop, or just want more information, using this flag is a good first step.","title":"Overview of the OpenShift CLI"},{"location":"lab002/lab002-4/","text":"Deploy Container Image from the CLI \u00b6 oc new-app is a powerful and commonly used command in the OpenShift CLI. It has the ability to deploy applications from components that include: Source or binary code Container images Templates The set of objects created by oc new-app depends on the artifacts passed as an input. Run the following command to start a MongoDB deployment from a template : oc new-app --template=mongodb-ephemeral Example Output user01@lab061:~$ oc new-app --template=mongodb-ephemeral --> Deploying template \"openshift/mongodb-ephemeral\" to project user01-project MongoDB (Ephemeral) --------- MongoDB database service, without persistent storage. For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container. WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing The following service(s) have been created in your project: mongodb. Username: userFUX Password: AXGgm5dnKY44Byuk Database Name: sampledb Connection URL: mongodb://userFUX:AXGgm5dnKY44Byuk@mongodb/sampledb For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container. * With parameters: * Memory Limit=512Mi * Namespace=openshift * Database Service Name=mongodb * MongoDB Connection Username=userFUX # generated * MongoDB Connection Password=AXGgm5dnKY44Byuk # generated * MongoDB Database Name=sampledb * MongoDB Admin Password=JibwnlSwiow18owJ # generated * Version of MongoDB Image=3.6 --> Creating resources ... secret \"mongodb\" created service \"mongodb\" created deploymentconfig.apps.openshift.io \"mongodb\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/mongodb' Run 'oc status' to view your app. Note Notice a few things: OpenShift went out and found a template that matches your desired deployment \u2013 MongoDB-ephemeral. You\u2019re told what exactly is going to be created and what it will be named. Those objects are then created within your project space. You\u2019re told that the application was successfully deployed, but it is not yet exposed. This means that it\u2019s running, but it\u2019s not accessible from outside the cluster. Run the following command to view the app in your project space: oc status Example Output user01@lab061:~$ oc status In project user01-project on server https://api.atsocppa.dmz:6443 svc/mongodb - 172.30.94.118:27017 dc/mongodb deploys istag/mongodb:latest deployment #1 deployed 3 minutes ago - 1 pod View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'. Now run the following command to see all of the objects that were built: oc get all Example Output user01@lab061:~$ oc get all NAME READY STATUS RESTARTS AGE pod/mongodb-1-deploy 0/1 Completed 0 5m30s pod/mongodb-1-sj6mk 1/1 Running 0 5m22s NAME DESIRED CURRENT READY AGE replicationcontroller/mongodb-1 1 1 1 5m30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mongodb ClusterIP 172.30.94.118 <none> 27017/TCP 5m32s NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/mongodb 1 1 1 config,image(mongodb:3.6) These are the objects that OpenShift told us would be created, and they all work together to run the application. While they\u2019re all important pieces of this puzzle, pods are where the application code is actually running. Let\u2019s narrow down on our pods. Note You might also have objects left over from other labs if they were not completely cleaned out. This is okay and the objects for different applications will not interfere with one another due to their use of labels . Run the command : oc get pods Example Output user01@lab061:~$ oc get pods NAME READY STATUS RESTARTS AGE mongodb-1-deploy 0/1 Completed 0 28s mongodb-1-r8dpw 1/1 Running 0 19s The oc new-app command created two pods. One ending with \u201cdeploy\u201d, and the other ending with a randomly-generated string of 5 characters (r8dpw in the screenshot above). They are both associated with your mongo deployment, but one is in a Completed status, and one is Running . The Completed pod had one simple job \u2013 scale the other pod to its desired count of 1. Run the following command to see the logs for the deploy pod oc logs pod/mongodb-1-deploy Example Output user01@lab061:~$ oc logs pod/mongodb-1-deploy --> Scaling mongodb-1 to 1 --> Success That\u2019s a pretty simple responsibility. The second pod, ending in the randomly generated string of characters, has a much more complicated job. This is the pod where the MongoDB application code is actually running. Run the following command to see the logs for the MongoDB deployment: oc logs pod/mongodb-1-XXXXX Where XXXXX is your unique string of characters that you saw in the oc get pods output. Example Output user01@lab061:~$ oc logs pod/mongodb-1-r8dpw 2020-04-15T16:56:12.344+0000 I CONTROL [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none' 2020-04-15T16:56:12.346+0000 W ASIO [main] No TransportLayer configured during NetworkInterface startup 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=mongo-1-r8dpw 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] db version v4.2.5 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] git version: 2261279b51ea13df08ae708ff278f0679c59dc32 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.1.1 11 Sep 2018 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] allocator: tcmalloc 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] modules: none 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] build environment: 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] distmod: ubuntu1804 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] distarch: s390x 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] target_arch: s390x This is obviously a much busier pod. One of the first lines in its log tells you which version of MongoDB is running. In the next section, you will connect to the pod and see that it is actually running MongoDB.","title":"Deploy Container Image from the CLI"},{"location":"lab002/lab002-4/#deploy-container-image-from-the-cli","text":"oc new-app is a powerful and commonly used command in the OpenShift CLI. It has the ability to deploy applications from components that include: Source or binary code Container images Templates The set of objects created by oc new-app depends on the artifacts passed as an input. Run the following command to start a MongoDB deployment from a template : oc new-app --template=mongodb-ephemeral Example Output user01@lab061:~$ oc new-app --template=mongodb-ephemeral --> Deploying template \"openshift/mongodb-ephemeral\" to project user01-project MongoDB (Ephemeral) --------- MongoDB database service, without persistent storage. For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container. WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing The following service(s) have been created in your project: mongodb. Username: userFUX Password: AXGgm5dnKY44Byuk Database Name: sampledb Connection URL: mongodb://userFUX:AXGgm5dnKY44Byuk@mongodb/sampledb For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container. * With parameters: * Memory Limit=512Mi * Namespace=openshift * Database Service Name=mongodb * MongoDB Connection Username=userFUX # generated * MongoDB Connection Password=AXGgm5dnKY44Byuk # generated * MongoDB Database Name=sampledb * MongoDB Admin Password=JibwnlSwiow18owJ # generated * Version of MongoDB Image=3.6 --> Creating resources ... secret \"mongodb\" created service \"mongodb\" created deploymentconfig.apps.openshift.io \"mongodb\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/mongodb' Run 'oc status' to view your app. Note Notice a few things: OpenShift went out and found a template that matches your desired deployment \u2013 MongoDB-ephemeral. You\u2019re told what exactly is going to be created and what it will be named. Those objects are then created within your project space. You\u2019re told that the application was successfully deployed, but it is not yet exposed. This means that it\u2019s running, but it\u2019s not accessible from outside the cluster. Run the following command to view the app in your project space: oc status Example Output user01@lab061:~$ oc status In project user01-project on server https://api.atsocppa.dmz:6443 svc/mongodb - 172.30.94.118:27017 dc/mongodb deploys istag/mongodb:latest deployment #1 deployed 3 minutes ago - 1 pod View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'. Now run the following command to see all of the objects that were built: oc get all Example Output user01@lab061:~$ oc get all NAME READY STATUS RESTARTS AGE pod/mongodb-1-deploy 0/1 Completed 0 5m30s pod/mongodb-1-sj6mk 1/1 Running 0 5m22s NAME DESIRED CURRENT READY AGE replicationcontroller/mongodb-1 1 1 1 5m30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mongodb ClusterIP 172.30.94.118 <none> 27017/TCP 5m32s NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/mongodb 1 1 1 config,image(mongodb:3.6) These are the objects that OpenShift told us would be created, and they all work together to run the application. While they\u2019re all important pieces of this puzzle, pods are where the application code is actually running. Let\u2019s narrow down on our pods. Note You might also have objects left over from other labs if they were not completely cleaned out. This is okay and the objects for different applications will not interfere with one another due to their use of labels . Run the command : oc get pods Example Output user01@lab061:~$ oc get pods NAME READY STATUS RESTARTS AGE mongodb-1-deploy 0/1 Completed 0 28s mongodb-1-r8dpw 1/1 Running 0 19s The oc new-app command created two pods. One ending with \u201cdeploy\u201d, and the other ending with a randomly-generated string of 5 characters (r8dpw in the screenshot above). They are both associated with your mongo deployment, but one is in a Completed status, and one is Running . The Completed pod had one simple job \u2013 scale the other pod to its desired count of 1. Run the following command to see the logs for the deploy pod oc logs pod/mongodb-1-deploy Example Output user01@lab061:~$ oc logs pod/mongodb-1-deploy --> Scaling mongodb-1 to 1 --> Success That\u2019s a pretty simple responsibility. The second pod, ending in the randomly generated string of characters, has a much more complicated job. This is the pod where the MongoDB application code is actually running. Run the following command to see the logs for the MongoDB deployment: oc logs pod/mongodb-1-XXXXX Where XXXXX is your unique string of characters that you saw in the oc get pods output. Example Output user01@lab061:~$ oc logs pod/mongodb-1-r8dpw 2020-04-15T16:56:12.344+0000 I CONTROL [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none' 2020-04-15T16:56:12.346+0000 W ASIO [main] No TransportLayer configured during NetworkInterface startup 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=mongo-1-r8dpw 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] db version v4.2.5 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] git version: 2261279b51ea13df08ae708ff278f0679c59dc32 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.1.1 11 Sep 2018 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] allocator: tcmalloc 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] modules: none 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] build environment: 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] distmod: ubuntu1804 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] distarch: s390x 2020-04-15T16:56:12.351+0000 I CONTROL [initandlisten] target_arch: s390x This is obviously a much busier pod. One of the first lines in its log tells you which version of MongoDB is running. In the next section, you will connect to the pod and see that it is actually running MongoDB.","title":"Deploy Container Image from the CLI"},{"location":"lab002/lab002-5/","text":"Open a Remote Shell Session into the MongoDB Pod \u00b6 OpenShift provides Remote Shell capabilities from both the command line and from the web console. With the oc rsh command, you can issue commands as if you are inside the container and perform local operations like monitoring, debugging, and using CLI commands specific to what is running in the container. Information For example, if you open a remote shell session into a MySQL container, you can count the number of records in the database by invoking the mysql command, then using the prompt to type in the SELECT command. You can also use commands like ps(1) and ls(1) for validation. With the MongoDB application you deployed, you can rsh into the MongoDB pod to run mongo CLI commands. Enter the following command to rsh into the container: oc rsh mongo-XXXXX Where XXXXX is your unique string of 5 characters Example Output user01@lab061:~$ oc rsh mongodb-1-r8dpw $ This new line that does not start with userNN@lab061 indicates that you are now in the remote shell session for the pod Important If you wait too long to interact with the remote shell (about a minute), it will automatically time-out and you will have to re-connect. You can tell that this happened if the prompt reappears. In the remote session, issue the command : mongo Example Output $ mongo MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"2320e01b-168e-41d0-a132-af0c9243d29c\") } MongoDB server version: 4.2.5 Welcome to the MongoDB shell. For interactive help, type \"help\". For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user mongo is the shell command for MongoDB. Issuing the mongo command without any options or flags connects you to a MongoDB instance running on your localhost with port 27017. If you see this message, MongoDB is up and running in the container. Exit the MongoDB shell by entering the command : exit Exit the remote shell session by entering, once again : exit You should be back in the userNN@lab061 command line.","title":"Open a Remote Shell Session into the MongoDB Pod"},{"location":"lab002/lab002-5/#open-a-remote-shell-session-into-the-mongodb-pod","text":"OpenShift provides Remote Shell capabilities from both the command line and from the web console. With the oc rsh command, you can issue commands as if you are inside the container and perform local operations like monitoring, debugging, and using CLI commands specific to what is running in the container. Information For example, if you open a remote shell session into a MySQL container, you can count the number of records in the database by invoking the mysql command, then using the prompt to type in the SELECT command. You can also use commands like ps(1) and ls(1) for validation. With the MongoDB application you deployed, you can rsh into the MongoDB pod to run mongo CLI commands. Enter the following command to rsh into the container: oc rsh mongo-XXXXX Where XXXXX is your unique string of 5 characters Example Output user01@lab061:~$ oc rsh mongodb-1-r8dpw $ This new line that does not start with userNN@lab061 indicates that you are now in the remote shell session for the pod Important If you wait too long to interact with the remote shell (about a minute), it will automatically time-out and you will have to re-connect. You can tell that this happened if the prompt reappears. In the remote session, issue the command : mongo Example Output $ mongo MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"2320e01b-168e-41d0-a132-af0c9243d29c\") } MongoDB server version: 4.2.5 Welcome to the MongoDB shell. For interactive help, type \"help\". For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user mongo is the shell command for MongoDB. Issuing the mongo command without any options or flags connects you to a MongoDB instance running on your localhost with port 27017. If you see this message, MongoDB is up and running in the container. Exit the MongoDB shell by entering the command : exit Exit the remote shell session by entering, once again : exit You should be back in the userNN@lab061 command line.","title":"Open a Remote Shell Session into the MongoDB Pod"},{"location":"lab002/lab002-6/","text":"Working with Pods \u00b6 One of the main benefits of using containers and Kubernetes-based cloud platforms like OpenShift is the ability to scale horizontally \u2013 rapidly duplicating or deleting pods to meet a desired state. Information One of the core concepts of Kubernetes is the Declarative State . Users declare what resources they want, and Kubernetes does whatever it can to make that happen. Scaling is one example of this. Scaling essentially creates copies of the application in order to distribute traffic to multiple instances and/or compute nodes for high availability and load balancing. Enter the following command to get the name of your MongoDB deploymentconfig (dc) oc get dc Example Output user01@lab061:~$ oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY mongodb 1 1 1 config,image(mongodb:3.6) Your deploymentconfig named mongo has a count desired = current = 1. Scale the mongo deployment to 3 replicas : oc scale dc/mongodb --replicas=3 Example Output user01@lab061:~$ oc scale dc/mongodb --replicas=3 deploymentconfig.apps.openshift.io/mongodb scaled Enter the following command again to see the scaled application. oc get dc Example Output user01@lab061:~$ oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY mongodb 1 3 3 config,image(mongodb:3.6) This output is telling you that OpenShift knows that you want three copies (pods) of MongoDB, and it is successfully meeting that declared state. Enter the following command again to see your three pods : oc get pods Example Output user01@lab061:~$ oc get pods NAME READY STATUS RESTARTS AGE mongodb-1-5nmjn 1/1 Running 0 2m6s mongodb-1-deploy 0/1 Completed 0 20m mongodb-1-dh49x 1/1 Running 0 2m6s mongodb-1-r8dpw 1/1 Running 0 19m Two of the pods will have a shorter age than the original one \u2013 these are the two new pods that were just created when you scaled the application. Dig into the pods a little bit further by entering the following command : oc describe pod/mongodb-1-XXXXX Where XXXXX is one of your unique strings of characters. Example Output user01@lab061:~$ oc describe pod/mongodb-1-5nmjn Name: mongodb-1-5nmjn Namespace: user01-project Priority: 0 PriorityClassName: <none> Node: worker-0.atsocppa.dmz/192.168.176.175 Start Time: Wed, 15 Apr 2020 13:13:53 -0400 Labels: app=mongodb deployment=mongodb-1 deploymentconfig=mongodb This command gives you all kinds of information about your pod. Notice the Node: field that begins with worker-# . Run the same command again, but on a different pod this time : oc describe pod/mongodb-1-YYYYY Where YYYYY is one of your other unique strings of characters. Pick one different than the previous step. Example Output user01@lab061:~$ oc describe pod/mongodb-1-r8dpw Name: mongodb-1-r8dpw Namespace: user01-project Priority: 0 PriorityClassName: <none> Node: worker-2.atsocppa.dmz/192.168.176.177 Start Time: Wed, 15 Apr 2020 12:56:03 -0400 Labels: app=mongodb deployment=mongodb-1 deploymentconfig=mongodb It is likely (but not guaranteed) that this pod has been placed on a different compute node than the first pod you described. The reason for this is that you have three compute nodes in this OpenShift cluster, and Kubernetes balances the load for this application across multiple nodes.","title":"Working with Pods"},{"location":"lab002/lab002-6/#working-with-pods","text":"One of the main benefits of using containers and Kubernetes-based cloud platforms like OpenShift is the ability to scale horizontally \u2013 rapidly duplicating or deleting pods to meet a desired state. Information One of the core concepts of Kubernetes is the Declarative State . Users declare what resources they want, and Kubernetes does whatever it can to make that happen. Scaling is one example of this. Scaling essentially creates copies of the application in order to distribute traffic to multiple instances and/or compute nodes for high availability and load balancing. Enter the following command to get the name of your MongoDB deploymentconfig (dc) oc get dc Example Output user01@lab061:~$ oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY mongodb 1 1 1 config,image(mongodb:3.6) Your deploymentconfig named mongo has a count desired = current = 1. Scale the mongo deployment to 3 replicas : oc scale dc/mongodb --replicas=3 Example Output user01@lab061:~$ oc scale dc/mongodb --replicas=3 deploymentconfig.apps.openshift.io/mongodb scaled Enter the following command again to see the scaled application. oc get dc Example Output user01@lab061:~$ oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY mongodb 1 3 3 config,image(mongodb:3.6) This output is telling you that OpenShift knows that you want three copies (pods) of MongoDB, and it is successfully meeting that declared state. Enter the following command again to see your three pods : oc get pods Example Output user01@lab061:~$ oc get pods NAME READY STATUS RESTARTS AGE mongodb-1-5nmjn 1/1 Running 0 2m6s mongodb-1-deploy 0/1 Completed 0 20m mongodb-1-dh49x 1/1 Running 0 2m6s mongodb-1-r8dpw 1/1 Running 0 19m Two of the pods will have a shorter age than the original one \u2013 these are the two new pods that were just created when you scaled the application. Dig into the pods a little bit further by entering the following command : oc describe pod/mongodb-1-XXXXX Where XXXXX is one of your unique strings of characters. Example Output user01@lab061:~$ oc describe pod/mongodb-1-5nmjn Name: mongodb-1-5nmjn Namespace: user01-project Priority: 0 PriorityClassName: <none> Node: worker-0.atsocppa.dmz/192.168.176.175 Start Time: Wed, 15 Apr 2020 13:13:53 -0400 Labels: app=mongodb deployment=mongodb-1 deploymentconfig=mongodb This command gives you all kinds of information about your pod. Notice the Node: field that begins with worker-# . Run the same command again, but on a different pod this time : oc describe pod/mongodb-1-YYYYY Where YYYYY is one of your other unique strings of characters. Pick one different than the previous step. Example Output user01@lab061:~$ oc describe pod/mongodb-1-r8dpw Name: mongodb-1-r8dpw Namespace: user01-project Priority: 0 PriorityClassName: <none> Node: worker-2.atsocppa.dmz/192.168.176.177 Start Time: Wed, 15 Apr 2020 12:56:03 -0400 Labels: app=mongodb deployment=mongodb-1 deploymentconfig=mongodb It is likely (but not guaranteed) that this pod has been placed on a different compute node than the first pod you described. The reason for this is that you have three compute nodes in this OpenShift cluster, and Kubernetes balances the load for this application across multiple nodes.","title":"Working with Pods"},{"location":"lab002/lab002-7/","text":"Administrative CLI Commands \u00b6 If you\u2019ve already completed Exploring the OpenShift Console , you\u2019ll remember that there are both developer and administrator perspectives. The same is true in the OpenShift CLI. The oc adm command gives cluster administrators the ability to check logs, manage users, groups, policies, certificates, and many other tasks usually associated with administrative roles. Issue the following command to see all of the OpenShift administrator commands . oc adm --help Example Output user01@lab061:~$ oc adm --help Administrative Commands Actions for administering an OpenShift cluster are exposed here. Usage: oc adm [flags] Cluster Management: upgrade Upgrade a cluster top Show usage statistics of resources on the server must-gather Launch a new instance of a pod for gathering debug information Node Management: drain Drain node in preparation for maintenance cordon Mark node as unschedulable uncordon Mark node as schedulable taint Update the taints on one or more nodes node-logs Display and filter node logs Security and Policy: new-project Create a new project policy Manage cluster authorization and security policy groups Manage groups certificate Approve or reject certificate requests pod-network Manage pod network Maintenance: prune Remove older versions of resources from the server migrate Migrate data in the cluster Configuration: create-kubeconfig Create a basic .kubeconfig file from client certs create-bootstrap-project-template Create a bootstrap project template create-login-template Create a login template create-provider-selection-template Create a provider selection template create-error-template Create an error page template Other Commands: build-chain Output the inputs and dependencies of your builds completion Output shell completion code for the specified shell (text or zsh) config Change configuration files for the client verify-image-signature Verify the image identity contained in the image signature Note Your userNN credential has the privileges required to run some, but not all of these commands. Run the following administrative command to show see usage statistics for pods in your project . oc adm top pods Example Output user01@lab061:~$ oc adm top pods NAME CPU(cores) MEMORY(bytes) mongodb-1-5nmjn 3m 83Mi mongodb-1-dh49x 3m 83Mi mongodb-1-r8dpw 3m 85Mi As OpenShift clusters grow in production, administrative commands like this one become more and more essential to keep everything running smoothly.","title":"Administrative CLI Commands"},{"location":"lab002/lab002-7/#administrative-cli-commands","text":"If you\u2019ve already completed Exploring the OpenShift Console , you\u2019ll remember that there are both developer and administrator perspectives. The same is true in the OpenShift CLI. The oc adm command gives cluster administrators the ability to check logs, manage users, groups, policies, certificates, and many other tasks usually associated with administrative roles. Issue the following command to see all of the OpenShift administrator commands . oc adm --help Example Output user01@lab061:~$ oc adm --help Administrative Commands Actions for administering an OpenShift cluster are exposed here. Usage: oc adm [flags] Cluster Management: upgrade Upgrade a cluster top Show usage statistics of resources on the server must-gather Launch a new instance of a pod for gathering debug information Node Management: drain Drain node in preparation for maintenance cordon Mark node as unschedulable uncordon Mark node as schedulable taint Update the taints on one or more nodes node-logs Display and filter node logs Security and Policy: new-project Create a new project policy Manage cluster authorization and security policy groups Manage groups certificate Approve or reject certificate requests pod-network Manage pod network Maintenance: prune Remove older versions of resources from the server migrate Migrate data in the cluster Configuration: create-kubeconfig Create a basic .kubeconfig file from client certs create-bootstrap-project-template Create a bootstrap project template create-login-template Create a login template create-provider-selection-template Create a provider selection template create-error-template Create an error page template Other Commands: build-chain Output the inputs and dependencies of your builds completion Output shell completion code for the specified shell (text or zsh) config Change configuration files for the client verify-image-signature Verify the image identity contained in the image signature Note Your userNN credential has the privileges required to run some, but not all of these commands. Run the following administrative command to show see usage statistics for pods in your project . oc adm top pods Example Output user01@lab061:~$ oc adm top pods NAME CPU(cores) MEMORY(bytes) mongodb-1-5nmjn 3m 83Mi mongodb-1-dh49x 3m 83Mi mongodb-1-r8dpw 3m 85Mi As OpenShift clusters grow in production, administrative commands like this one become more and more essential to keep everything running smoothly.","title":"Administrative CLI Commands"},{"location":"lab002/lab002-8/","text":"Cleaning Up \u00b6 Double check that you are in your own userNN-project by issuing the command : oc project Example Output user01@lab061:~$ oc project Using project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\". Once you\u2019re sure you\u2019re in your own project, issue the following command to delete all objects associated with your application labeled mongodb-ephemeral . oc delete all --selector app=mongodb-ephemeral -o name Example Output user01@lab061:~$ oc delete all --selector app=mongodb-ephemeral -o name replicationcontroller/mongodb-1 service/mongodb deploymentconfig.apps.openshift.io/mongodb Run the following command to check that all of your mongo application resources were deleted : oc get all Example Output user01@lab061:~$ oc get all No resources found. user00@lab061:~$ ( Optional ) If there are leftover resources from other labs that you would like to delete, run the command : oc delete all --all Example Output user01@lab061:~$ oc delete all --all pod \"rails-postgresql-example-1-build\" deleted service \"postgresql\" deleted service \"rails-postgresql-example\" deleted buildconfig.build.openshift.io \"rails-postgresql-example\" deleted build.build.openshift.io \"rails-postgresql-example-1\" deleted imagestream.image.openshift.io \"rails-postgresql-example\" deleted route.route.openshift.io \"rails-postgresql-example\" deleted","title":"Cleaning Up"},{"location":"lab002/lab002-8/#cleaning-up","text":"Double check that you are in your own userNN-project by issuing the command : oc project Example Output user01@lab061:~$ oc project Using project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\". Once you\u2019re sure you\u2019re in your own project, issue the following command to delete all objects associated with your application labeled mongodb-ephemeral . oc delete all --selector app=mongodb-ephemeral -o name Example Output user01@lab061:~$ oc delete all --selector app=mongodb-ephemeral -o name replicationcontroller/mongodb-1 service/mongodb deploymentconfig.apps.openshift.io/mongodb Run the following command to check that all of your mongo application resources were deleted : oc get all Example Output user01@lab061:~$ oc get all No resources found. user00@lab061:~$ ( Optional ) If there are leftover resources from other labs that you would like to delete, run the command : oc delete all --all Example Output user01@lab061:~$ oc delete all --all pod \"rails-postgresql-example-1-build\" deleted service \"postgresql\" deleted service \"rails-postgresql-example\" deleted buildconfig.build.openshift.io \"rails-postgresql-example\" deleted build.build.openshift.io \"rails-postgresql-example-1\" deleted imagestream.image.openshift.io \"rails-postgresql-example\" deleted route.route.openshift.io \"rails-postgresql-example\" deleted","title":"Cleaning Up"},{"location":"lab003/lab003-1/","text":"The z/OS Cloud Broker \u00b6 The IBM z/OS Cloud Broker is an IBM offering that connects z/OS services running on an IBM Z backend to a frontend container platform providing self-service access and consumption of these services to developers. This allows developers to provision their own z/OS resources directly from the OpenShift console \u2013 without the need for z/OS skills or direct access. The services available for the z/OS Cloud Broker to expose into OpenShift are: z/OS Connect EE Db2 CICS IMS MQ WLP Provision / deprovision z/OS Connect Servers. Start/Stop z/OS Connect Servers Provision / deprovision Db2 subsystems, schemas, and databases + snapshot / restore Provision / deprovision CICS regions Provision / deprovision IMS TM/DB systems Provision / deprovision MQ Queue Manager subsystem WebSphere Liberty Profile server provisioning, start/stop server In this lab, you will be provisioning a WebSphere Liberty Profile (WLP) server on z/OS using the z/OS Cloud Broker on OpenShift.","title":"The z/OS Cloud Broker"},{"location":"lab003/lab003-1/#the-zos-cloud-broker","text":"The IBM z/OS Cloud Broker is an IBM offering that connects z/OS services running on an IBM Z backend to a frontend container platform providing self-service access and consumption of these services to developers. This allows developers to provision their own z/OS resources directly from the OpenShift console \u2013 without the need for z/OS skills or direct access. The services available for the z/OS Cloud Broker to expose into OpenShift are: z/OS Connect EE Db2 CICS IMS MQ WLP Provision / deprovision z/OS Connect Servers. Start/Stop z/OS Connect Servers Provision / deprovision Db2 subsystems, schemas, and databases + snapshot / restore Provision / deprovision CICS regions Provision / deprovision IMS TM/DB systems Provision / deprovision MQ Queue Manager subsystem WebSphere Liberty Profile server provisioning, start/stop server In this lab, you will be provisioning a WebSphere Liberty Profile (WLP) server on z/OS using the z/OS Cloud Broker on OpenShift.","title":"The z/OS Cloud Broker"},{"location":"lab003/lab003-2/","text":"Connect to OpenShift and Authenticate \u00b6 In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OpenShift and Authenticate"},{"location":"lab003/lab003-2/#connect-to-openshift-and-authenticate","text":"In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OpenShift and Authenticate"},{"location":"lab003/lab003-3/","text":"Deploy Liberty for z/OS Using the z/OS Cloud Broker \u00b6 With the z/OS Cloud Broker and OpenShift, provisioning z/OS resources is as easy as clicking on a tile in the OpenShift Developer Catalog. Enter the Developer Perspective , if you aren\u2019t there already. Make sure that you\u2019re working under the z/OS Cloud Broker project atg-zoscb . Important Unlike other labs, this lab uses a shared project for all lab attendees. Please pay close attention to naming conventions so you do not end up deleting other attendees\u2019 provisioned services . Click the +Add button in the left-side menu . Click the All Services option under the Developer Catalog section . Search the catalog for Liberty for z/OS and click on it . Click the Create button at the bottom of the page. Information This Liberty service does not live inside this OpenShift cluster. It is, in fact, a template for a z/OS Liberty instance that z/OSMF has found and displayed. When you provision an instance, it will spin up the service in a completely different z/OS LPAR separate from the Linux LPAR where this OpenShift cluster is running. All of the required fields will automatically populate for you, but rename the wlp service to userNN-wlp where NN is your user number. Important Please double check that you have correctly typed your user number for userNN. Remember that you are using a shared project for this lab, and nothing is stopping you from interfering with another lab participant's provisioned service if you use the wrong name. Click the create button . You will be brought to the topology page. After you click create, you will need to navigate to the service instance page: Switch to the Administrator Perspective -> Operators in the menu bar -> Installed Operators -> Liberty for z/OS -> Liberty for z/OS tab -> Click on the instance with your user number NN . You will end up on a screen that looks like the following: Notice two things: Depending on how quickly you navigated to this page and how long the WLP instance takes to provision, your status will be either Pending or Succeeded . Once it\u2019s Succeeded , you will have a link to your Dashboard. OpenShift is telling you that the service is either provisioned in z/OS, or in the process of being provisioned. While you don\u2019t have access to z/OSMF, the following is what you would see over in the z/OSMF console: If you want to look at the z/OSMF console, ask an instructor and they will give you a tour. This service will take a minute or two to provision. Wait until you see the following messages on the service instance page: Over on z/OSMF page again, this is what one would see: And in z/OS itself, the following task is started: You have just successfully provisioned a Liberty instance on z/OS, without leaving the OpenShift console. From the OpenShift WLP instance page, click the Dashboard URL hyperlink . Click the Log in with OpenShift button . You might get a security challenge here. If you do, make sure that both of the two checkboxes are checked, and click Allow Selected Permissions . You will be taken to the dashboard for your z/OS Liberty instance. This page will be referred to as the Dashboard tab. The right side of the page contains information about your WLP service and the z/OS system it\u2019s running on. The left side of the page contains buttons you can use to perform various actions. You will use a few of them shortly. Scroll to the bottom of the right-hand column, and locate the IP_ADDRESS variable . Hint It's 192.168.176.154 . That\u2019s the IP address of the z/OS system on which z/OSMF and Liberty for z/OS are hosted. Scroll up a bit and locate the HTTP_PORT variable . It\u2019s just about in the middle of the column. Hint It's something like 9XXX , where XXX will be unique for each user. Keeping the Dashboard tab open, open a new browser tab . In the new tab, navigate to <IP_ADDRESS>:<HTTP_PORT> Hint It will look something like 192.168.176.154:9XXX , where the XXX is unique for each user. You should see the default Liberty homepage. This is the Liberty service you just provisioned on z/OS. Staying in this \u201cWelcome to Liberty\u201d tab, add the following string to the end of the URL : /CloudTestServlet Press enter . That will take you to a sample application that was deployed into the Liberty z/OS instance you provisioned. You will see something like this: Note the date and timestamp. It should be the current time (in U.S. Eastern time format). Reload the browser tab . You should see the time-stamp change. Do not close this tab . Return to the Dashboard tab , which had all the information about the provisioned instance in it.","title":"Deploy Liberty for z/OS Using the z/OS Cloud Broker"},{"location":"lab003/lab003-3/#deploy-liberty-for-zos-using-the-zos-cloud-broker","text":"With the z/OS Cloud Broker and OpenShift, provisioning z/OS resources is as easy as clicking on a tile in the OpenShift Developer Catalog. Enter the Developer Perspective , if you aren\u2019t there already. Make sure that you\u2019re working under the z/OS Cloud Broker project atg-zoscb . Important Unlike other labs, this lab uses a shared project for all lab attendees. Please pay close attention to naming conventions so you do not end up deleting other attendees\u2019 provisioned services . Click the +Add button in the left-side menu . Click the All Services option under the Developer Catalog section . Search the catalog for Liberty for z/OS and click on it . Click the Create button at the bottom of the page. Information This Liberty service does not live inside this OpenShift cluster. It is, in fact, a template for a z/OS Liberty instance that z/OSMF has found and displayed. When you provision an instance, it will spin up the service in a completely different z/OS LPAR separate from the Linux LPAR where this OpenShift cluster is running. All of the required fields will automatically populate for you, but rename the wlp service to userNN-wlp where NN is your user number. Important Please double check that you have correctly typed your user number for userNN. Remember that you are using a shared project for this lab, and nothing is stopping you from interfering with another lab participant's provisioned service if you use the wrong name. Click the create button . You will be brought to the topology page. After you click create, you will need to navigate to the service instance page: Switch to the Administrator Perspective -> Operators in the menu bar -> Installed Operators -> Liberty for z/OS -> Liberty for z/OS tab -> Click on the instance with your user number NN . You will end up on a screen that looks like the following: Notice two things: Depending on how quickly you navigated to this page and how long the WLP instance takes to provision, your status will be either Pending or Succeeded . Once it\u2019s Succeeded , you will have a link to your Dashboard. OpenShift is telling you that the service is either provisioned in z/OS, or in the process of being provisioned. While you don\u2019t have access to z/OSMF, the following is what you would see over in the z/OSMF console: If you want to look at the z/OSMF console, ask an instructor and they will give you a tour. This service will take a minute or two to provision. Wait until you see the following messages on the service instance page: Over on z/OSMF page again, this is what one would see: And in z/OS itself, the following task is started: You have just successfully provisioned a Liberty instance on z/OS, without leaving the OpenShift console. From the OpenShift WLP instance page, click the Dashboard URL hyperlink . Click the Log in with OpenShift button . You might get a security challenge here. If you do, make sure that both of the two checkboxes are checked, and click Allow Selected Permissions . You will be taken to the dashboard for your z/OS Liberty instance. This page will be referred to as the Dashboard tab. The right side of the page contains information about your WLP service and the z/OS system it\u2019s running on. The left side of the page contains buttons you can use to perform various actions. You will use a few of them shortly. Scroll to the bottom of the right-hand column, and locate the IP_ADDRESS variable . Hint It's 192.168.176.154 . That\u2019s the IP address of the z/OS system on which z/OSMF and Liberty for z/OS are hosted. Scroll up a bit and locate the HTTP_PORT variable . It\u2019s just about in the middle of the column. Hint It's something like 9XXX , where XXX will be unique for each user. Keeping the Dashboard tab open, open a new browser tab . In the new tab, navigate to <IP_ADDRESS>:<HTTP_PORT> Hint It will look something like 192.168.176.154:9XXX , where the XXX is unique for each user. You should see the default Liberty homepage. This is the Liberty service you just provisioned on z/OS. Staying in this \u201cWelcome to Liberty\u201d tab, add the following string to the end of the URL : /CloudTestServlet Press enter . That will take you to a sample application that was deployed into the Liberty z/OS instance you provisioned. You will see something like this: Note the date and timestamp. It should be the current time (in U.S. Eastern time format). Reload the browser tab . You should see the time-stamp change. Do not close this tab . Return to the Dashboard tab , which had all the information about the provisioned instance in it.","title":"Deploy Liberty for z/OS Using the z/OS Cloud Broker"},{"location":"lab003/lab003-4/","text":"Stop and Restart Liberty for z/OS from OCP \u00b6 In the Dashboard tab, you should see the following on the left side of the screen: Click the \"Run\" button that's associated with \"Stop\" . Click the \u201cAction History\u201d button above . Depending how quickly you click on this button, you\u2019ll see either: If the stop is in progress, or: if the stop completed before you looked at the Action History. Over in z/OSMF, one would see: Once you see in the Action History that the stop has completed, go back to the tab where the timestamp application was ( 192.168.176.154:9XXX , if you accidentally closed it). Reload this page . You will see the following: Go back to the Dashboard tab and click the \u201cRun\u201d button that\u2019s associated with \u201cStart\u201d . This will trigger a workflow over in z/OSMF to start the server. Click \u201cAction History\u201d and refresh until you see that the Start workflow is complete . Go back to the tab with the timestamp application and reload the page . You should see the time and date with the current time shown: This indicates that the server is back up and serving pages","title":"Stop and Restart Liberty for z/OS from OCP"},{"location":"lab003/lab003-4/#stop-and-restart-liberty-for-zos-from-ocp","text":"In the Dashboard tab, you should see the following on the left side of the screen: Click the \"Run\" button that's associated with \"Stop\" . Click the \u201cAction History\u201d button above . Depending how quickly you click on this button, you\u2019ll see either: If the stop is in progress, or: if the stop completed before you looked at the Action History. Over in z/OSMF, one would see: Once you see in the Action History that the stop has completed, go back to the tab where the timestamp application was ( 192.168.176.154:9XXX , if you accidentally closed it). Reload this page . You will see the following: Go back to the Dashboard tab and click the \u201cRun\u201d button that\u2019s associated with \u201cStart\u201d . This will trigger a workflow over in z/OSMF to start the server. Click \u201cAction History\u201d and refresh until you see that the Start workflow is complete . Go back to the tab with the timestamp application and reload the page . You should see the time and date with the current time shown: This indicates that the server is back up and serving pages","title":"Stop and Restart Liberty for z/OS from OCP"},{"location":"lab003/lab003-5/","text":"Cleaning Up \u00b6 Close the tab with the timestamp application . Close the Dashboard tab . Navigate back to your userNN-wlp instance Hint Administrator -> Operators -> Installed Operators -> Liberty for z/OS -> Liberty for z/OS tab Click the three dots to the far right of your provisioned service and click Delete WLP . Over in z/OSMF, that will trigger a de-provision operation: When the operation is complete, you will see On z/OSMF, the Liberty z/OS server instance has been de-provisioned, which means it was stopped and the file system location for the server instance removed.","title":"Cleaning Up"},{"location":"lab003/lab003-5/#cleaning-up","text":"Close the tab with the timestamp application . Close the Dashboard tab . Navigate back to your userNN-wlp instance Hint Administrator -> Operators -> Installed Operators -> Liberty for z/OS -> Liberty for z/OS tab Click the three dots to the far right of your provisioned service and click Delete WLP . Over in z/OSMF, that will trigger a de-provision operation: When the operation is complete, you will see On z/OSMF, the Liberty z/OS server instance has been de-provisioned, which means it was stopped and the file system location for the server instance removed.","title":"Cleaning Up"},{"location":"lab004/lab004-1/","text":"Deploying an Application from Source Code \u00b6 OpenShift is designed for users with various responsibilities, backgrounds and skillsets. Most broadly, OpenShift is designed for two main groups \u2013 administrators and developers . Furthermore, there are different types of administrators, and different types of developers. As much of the Information Technology world moves toward cloud technology as the consumption model for enterprise computing, developers are required to make a shift in the tools they use to perform their work. At the heart of almost every cloud platform there are two of these new, core technologies \u2013 containers and container orchestrators . Note We won\u2019t be specifically covering these technologies in this lab, but you\u2019ve probably heard of them. Docker is the most popular container runtime, and Kubernetes is the most popular container orchestrator. For the curious, OpenShift replaced Docker containers with CRI-O containers when moving from v3.11 to v4.1 (although Docker containers will still work in OpenShift 4.x). However, not every developer wants (or needs) to learn these new technologies in order to take advantage of them. In fact, OpenShift enables developers with no container experience at all to simply provide their source code (written in Javascript, Python, Go, etc.) and let OpenShift build the container for them using its Source-to-Image (S2I) capability . OpenShift's S2I capability allows developers to focus on developing their application and leaves the containerization process to OpenShift. Using the S2I tooling and Builder Images loaded into the OpenShift image registry, the developer does not need to create a Dockerfile, use any podman or docker commands, or do anything else that is usually required to make a container image out of application source code.","title":"Source-to-Image (S2I) Overview"},{"location":"lab004/lab004-1/#deploying-an-application-from-source-code","text":"OpenShift is designed for users with various responsibilities, backgrounds and skillsets. Most broadly, OpenShift is designed for two main groups \u2013 administrators and developers . Furthermore, there are different types of administrators, and different types of developers. As much of the Information Technology world moves toward cloud technology as the consumption model for enterprise computing, developers are required to make a shift in the tools they use to perform their work. At the heart of almost every cloud platform there are two of these new, core technologies \u2013 containers and container orchestrators . Note We won\u2019t be specifically covering these technologies in this lab, but you\u2019ve probably heard of them. Docker is the most popular container runtime, and Kubernetes is the most popular container orchestrator. For the curious, OpenShift replaced Docker containers with CRI-O containers when moving from v3.11 to v4.1 (although Docker containers will still work in OpenShift 4.x). However, not every developer wants (or needs) to learn these new technologies in order to take advantage of them. In fact, OpenShift enables developers with no container experience at all to simply provide their source code (written in Javascript, Python, Go, etc.) and let OpenShift build the container for them using its Source-to-Image (S2I) capability . OpenShift's S2I capability allows developers to focus on developing their application and leaves the containerization process to OpenShift. Using the S2I tooling and Builder Images loaded into the OpenShift image registry, the developer does not need to create a Dockerfile, use any podman or docker commands, or do anything else that is usually required to make a container image out of application source code.","title":"Deploying an Application from Source Code"},{"location":"lab004/lab004-2/","text":"Exploring GitHub and the Example Health Source Code \u00b6 In this lab, you will be deploying a sample healthcare application called Example Health . The application is written in JavaScript, and it\u2019s loaded into IBM\u2019s GitHub repository. In Firefox, navigate to https://github.com/IBM/node-s2i-openshift This is an IBM repository that contains everything you need in order to deploy the application \u2013 including a README.md file with information and instructions, additional files required for the source code to work, and the source code itself. Let\u2019s look at the source code now. Open the site folder . Open the app.js file . This is the source code for the frontend application that OpenShift will build into a container. Notice that it is NOT any sort of container image, Dockerfile, or YAML file itself \u2013 rather, it is written in Javascript . Feel free to look through the code. Click on the node-s2i-openshift hyperlink to get back to the main repository page. Your URL should again be https://github.com/IBM/node-s2i-openshift You\u2019ll need to make a fork of this repository so you have your own copy to work with. To do so, you\u2019ll first need to sign into GitHub. Click the Sign In button in the top right . Log in with YOUR OWN GitHub credentials . Note If you don\u2019t have a GitHub account already, please create one and then sign in with it. After a successful login, you will be taken back to the main repository page. Now you can create your own fork of the repository. Click the Fork button on the left side of the page . When complete, you will be taken to your forked repository page Notice that while everything else looks basically the same, the URL has changed from https://github.com/IBM/node-s2i-openshift to: https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift Leave this browser tab open, and open another .","title":"Exploring GitHub and the Example Health Source Code"},{"location":"lab004/lab004-2/#exploring-github-and-the-example-health-source-code","text":"In this lab, you will be deploying a sample healthcare application called Example Health . The application is written in JavaScript, and it\u2019s loaded into IBM\u2019s GitHub repository. In Firefox, navigate to https://github.com/IBM/node-s2i-openshift This is an IBM repository that contains everything you need in order to deploy the application \u2013 including a README.md file with information and instructions, additional files required for the source code to work, and the source code itself. Let\u2019s look at the source code now. Open the site folder . Open the app.js file . This is the source code for the frontend application that OpenShift will build into a container. Notice that it is NOT any sort of container image, Dockerfile, or YAML file itself \u2013 rather, it is written in Javascript . Feel free to look through the code. Click on the node-s2i-openshift hyperlink to get back to the main repository page. Your URL should again be https://github.com/IBM/node-s2i-openshift You\u2019ll need to make a fork of this repository so you have your own copy to work with. To do so, you\u2019ll first need to sign into GitHub. Click the Sign In button in the top right . Log in with YOUR OWN GitHub credentials . Note If you don\u2019t have a GitHub account already, please create one and then sign in with it. After a successful login, you will be taken back to the main repository page. Now you can create your own fork of the repository. Click the Fork button on the left side of the page . When complete, you will be taken to your forked repository page Notice that while everything else looks basically the same, the URL has changed from https://github.com/IBM/node-s2i-openshift to: https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift Leave this browser tab open, and open another .","title":"Exploring GitHub and the Example Health Source Code"},{"location":"lab004/lab004-3/","text":"Connect to OCP and Authenticate \u00b6 In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP and Authenticate"},{"location":"lab004/lab004-3/#connect-to-ocp-and-authenticate","text":"In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP and Authenticate"},{"location":"lab004/lab004-4/","text":"Edit the Source Code and Push an Update \u00b6 Switch to the Developer Perspective , if not already on it. Change into your userNN-project if not already in it. Click the +Add button from the left-side menu . Click the From Git option in the Git Repository section of the +Add page. In the Git Repo URL field, enter the URL of your forked repository . It will look something like: https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift Click the Show Advanced Git Options hyperlink . In the Context Dir field, enter /site . Recall This is the folder in the GitHub repository that you dug into to view the source code. For Builder Image, select the Node.js tile . It is likely that OpenShift will detect the programming language in the GitHub repository and automatically select the Node.js builder image. Scroll to the bottom of this page and click the Create button . You will be taken to the Topology page, which will show your new application along with three smaller circular buttons that can be used to perform different actions against the application. Click the circular Node.js application icon . At first, the icon will be all white and you will see \u201cBuild #1 is running\u201d in the righthand panel. If you wish, you can watch the logs for the running Build to see everything it\u2019s doing. After a minute or two, the icon will have a green check mark next to it, indicating the Build is complete. Once the Build is complete, your Pod will be created. You can also watch the logs for this, if you wish. About 10 seconds later, a solid blue ring will appear around the edge of the circular icon, indicating that the pod is up and running. Note Feel free to click on the View Logs hyperlink to watch everything that the build is creating. When complete, the log will display Push Successful , and you can return to the Topology page by clicking the link on the left side of the page. If you clicked off of the Node.js application Resources tab, click on the circular icon again, then click on the Resources tab. Click the Route URL \u2013 beginning with http:// You will be taken to the Example Health application. Log into the Example Health application using the following credentials : Username: test Password: test All of the data in this application is simulated to look similar to the health records of an insurance company. Feel free to explore the application and notice the multiple tabs it contains. The JavaScript code you looked at in your forked GitHub repository was containerized by OpenShift\u2019s S2I function and deployed onto the OpenShift cluster. Now that your application is running in a container in OpenShift, let\u2019s see how an application developer can make a change to the source code, and then push the update to the running application. We\u2019ll make a simple change in a few lines of text to demonstrate. As we see in the Example Health application, there is a section with Personal Information. In your forked GitHub repository, navigate to the source code again : Make sure you are in your own fork of the repository . From /node-s2i-openshift>, click on the site folder . Click on the app.js file . With the app.js file open, click on the edit button pointed out in the picture below. Scroll down to line 55 , which displays the patient name. Edit lines 55-60 as you wish, modifying the text strings for Name, Age, Gender, etc . Click Commit Changes at the bottom of the page. You just edited the source code, but you still need to push the update to the running application. Back in the OpenShift console, navigate to the Topology page -> Click the Node.js application icon -> Click the Resources tab -> Click the Start Build button . A new Build #2 will be created. As with the first build, you can view the build logs to watch everything it\u2019s doing, or you can simply wait for the console to display Build #2 is complete and your Pod Running . When the Pod is running , refresh the Example Health browser tab . Your code changes have been pushed to the running Example Health application.","title":"Edit the Source Code and Push an Update"},{"location":"lab004/lab004-4/#edit-the-source-code-and-push-an-update","text":"Switch to the Developer Perspective , if not already on it. Change into your userNN-project if not already in it. Click the +Add button from the left-side menu . Click the From Git option in the Git Repository section of the +Add page. In the Git Repo URL field, enter the URL of your forked repository . It will look something like: https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift Click the Show Advanced Git Options hyperlink . In the Context Dir field, enter /site . Recall This is the folder in the GitHub repository that you dug into to view the source code. For Builder Image, select the Node.js tile . It is likely that OpenShift will detect the programming language in the GitHub repository and automatically select the Node.js builder image. Scroll to the bottom of this page and click the Create button . You will be taken to the Topology page, which will show your new application along with three smaller circular buttons that can be used to perform different actions against the application. Click the circular Node.js application icon . At first, the icon will be all white and you will see \u201cBuild #1 is running\u201d in the righthand panel. If you wish, you can watch the logs for the running Build to see everything it\u2019s doing. After a minute or two, the icon will have a green check mark next to it, indicating the Build is complete. Once the Build is complete, your Pod will be created. You can also watch the logs for this, if you wish. About 10 seconds later, a solid blue ring will appear around the edge of the circular icon, indicating that the pod is up and running. Note Feel free to click on the View Logs hyperlink to watch everything that the build is creating. When complete, the log will display Push Successful , and you can return to the Topology page by clicking the link on the left side of the page. If you clicked off of the Node.js application Resources tab, click on the circular icon again, then click on the Resources tab. Click the Route URL \u2013 beginning with http:// You will be taken to the Example Health application. Log into the Example Health application using the following credentials : Username: test Password: test All of the data in this application is simulated to look similar to the health records of an insurance company. Feel free to explore the application and notice the multiple tabs it contains. The JavaScript code you looked at in your forked GitHub repository was containerized by OpenShift\u2019s S2I function and deployed onto the OpenShift cluster. Now that your application is running in a container in OpenShift, let\u2019s see how an application developer can make a change to the source code, and then push the update to the running application. We\u2019ll make a simple change in a few lines of text to demonstrate. As we see in the Example Health application, there is a section with Personal Information. In your forked GitHub repository, navigate to the source code again : Make sure you are in your own fork of the repository . From /node-s2i-openshift>, click on the site folder . Click on the app.js file . With the app.js file open, click on the edit button pointed out in the picture below. Scroll down to line 55 , which displays the patient name. Edit lines 55-60 as you wish, modifying the text strings for Name, Age, Gender, etc . Click Commit Changes at the bottom of the page. You just edited the source code, but you still need to push the update to the running application. Back in the OpenShift console, navigate to the Topology page -> Click the Node.js application icon -> Click the Resources tab -> Click the Start Build button . A new Build #2 will be created. As with the first build, you can view the build logs to watch everything it\u2019s doing, or you can simply wait for the console to display Build #2 is complete and your Pod Running . When the Pod is running , refresh the Example Health browser tab . Your code changes have been pushed to the running Example Health application.","title":"Edit the Source Code and Push an Update"},{"location":"lab004/lab004-5/","text":"Cleaning Up \u00b6 There is no easy way to delete all of these objects from the OpenShift console. This is a much easier task in the OpenShift command line. In the OpenShift CLI, make sure you are in your own project (i.e. userNN-project) and run the following command : oc delete all --selector app=node-s-2-i-openshift -o name Note If you are not connected to the OpenShift command line, refer to Using the OpenShift Command Line . In this lab, you have exposed JavaScript source code in a GitHub repository to an OpenShift cluster, which containerized that JavaScript code into a container image, and then deployed it as a container running in a pod. You then made a code change to the JavaScript code and pushed an update to the application while it was running","title":"Cleaning Up"},{"location":"lab004/lab004-5/#cleaning-up","text":"There is no easy way to delete all of these objects from the OpenShift console. This is a much easier task in the OpenShift command line. In the OpenShift CLI, make sure you are in your own project (i.e. userNN-project) and run the following command : oc delete all --selector app=node-s-2-i-openshift -o name Note If you are not connected to the OpenShift command line, refer to Using the OpenShift Command Line . In this lab, you have exposed JavaScript source code in a GitHub repository to an OpenShift cluster, which containerized that JavaScript code into a container image, and then deployed it as a container running in a pod. You then made a code change to the JavaScript code and pushed an update to the application while it was running","title":"Cleaning Up"},{"location":"lab005/lab005-1/","text":"Monitoring, Metering, and Metrics \u00b6 A significant architectural shift toward containers is underway and, as with any architectural shift, this brings new operational challenges. It can be challenging for many of the legacy monitoring tools to monitor container platforms in fast moving, often ephemeral environments. The good news is newer cloud-based offerings can ensure monitoring solutions are as scalable as the services being built and monitored. These new solutions have evolved to address the growing need to monitor your stack from the bottom to the top. From an operations point of view, infrastructure monitoring tools collect metrics about the host or container, such as CPU load, available memory and network I/O. The default monitoring stack is the 3-pronged open source approach of, Grafana, Alertmanager, and Prometheus. Prometheus gives you finely grained metrics at a huge scale. With the right configuration, Prometheus can handle millions of time series. Grafana can visualize the data being scraped by Prometheus. Grafana comes with pre-built dashboards for typical use cases, or you can create your own custom ones. Alertmanager forwards alerts to a service such as Slack or another webhook . Alertmanager can use metadata to classify alerts into groups such as errors, notifications, etc. The Grafana-Alertmanager-Prometheus monitoring stack provides a highly configurable, open source option to monitor Kubernetes workloads.","title":"Overview of OpenShift Monitoring"},{"location":"lab005/lab005-1/#monitoring-metering-and-metrics","text":"A significant architectural shift toward containers is underway and, as with any architectural shift, this brings new operational challenges. It can be challenging for many of the legacy monitoring tools to monitor container platforms in fast moving, often ephemeral environments. The good news is newer cloud-based offerings can ensure monitoring solutions are as scalable as the services being built and monitored. These new solutions have evolved to address the growing need to monitor your stack from the bottom to the top. From an operations point of view, infrastructure monitoring tools collect metrics about the host or container, such as CPU load, available memory and network I/O. The default monitoring stack is the 3-pronged open source approach of, Grafana, Alertmanager, and Prometheus. Prometheus gives you finely grained metrics at a huge scale. With the right configuration, Prometheus can handle millions of time series. Grafana can visualize the data being scraped by Prometheus. Grafana comes with pre-built dashboards for typical use cases, or you can create your own custom ones. Alertmanager forwards alerts to a service such as Slack or another webhook . Alertmanager can use metadata to classify alerts into groups such as errors, notifications, etc. The Grafana-Alertmanager-Prometheus monitoring stack provides a highly configurable, open source option to monitor Kubernetes workloads.","title":"Monitoring, Metering, and Metrics"},{"location":"lab005/lab005-2/","text":"Connect to OCP and Authenticate \u00b6 In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP and Authenticate"},{"location":"lab005/lab005-2/#connect-to-ocp-and-authenticate","text":"In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP and Authenticate"},{"location":"lab005/lab005-3/","text":"Using OpenShifft Metrics (Prometheus) \u00b6 OpenShift provides a web interface to Prometheus , which enables you to run Prometheus Query Language (PromQL) queries and visualize the metrics on a plot. This functionality provides an extensive overview of the cluster state and helps to troubleshoot problems. In the OpenShift console, switch to the Administrator perspective if you are not already on it. In the menu bar on the left side of the page, click Monitoring and then Metrics . You will be taken to a Prometheus interface within the OpenShift console. Once you enter a query, the graph will populate. Click the Insert Metric at Cursor dropdown and enter the following string in the new query bar : namespace:container_memory_usage_bytes:sum Click the associated query result that is returned . The string will populate the query text box. Click the blue \"Run Queries\" button . The graph should now display the memory usage over time for each namespace. Scroll down the page to the table displaying each namespace and its memory usage in bytes. Note You table will look different depending on what work is being done in the OpenShift cluster at the time. OpenShift passes around a massive amount of data to run itself and the applications running on top of it. Prometheus is an extremely powerful data source that can return results for millions of time strings with extremely granular precision. Because of OpenShift\u2019s vast data production and Prometheus\u2019 ability to process it, certain queries can produce simply too much data to be useful. Because Prometheus makes use of labels, we can use these labels to filter data to make better sense of it. Modify your query to the following : namespace:container_memory_usage_bytes:sum{namespace=\"userNN-project\"} Important Make sure you change the one instance of NN to your user number. Also, notice that they are squiggly brackets {} in the query, not regular parentheses. Click Run Queries Your graph is now displaying the memory usage over time for your own project. If you see a \u201cNo datapoints found\u201d message, select a longer timeframe using the dropdown menu in the top left of the graph. Note If you skipped ahead to this lab without completing the others, it\u2019s possible that your project has not had workload deployed in it for more than the maximum time frame. If this is the case, run a simple application in your project, and you will see the data start to populate (refer to Exploring the OpenShift Console for help with this.) As you might have noticed, working directly with Prometheus can be tedious and requires specific PromQL queries that aren\u2019t the easiest to work with. That\u2019s why people typically use Prometheus for its data source functionality, and then move to Grafana for the data visualization .","title":"Using OpenShift Metrics (Prometheus)"},{"location":"lab005/lab005-3/#using-openshifft-metrics-prometheus","text":"OpenShift provides a web interface to Prometheus , which enables you to run Prometheus Query Language (PromQL) queries and visualize the metrics on a plot. This functionality provides an extensive overview of the cluster state and helps to troubleshoot problems. In the OpenShift console, switch to the Administrator perspective if you are not already on it. In the menu bar on the left side of the page, click Monitoring and then Metrics . You will be taken to a Prometheus interface within the OpenShift console. Once you enter a query, the graph will populate. Click the Insert Metric at Cursor dropdown and enter the following string in the new query bar : namespace:container_memory_usage_bytes:sum Click the associated query result that is returned . The string will populate the query text box. Click the blue \"Run Queries\" button . The graph should now display the memory usage over time for each namespace. Scroll down the page to the table displaying each namespace and its memory usage in bytes. Note You table will look different depending on what work is being done in the OpenShift cluster at the time. OpenShift passes around a massive amount of data to run itself and the applications running on top of it. Prometheus is an extremely powerful data source that can return results for millions of time strings with extremely granular precision. Because of OpenShift\u2019s vast data production and Prometheus\u2019 ability to process it, certain queries can produce simply too much data to be useful. Because Prometheus makes use of labels, we can use these labels to filter data to make better sense of it. Modify your query to the following : namespace:container_memory_usage_bytes:sum{namespace=\"userNN-project\"} Important Make sure you change the one instance of NN to your user number. Also, notice that they are squiggly brackets {} in the query, not regular parentheses. Click Run Queries Your graph is now displaying the memory usage over time for your own project. If you see a \u201cNo datapoints found\u201d message, select a longer timeframe using the dropdown menu in the top left of the graph. Note If you skipped ahead to this lab without completing the others, it\u2019s possible that your project has not had workload deployed in it for more than the maximum time frame. If this is the case, run a simple application in your project, and you will see the data start to populate (refer to Exploring the OpenShift Console for help with this.) As you might have noticed, working directly with Prometheus can be tedious and requires specific PromQL queries that aren\u2019t the easiest to work with. That\u2019s why people typically use Prometheus for its data source functionality, and then move to Grafana for the data visualization .","title":"Using OpenShifft Metrics (Prometheus)"},{"location":"lab005/lab005-4/","text":"Using the In-Browser Grafana Dashboards \u00b6 From the OpenShift menu, navigate to Monitoring -> Dashboards . This takes you to an in-browser user interface for the Grafana monitoring solution. By default, there are various preconfigured dashboards for common use cases. Click the \"Dashboard\" dropdown in the top-left of the page, and select another that is of interest to you .","title":"Using the In-Browser Grafana Dashboards"},{"location":"lab005/lab005-4/#using-the-in-browser-grafana-dashboards","text":"From the OpenShift menu, navigate to Monitoring -> Dashboards . This takes you to an in-browser user interface for the Grafana monitoring solution. By default, there are various preconfigured dashboards for common use cases. Click the \"Dashboard\" dropdown in the top-left of the page, and select another that is of interest to you .","title":"Using the In-Browser Grafana Dashboards"},{"location":"lab005/lab005-5/","text":"Connect to Grafana \u00b6 To utilize further Grafana functions, navigate to the Grafana UI at the following address . https://grafana-openshift-monitoring.apps.atsocppa.dmz/ Expand for More Information Where is this URL coming from? It is exposed service (or route) for the Grafana service. You could open a terminal and run the following command to find the URLs to Prometheus, Grafana, and Alertmanager: root # ===> oc -n openshift-monitoring get routes NAME HOST/PORT alertmanager-main alertmanager-main-openshift-monitoring.apps.atsocppa.dmz grafana grafana-openshift-monitoring.apps.atsocppa.dmz prometheus-k8s prometheus-k8s-openshift-monitoring.apps.atsocppa.dmz Information You might see a security challenge if the cluster has not yet been accessed from your workstation. Accept the challenge to continue. You should now see login page prompting your OpenShift credentials. Log into Grafana using the your OpenShift credentials . Username: userNN Password: p@ssw0rd Notice that the credentials you use to log into Grafana are the same as those you use to log into OpenShift itself. OpenShift\u2019s role-bases access control (RBAC) functionality extends to the monitoring stack, so administrators can control who can see this part of the environment.","title":"Connect to Grafana"},{"location":"lab005/lab005-5/#connect-to-grafana","text":"To utilize further Grafana functions, navigate to the Grafana UI at the following address . https://grafana-openshift-monitoring.apps.atsocppa.dmz/ Expand for More Information Where is this URL coming from? It is exposed service (or route) for the Grafana service. You could open a terminal and run the following command to find the URLs to Prometheus, Grafana, and Alertmanager: root # ===> oc -n openshift-monitoring get routes NAME HOST/PORT alertmanager-main alertmanager-main-openshift-monitoring.apps.atsocppa.dmz grafana grafana-openshift-monitoring.apps.atsocppa.dmz prometheus-k8s prometheus-k8s-openshift-monitoring.apps.atsocppa.dmz Information You might see a security challenge if the cluster has not yet been accessed from your workstation. Accept the challenge to continue. You should now see login page prompting your OpenShift credentials. Log into Grafana using the your OpenShift credentials . Username: userNN Password: p@ssw0rd Notice that the credentials you use to log into Grafana are the same as those you use to log into OpenShift itself. OpenShift\u2019s role-bases access control (RBAC) functionality extends to the monitoring stack, so administrators can control who can see this part of the environment.","title":"Connect to Grafana"},{"location":"lab005/lab005-6/","text":"Using Grafana Dashboards \u00b6 Once logged into Grafana, you\u2019ll be taken to the Home Dashboard from which you can navigate to your starred or recently viewed dashboards. You can also install various types of plugins from the official Grafana list and also from third-party sources from this page. Click the Home dropdown in the top-left corner, and expand the Default dashboards if they aren\u2019t already visible . A list of the recent and pre-installed dashboards will pop up. You may or may not see any recent dashboards, depending on previous usage of your userNN credentials. Notice that you can search dashboards by keyword in the search bar up top, or filter by labels on the right side. Click the Kubernetes / Compute Resources / Cluster link in the General tab. You will see a dashboard populated with information related to the cluster\u2019s compute resources such as CPU and memory utilization. This dashboard displays CPU usage and CPU quota/memory requests by namespace. Click the CPU Usage dropdown above the first graph in this dashboard and click View . Information Alternatively, you can click on the CPU Usage graph to activate it, and hit the V key on your keyboard. This will bring up a full screen view of the graph to more easily see details. Hover your cursor over the graph various points to see details for a certain namespace at a specific point in time . This still might be difficult to target a specific namespace, especially for the namespaces that aren\u2019t using much CPU. Click a namespaces in the chart\u2019s legend , such as atg-zoscb or openshift-apiserver . Hold the Shift key and click a few more namespaces . This will display only the CPU usage for the few namespaces you selected. Click the Share dashboard button in the top right of the page . From here, you can share a snapshot of the graph either internally or externally. When creating a snapshot to share externally, sensitive data will be stripped. Note If you try to share or export a graph here, you will find that it\u2019s unsuccessful. The userNN profiles have administrator-viewer credentials, so you are limited to the features you can actually change. A profile with full cluster administrator authority would have more access to Grafana functions such as sharing graphs and snapshots, creating their own custom dashboards, editing and saving pre-built dashboards, and installing various plugins and other tools that extend Grafana\u2019s built-in features. Close this browser window when you are ready to move on .","title":"Using Grafana Dashboards"},{"location":"lab005/lab005-6/#using-grafana-dashboards","text":"Once logged into Grafana, you\u2019ll be taken to the Home Dashboard from which you can navigate to your starred or recently viewed dashboards. You can also install various types of plugins from the official Grafana list and also from third-party sources from this page. Click the Home dropdown in the top-left corner, and expand the Default dashboards if they aren\u2019t already visible . A list of the recent and pre-installed dashboards will pop up. You may or may not see any recent dashboards, depending on previous usage of your userNN credentials. Notice that you can search dashboards by keyword in the search bar up top, or filter by labels on the right side. Click the Kubernetes / Compute Resources / Cluster link in the General tab. You will see a dashboard populated with information related to the cluster\u2019s compute resources such as CPU and memory utilization. This dashboard displays CPU usage and CPU quota/memory requests by namespace. Click the CPU Usage dropdown above the first graph in this dashboard and click View . Information Alternatively, you can click on the CPU Usage graph to activate it, and hit the V key on your keyboard. This will bring up a full screen view of the graph to more easily see details. Hover your cursor over the graph various points to see details for a certain namespace at a specific point in time . This still might be difficult to target a specific namespace, especially for the namespaces that aren\u2019t using much CPU. Click a namespaces in the chart\u2019s legend , such as atg-zoscb or openshift-apiserver . Hold the Shift key and click a few more namespaces . This will display only the CPU usage for the few namespaces you selected. Click the Share dashboard button in the top right of the page . From here, you can share a snapshot of the graph either internally or externally. When creating a snapshot to share externally, sensitive data will be stripped. Note If you try to share or export a graph here, you will find that it\u2019s unsuccessful. The userNN profiles have administrator-viewer credentials, so you are limited to the features you can actually change. A profile with full cluster administrator authority would have more access to Grafana functions such as sharing graphs and snapshots, creating their own custom dashboards, editing and saving pre-built dashboards, and installing various plugins and other tools that extend Grafana\u2019s built-in features. Close this browser window when you are ready to move on .","title":"Using Grafana Dashboards"},{"location":"lab005/lab005-7/","text":"Using OpenShift Alerts with Alertmanager \u00b6 Alerting with Prometheus is separated into two parts. Alerting rules in Prometheus send alerts to Alertmanager . Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email or chat platforms like Slack. An example rules file with an alert would be: groups : - name : example rules : - alert : HighRequestLatency expr : job:request_latency_seconds:mean5m{job=\"myjob\"} > 0.5 for : 10m labels : severity : page annotations : summary : High request latency The optional for clause causes Prometheus to wait for a certain duration between first encountering a new expression output vector element and counting an alert as firing for this element. In this case, Prometheus will check that the alert continues to be active during each evaluation for 10 minutes before firing the alert. Elements that are active, but not firing yet, are in the pending state. The labels clause allows specifying a set of additional labels to be attached to the alert. Any existing conflicting labels will be overwritten. The annotations clause specifies a set of informational labels that can be used to store longer additional information such as alert descriptions or runbook links. In the menu bar on the left side of the OpenShift console, click Monitoring and then Alerting . You will be taken to an Alertmanager interface within the OpenShift console. Click the Alerting Rules tab to see the 100+ alerts that are not currently firing (hopefully!) These alerts come pre-built with the monitoring stack, and they will start firing if triggered. This list includes alerts for critical operators going down, pods crash-looping, nodes being unreachable, and many more. Feel free to look through them.","title":"Using OpenShift Alerts (Alertmanager)"},{"location":"lab005/lab005-7/#using-openshift-alerts-with-alertmanager","text":"Alerting with Prometheus is separated into two parts. Alerting rules in Prometheus send alerts to Alertmanager . Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email or chat platforms like Slack. An example rules file with an alert would be: groups : - name : example rules : - alert : HighRequestLatency expr : job:request_latency_seconds:mean5m{job=\"myjob\"} > 0.5 for : 10m labels : severity : page annotations : summary : High request latency The optional for clause causes Prometheus to wait for a certain duration between first encountering a new expression output vector element and counting an alert as firing for this element. In this case, Prometheus will check that the alert continues to be active during each evaluation for 10 minutes before firing the alert. Elements that are active, but not firing yet, are in the pending state. The labels clause allows specifying a set of additional labels to be attached to the alert. Any existing conflicting labels will be overwritten. The annotations clause specifies a set of informational labels that can be used to store longer additional information such as alert descriptions or runbook links. In the menu bar on the left side of the OpenShift console, click Monitoring and then Alerting . You will be taken to an Alertmanager interface within the OpenShift console. Click the Alerting Rules tab to see the 100+ alerts that are not currently firing (hopefully!) These alerts come pre-built with the monitoring stack, and they will start firing if triggered. This list includes alerts for critical operators going down, pods crash-looping, nodes being unreachable, and many more. Feel free to look through them.","title":"Using OpenShift Alerts with Alertmanager"},{"location":"lab006/lab006-1/","text":"Using Persistent Storage - MongoDB and NodeJS \u00b6 In production Kubernetes clusters, applications need to write data to storage where it will persist even if the application pods go down. In this lab, we\u2019ll see how that\u2019s done using Persistent Volumes and Persistent Volume Claims. OpenShift on IBM Z supports various types of persistent storage, including Spectrum Scale, OpenShift Container Storage, and NFS, which is what this cluster uses. Before the start of the workshop, persistent volumes were defined in OpenShift, more than enough for one per user. Each persistent volume definition maps to the NFS server. In this lab, you will deploy an application consisting of two components, a containerized Node.js web application and a containerized MongoDB instance, which you will back with persistent storage. Using the Node.js web application, you will be able to query the database, as well as insert new data into it. To deploy the Node.js application , you will build and run the container from a Dockerfile residing in a GitHub repository. To deploy MongoDB , you will pull a MongoDB image from and run it. The image in quay.io is the official MongoDB container image pulled from Docker Hub and moved to the Quay registry. This was done simply because Dockerhub has rate limits on pull requests from their public repository.","title":"Overview of Persistent Storage and Application Architecture"},{"location":"lab006/lab006-1/#using-persistent-storage-mongodb-and-nodejs","text":"In production Kubernetes clusters, applications need to write data to storage where it will persist even if the application pods go down. In this lab, we\u2019ll see how that\u2019s done using Persistent Volumes and Persistent Volume Claims. OpenShift on IBM Z supports various types of persistent storage, including Spectrum Scale, OpenShift Container Storage, and NFS, which is what this cluster uses. Before the start of the workshop, persistent volumes were defined in OpenShift, more than enough for one per user. Each persistent volume definition maps to the NFS server. In this lab, you will deploy an application consisting of two components, a containerized Node.js web application and a containerized MongoDB instance, which you will back with persistent storage. Using the Node.js web application, you will be able to query the database, as well as insert new data into it. To deploy the Node.js application , you will build and run the container from a Dockerfile residing in a GitHub repository. To deploy MongoDB , you will pull a MongoDB image from and run it. The image in quay.io is the official MongoDB container image pulled from Docker Hub and moved to the Quay registry. This was done simply because Dockerhub has rate limits on pull requests from their public repository.","title":"Using Persistent Storage - MongoDB and NodeJS"},{"location":"lab006/lab006-2/","text":"Connect to OCP and Authenticate \u00b6 In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP and Authenticate"},{"location":"lab006/lab006-2/#connect-to-ocp-and-authenticate","text":"In your virtual machine desktop, open a Firefox web browser . In the browser, navigate to the OpenShift on IBM Z console at the following address: https://console-openshift-console.apps.atsocppa.dmz/ . Note You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being \u201cself-signed\u201d and not yet recognized. Accept the challenge to continue by clicking Advanced and then clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe) . You will likely need to do this twice due to how OpenShift reroutes Oauth requests. Expand for screenshot You will now see the OpenShift console login page. Log in with the OpenShift credentials provided to you on the Lab Assignments page. Hint Your OpenShift credentials will be something like the following: Username: userNN (where NN is your user number) Password: p@ssw0rd","title":"Connect to OCP and Authenticate"},{"location":"lab006/lab006-3/","text":"Create a PersistentVolumeClaim \u00b6 As described in a previous section, a PersistentVolume has been already been predefined for each lab user. Next you will create a PersistentVolumeClaim that will bind to one of the available PersistentVolumes. Change to the Administrator perspective , if not already there. Navigate to the Projects page . You can find it in the Menu, under Home -> Projects. Find and click on your userNN-project . Under the menu button, click Storage -> Persistent Volume Claims . Click \u201cCreate Persistent Volume Claim\u201d . The Create Persistent Volume Claim form has four fields, and you\u2019ll need to manually change each. For Storage Class, select rootsquash-nfs For Persistent Volume Claim Name, change the value to pvc-userNN (Replacing NN with your user #). For Access Mode, select Shared Access (RWX ). For Size, change the value to 2 Gi . Your form should look like the following: Click the create button . You\u2019ll be brought to the Overview for your newly created Persistent Volume Claim. The status of your claim should be Bound . Note If your PersistentVolumeClaim does not bind almost immediately to a PersistentVolume, you likely did not fill out the fields as described above. You can delete your persistent volume claim and try again by clicking on the Actions dropdown and selecting Delete Persistent Volume Claim.","title":"Create a PersistentVolumeClaim"},{"location":"lab006/lab006-3/#create-a-persistentvolumeclaim","text":"As described in a previous section, a PersistentVolume has been already been predefined for each lab user. Next you will create a PersistentVolumeClaim that will bind to one of the available PersistentVolumes. Change to the Administrator perspective , if not already there. Navigate to the Projects page . You can find it in the Menu, under Home -> Projects. Find and click on your userNN-project . Under the menu button, click Storage -> Persistent Volume Claims . Click \u201cCreate Persistent Volume Claim\u201d . The Create Persistent Volume Claim form has four fields, and you\u2019ll need to manually change each. For Storage Class, select rootsquash-nfs For Persistent Volume Claim Name, change the value to pvc-userNN (Replacing NN with your user #). For Access Mode, select Shared Access (RWX ). For Size, change the value to 2 Gi . Your form should look like the following: Click the create button . You\u2019ll be brought to the Overview for your newly created Persistent Volume Claim. The status of your claim should be Bound . Note If your PersistentVolumeClaim does not bind almost immediately to a PersistentVolume, you likely did not fill out the fields as described above. You can delete your persistent volume claim and try again by clicking on the Actions dropdown and selecting Delete Persistent Volume Claim.","title":"Create a PersistentVolumeClaim"},{"location":"lab006/lab006-4/","text":"Deploy MongoDB from a Container Image \u00b6 In this section, you will be deploying a container using a MongoDB container image from quay.io. A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the quay.io registry, Docker Hub, or a private registry of your own. Toggle to the Developer Perspective and ensure you are in the correct userNN-project . Click Add+ from the left-hand menu . Click the Container Images option . This brings up a new page which prompts you for an image name and further configurable parameters further down the page. Only the image name is required, and the rest will automatically populate for you. In the search bar for Image Name from external registry, type quay.io/mmondics/mongo . A green check mark and a \u201cvalidated\u201d message will appear in the search bar, indicating that the MongoDB image has been found and validated in Quay. Important The fields below will automatically populate, but it is imperative that you change the Name field for your MongoDB service , or else the Node.js app will not be able to find and connect to the database. For Application Name, leave the default value . Replace the value of the Name field with mongodb Leave Deployment and Create a Route to the Application checked . Click the Create Button . You will now be taken to the Topology view, where you will see an icon for your new MongoDB deployment. Click the icon for the mongodb deployment . This will bring up a window on the right-hand side of the screen with information about your deployment. Select the Details tab , if not already selected. Depending on how quickly you clicked the icon, it will display either 1 pod, or 0 scaling to 1. If it has not scaled up to 1 pod yet, it will after a few seconds. However, we\u2019re going to be adding and removing storage, so we will kill the pod once it comes up. Click the Down Arrow to reduce the pod count to zero. When a MongoDB pod is created, two storage volumes are attached to it. Let\u2019s examine those. Click the mongodb deployment in the right-side window . Once on the Deployment Details page, scroll down Volumes section . Upon its creation, the pod created two volumes, mongodb-1 and mongodb-2. By default, MongoDB stores data in the /data/db directory, which is where the mongodb-2 volume has been mounted. This volume is not persistent. If the pod gets deleted, all of the stored data will be lost. To make your MongoDB data persistent, you are going to delete mongodb-2 and instead mount your persistent volume claim at /data/db. Click the three dots for the mongodb-2 volume . Click Remove Volume . Now, you\u2019ll add persistent storage, mounting the volume at /data/db. Still on the Deployment Details page, scroll back up to the top and click the Actions dropdown . Click Add Storage . To add storage to your MongoDB deployment, you will need to fill out a couple of fields. Use Existing Claim should already be selected . From the Select Claim dropdown menu, select pvc-userNN . For Mount Path, enter /data/db . For Subpath, enter your userNN , where NN is your user number. Click the Save button . You will now be returned to the Deployment Details page. In the same way that you reduced the pod count to zero, you will now bring it back up to one. Click the Up Arrow to increase the pod count to one . Scroll Down to the section labeled Volumes , and you\u2019ll see the persistent volume now mounted at /data/db Now you\u2019re ready to deploy the Node.js web application.","title":"Deploy MongoDB from Container Image"},{"location":"lab006/lab006-4/#deploy-mongodb-from-a-container-image","text":"In this section, you will be deploying a container using a MongoDB container image from quay.io. A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the quay.io registry, Docker Hub, or a private registry of your own. Toggle to the Developer Perspective and ensure you are in the correct userNN-project . Click Add+ from the left-hand menu . Click the Container Images option . This brings up a new page which prompts you for an image name and further configurable parameters further down the page. Only the image name is required, and the rest will automatically populate for you. In the search bar for Image Name from external registry, type quay.io/mmondics/mongo . A green check mark and a \u201cvalidated\u201d message will appear in the search bar, indicating that the MongoDB image has been found and validated in Quay. Important The fields below will automatically populate, but it is imperative that you change the Name field for your MongoDB service , or else the Node.js app will not be able to find and connect to the database. For Application Name, leave the default value . Replace the value of the Name field with mongodb Leave Deployment and Create a Route to the Application checked . Click the Create Button . You will now be taken to the Topology view, where you will see an icon for your new MongoDB deployment. Click the icon for the mongodb deployment . This will bring up a window on the right-hand side of the screen with information about your deployment. Select the Details tab , if not already selected. Depending on how quickly you clicked the icon, it will display either 1 pod, or 0 scaling to 1. If it has not scaled up to 1 pod yet, it will after a few seconds. However, we\u2019re going to be adding and removing storage, so we will kill the pod once it comes up. Click the Down Arrow to reduce the pod count to zero. When a MongoDB pod is created, two storage volumes are attached to it. Let\u2019s examine those. Click the mongodb deployment in the right-side window . Once on the Deployment Details page, scroll down Volumes section . Upon its creation, the pod created two volumes, mongodb-1 and mongodb-2. By default, MongoDB stores data in the /data/db directory, which is where the mongodb-2 volume has been mounted. This volume is not persistent. If the pod gets deleted, all of the stored data will be lost. To make your MongoDB data persistent, you are going to delete mongodb-2 and instead mount your persistent volume claim at /data/db. Click the three dots for the mongodb-2 volume . Click Remove Volume . Now, you\u2019ll add persistent storage, mounting the volume at /data/db. Still on the Deployment Details page, scroll back up to the top and click the Actions dropdown . Click Add Storage . To add storage to your MongoDB deployment, you will need to fill out a couple of fields. Use Existing Claim should already be selected . From the Select Claim dropdown menu, select pvc-userNN . For Mount Path, enter /data/db . For Subpath, enter your userNN , where NN is your user number. Click the Save button . You will now be returned to the Deployment Details page. In the same way that you reduced the pod count to zero, you will now bring it back up to one. Click the Up Arrow to increase the pod count to one . Scroll Down to the section labeled Volumes , and you\u2019ll see the persistent volume now mounted at /data/db Now you\u2019re ready to deploy the Node.js web application.","title":"Deploy MongoDB from a Container Image"},{"location":"lab006/lab006-5/","text":"Deploy Node.js Application from a Dockerfile \u00b6 In this portion of the lab, you will deploy a Node.js web application created by the ATG. You will be building your deployment from a Dockerfile residing in a GitHub repository. Through the web application, you will be able to insert data into and query the MongoDB database you just created. For this lab the database will store sample name and email pairs displayed as a list of user information in the web UI. First, you need to deploy the application. Staying in the Developer Perspective, click +Add from the left-side menu . Click the From Dockerfile tile . Fill out the form as follows: In the Git Repo URL Field, enter : https://github.com/mmondics/mongodb-app You should see a \u201cValidated\u201d message below the URL field. Ensure that the value in the Application field is mongo-app . Replace the value in the Name field with nodejs-app . Leave Deployment checked . Click the Create button . \u2003 Your Node.js application will now pull the Dockerfile from GitHub and begin its build. You will be returned to the Topology view. You should see mongodb and nodejs-app grouped together in mongo-app. When the build is complete, you will see a blue ring form around nodejs-app. You can also check its status by clicking on the nodejs-app icon and examining the Details panel. Once the build is complete, click the Open URL button at the top right of the nodejs-app icon . This button is simply a shortcut to the route that was created as part of the deployment. You are brought to the following landing page for your Node.js application: In the next section, you will insert data into and query your MongoDB database.","title":"Deploy Node.js Application from Dockerfile"},{"location":"lab006/lab006-5/#deploy-nodejs-application-from-a-dockerfile","text":"In this portion of the lab, you will deploy a Node.js web application created by the ATG. You will be building your deployment from a Dockerfile residing in a GitHub repository. Through the web application, you will be able to insert data into and query the MongoDB database you just created. For this lab the database will store sample name and email pairs displayed as a list of user information in the web UI. First, you need to deploy the application. Staying in the Developer Perspective, click +Add from the left-side menu . Click the From Dockerfile tile . Fill out the form as follows: In the Git Repo URL Field, enter : https://github.com/mmondics/mongodb-app You should see a \u201cValidated\u201d message below the URL field. Ensure that the value in the Application field is mongo-app . Replace the value in the Name field with nodejs-app . Leave Deployment checked . Click the Create button . \u2003 Your Node.js application will now pull the Dockerfile from GitHub and begin its build. You will be returned to the Topology view. You should see mongodb and nodejs-app grouped together in mongo-app. When the build is complete, you will see a blue ring form around nodejs-app. You can also check its status by clicking on the nodejs-app icon and examining the Details panel. Once the build is complete, click the Open URL button at the top right of the nodejs-app icon . This button is simply a shortcut to the route that was created as part of the deployment. You are brought to the following landing page for your Node.js application: In the next section, you will insert data into and query your MongoDB database.","title":"Deploy Node.js Application from a Dockerfile"},{"location":"lab006/lab006-6/","text":"Interacting with MongoDB from Node.js Web Application \u00b6 You should be on the \u201cHello World\u201d landing page of your Node.js web application. If you have moved off of this screen, refer to the previous section for instructions on how to access the application. Since your application has not yet been used, the MongoDB database of user data will be empty. We can use the Node.js frontend application to insert data into the linked MongoDB pod and the persistent storage backing it. Click the Add a New User button . You will be brought to a new page titled Add New User. Enter a sample username and email and click Submit . You will be brought to a page titled User List which displays the entire contents of your database. Feel free to add additional users. The data you just entered through the NodeJS web application is now stored in a MongoDB database backed by persistent storage on our NFS server. Now, let\u2019s test that our data will persist if we simulate a database crash by deleting our MongoDB pod. Return to the OpenShift Console Developer Perspective and navigate to the Topology View . Click the mongodb deployment . Click the down arrow to reduce the pod count to zero, terminating the MongoDB pod . Return to the web application and refresh the page . The page will not connect, and if you wait long enough you will get a 504 Gateway Time-out error as the database no longer exists and no connection can be made. Back in OpenShift, click the up arrow to increase the pod count back to 1 . Return to the web application and refresh the page again . Your data still exists, even though the MongoDB pod was terminated and replaced by a completely new one. In this section, a new MongoDB pod was created. Since you mounted NFS persistent storage at /data/db in the original MongoDB pod, your data persisted even when the original MongoDB pod was deleted and replaced with a new one. Without persistent storage, the new MongoDB pod would have contained an empty database.","title":"Interacting with MongoDB from Node.js Application"},{"location":"lab006/lab006-6/#interacting-with-mongodb-from-nodejs-web-application","text":"You should be on the \u201cHello World\u201d landing page of your Node.js web application. If you have moved off of this screen, refer to the previous section for instructions on how to access the application. Since your application has not yet been used, the MongoDB database of user data will be empty. We can use the Node.js frontend application to insert data into the linked MongoDB pod and the persistent storage backing it. Click the Add a New User button . You will be brought to a new page titled Add New User. Enter a sample username and email and click Submit . You will be brought to a page titled User List which displays the entire contents of your database. Feel free to add additional users. The data you just entered through the NodeJS web application is now stored in a MongoDB database backed by persistent storage on our NFS server. Now, let\u2019s test that our data will persist if we simulate a database crash by deleting our MongoDB pod. Return to the OpenShift Console Developer Perspective and navigate to the Topology View . Click the mongodb deployment . Click the down arrow to reduce the pod count to zero, terminating the MongoDB pod . Return to the web application and refresh the page . The page will not connect, and if you wait long enough you will get a 504 Gateway Time-out error as the database no longer exists and no connection can be made. Back in OpenShift, click the up arrow to increase the pod count back to 1 . Return to the web application and refresh the page again . Your data still exists, even though the MongoDB pod was terminated and replaced by a completely new one. In this section, a new MongoDB pod was created. Since you mounted NFS persistent storage at /data/db in the original MongoDB pod, your data persisted even when the original MongoDB pod was deleted and replaced with a new one. Without persistent storage, the new MongoDB pod would have contained an empty database.","title":"Interacting with MongoDB from Node.js Web Application"},{"location":"lab006/lab006-7/","text":"Cleaning Up \u00b6 There is no easy way to delete all of these objects from the OpenShift console. This is a much easier task in the OpenShift command line. In the OpenShift CLI, make sure you are in your own project (i.e. userNN-project) and run the following command : oc delete all --all Note If you are not connected to the OpenShift command line, refer to Using the OpenShift Command Line . This will delete most of the objects in your project, but not the Persistent Volume Claim you created. To delete the PVC, run the command : oc delete pvc/pvc-userNN Where NN is your user number.","title":"Cleaning Up"},{"location":"lab006/lab006-7/#cleaning-up","text":"There is no easy way to delete all of these objects from the OpenShift console. This is a much easier task in the OpenShift command line. In the OpenShift CLI, make sure you are in your own project (i.e. userNN-project) and run the following command : oc delete all --all Note If you are not connected to the OpenShift command line, refer to Using the OpenShift Command Line . This will delete most of the objects in your project, but not the Persistent Volume Claim you created. To delete the PVC, run the command : oc delete pvc/pvc-userNN Where NN is your user number.","title":"Cleaning Up"},{"location":"lab007/lab007-1/","text":"Deploying an Application with the Open Liberty Operator \u00b6 Note: this lab is a modified version of the GitHub repository here: https://github.com/OpenShift-Z/openliberty-operator-ocpz Open Liberty: is a lightweight, open framework for building fast and efficient cloud-native Java microservices is fast to start up with low memory footprint and live reload for quick iteration. is simple to add and remove features from the latest versions of MicroProfile and Java EE. requires zero migration lets you focus on what's important, not the APIs changing under you. The Open Liberty Operator can be used to deploy and manage Open Liberty applications into OpenShift clusters. You can also perform Day-2 operations such as gathering traces and dumps using the operator. Because the Open Liberty Operator watches all namespaces in the OpenShift cluster, workshop users are not required to deploy the Operator itself. It has already been deployed in the openshift-operators project.","title":"Overview of the Open Liberty Operator and Sample Application"},{"location":"lab007/lab007-1/#deploying-an-application-with-the-open-liberty-operator","text":"Note: this lab is a modified version of the GitHub repository here: https://github.com/OpenShift-Z/openliberty-operator-ocpz Open Liberty: is a lightweight, open framework for building fast and efficient cloud-native Java microservices is fast to start up with low memory footprint and live reload for quick iteration. is simple to add and remove features from the latest versions of MicroProfile and Java EE. requires zero migration lets you focus on what's important, not the APIs changing under you. The Open Liberty Operator can be used to deploy and manage Open Liberty applications into OpenShift clusters. You can also perform Day-2 operations such as gathering traces and dumps using the operator. Because the Open Liberty Operator watches all namespaces in the OpenShift cluster, workshop users are not required to deploy the Operator itself. It has already been deployed in the openshift-operators project.","title":"Deploying an Application with the Open Liberty Operator"},{"location":"lab007/lab007-2/","text":"Log into OpenShift Using the CLI \u00b6 In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server : ssh userNN@192.168.176.61 Where NN is your user number. When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with oc login and ending with 6443 . Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab007/lab007-2/#log-into-openshift-using-the-cli","text":"In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server : ssh userNN@192.168.176.61 Where NN is your user number. When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with oc login and ending with 6443 . Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab007/lab007-3/","text":"Cloning the GitHub Repository and Reviewing its Contents \u00b6 In the terminal session, you should have been automatically placed in your home directory /home/userNN (where NN is your user number). Run the command pwd to check your current working directory . Example Output user01@lab061:~$ pwd /home/user01 If you are in any other directory, change into the correct home directory using the command : cd /home/userNN Where NN is your user number. Example Output user01@lab061:~$ cd /home/user01 user01@lab061:~$ pwd /home/user01 In your home directory, clone the Open Liberty Operator repository using the command : git clone https://github.com/mmondics/openliberty-operator-ocpz Example Output user01@lab061:~$ git clone https://github.com/mmondics/openliberty-operator-ocpz Cloning into 'openliberty-operator-ocpz'... remote: Enumerating objects: 70, done. remote: Counting objects: 100% (70/70), done. remote: Compressing objects: 100% (68/68), done. remote: Total 70 (delta 30), reused 2 (delta 1), pack-reused 0 Unpacking objects: 100% (70/70), done. Checking connectivity... done. This will create a new directory called openliberty-operator-ocpz . Change into this directory using the command : cd openliberty-operator-ocpz Then list its contents using the command : ls -l Example Output user01@lab061:~$ cd openliberty-operator-ocpz user01@lab061:~/openliberty-operator-ocpz$ ls -l total 24 -rw-r--r-- 6 user01 users 4096 Sep 8 14:33 README.md drwxr-xr-x 6 user01 users 4096 Sep 8 14:33 admin-ol-operator-install drwxr-xr-x 6 user01 users 4096 Sep 8 14:33 images drwxr-xr-x 6 user01 users 4096 Sep 8 14:33 ol-app-install Expand for More Information If you navigate to the GitHub in a web browser ( https://github.com/mmondics/openliberty-operator-ocpz ), you will notice that the sub-directories in your Linux session reflect the folders contained in the repository. File Description README.md Contains the content displayed on the GitHub page for this repository. You can read through this README file if you want to get more information about this lab. admin-ol-operator-install Directory used to install the Open Liberty Operator onto the OpenShift cluster. Since this has been done ahead of time, you won\u2019t be using this directory. images Contains the images referenced in the README.md file and displayed on the GitHub page for this repository. ol-app-install Contains all of the files needed to build, push, and deploy the Mod Resorts sample application. This is where we will be doing our work for this lab. Change into the ol-app-install directory using the command : cd ol-app-install List its contents using the command : ls -l Example Output user01@lab061:~$ cd ol-app-install user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ls -l total 8416 -rwxr--r\u20141 user01 users 845 Sep 8 14:33 1-build.sh -rwxr--r\u20141 user01 users 367 Sep 8 14:33 2-deploy.sh -rwxr--r\u20141 user01 users 612 Sep 8 14:33 3-cleanup.sh -rwxr--r\u20141 user01 users 142 Sep 8 14:33 Dockerfile -rwxr--r\u20141 user01 users 291 Sep 8 14:33 app-mod-withroute_cr.yaml -rwxr--r\u20141 user01 users 636 Sep 8 14:33 env -rwxr--r\u20141 user01 users 858364 Sep 8 14:33 modresorts-1.0.war -rwxr--r\u20141 user01 users 687 Sep 8 14:33 server.xml This directory contains 8 files that you will use to install the Mod Resorts sample application. Expand for More Information File Description 1-build.sh shell script that contains commands to log into the OpenShift Cluster and the internal registry, create a new project, build the container image and then push it into the registry. 2-deploy.sh shell script that contains commands to create the OpenLibertyApplication custom resource based off of the image build and pushed by 1-build.sh. 3-cleanup.sh shell script that will clean up the OpenLibertyApplication created by the previous scripts and delete the new project created by 1-build.sh. Dockerfile referenced by 1-build.sh to build the container image. app-mod-withroute_cr.yaml referenced by 2-deploy.sh to create the OpenLibertyApplication custom resource. env environment variables sourced by the shell scripts for various commands. modresorts-1.0.war a collection of JAR-files, JavaServer Pages, Java Servlets, Java classes, etc\u2026 that together constitute the sample Mod Resorts application. server.xml used in conjunction with the .war file to create the web application.","title":"Cloning the GitHub Repository and Reviewing its Contents"},{"location":"lab007/lab007-3/#cloning-the-github-repository-and-reviewing-its-contents","text":"In the terminal session, you should have been automatically placed in your home directory /home/userNN (where NN is your user number). Run the command pwd to check your current working directory . Example Output user01@lab061:~$ pwd /home/user01 If you are in any other directory, change into the correct home directory using the command : cd /home/userNN Where NN is your user number. Example Output user01@lab061:~$ cd /home/user01 user01@lab061:~$ pwd /home/user01 In your home directory, clone the Open Liberty Operator repository using the command : git clone https://github.com/mmondics/openliberty-operator-ocpz Example Output user01@lab061:~$ git clone https://github.com/mmondics/openliberty-operator-ocpz Cloning into 'openliberty-operator-ocpz'... remote: Enumerating objects: 70, done. remote: Counting objects: 100% (70/70), done. remote: Compressing objects: 100% (68/68), done. remote: Total 70 (delta 30), reused 2 (delta 1), pack-reused 0 Unpacking objects: 100% (70/70), done. Checking connectivity... done. This will create a new directory called openliberty-operator-ocpz . Change into this directory using the command : cd openliberty-operator-ocpz Then list its contents using the command : ls -l Example Output user01@lab061:~$ cd openliberty-operator-ocpz user01@lab061:~/openliberty-operator-ocpz$ ls -l total 24 -rw-r--r-- 6 user01 users 4096 Sep 8 14:33 README.md drwxr-xr-x 6 user01 users 4096 Sep 8 14:33 admin-ol-operator-install drwxr-xr-x 6 user01 users 4096 Sep 8 14:33 images drwxr-xr-x 6 user01 users 4096 Sep 8 14:33 ol-app-install Expand for More Information If you navigate to the GitHub in a web browser ( https://github.com/mmondics/openliberty-operator-ocpz ), you will notice that the sub-directories in your Linux session reflect the folders contained in the repository. File Description README.md Contains the content displayed on the GitHub page for this repository. You can read through this README file if you want to get more information about this lab. admin-ol-operator-install Directory used to install the Open Liberty Operator onto the OpenShift cluster. Since this has been done ahead of time, you won\u2019t be using this directory. images Contains the images referenced in the README.md file and displayed on the GitHub page for this repository. ol-app-install Contains all of the files needed to build, push, and deploy the Mod Resorts sample application. This is where we will be doing our work for this lab. Change into the ol-app-install directory using the command : cd ol-app-install List its contents using the command : ls -l Example Output user01@lab061:~$ cd ol-app-install user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ls -l total 8416 -rwxr--r\u20141 user01 users 845 Sep 8 14:33 1-build.sh -rwxr--r\u20141 user01 users 367 Sep 8 14:33 2-deploy.sh -rwxr--r\u20141 user01 users 612 Sep 8 14:33 3-cleanup.sh -rwxr--r\u20141 user01 users 142 Sep 8 14:33 Dockerfile -rwxr--r\u20141 user01 users 291 Sep 8 14:33 app-mod-withroute_cr.yaml -rwxr--r\u20141 user01 users 636 Sep 8 14:33 env -rwxr--r\u20141 user01 users 858364 Sep 8 14:33 modresorts-1.0.war -rwxr--r\u20141 user01 users 687 Sep 8 14:33 server.xml This directory contains 8 files that you will use to install the Mod Resorts sample application. Expand for More Information File Description 1-build.sh shell script that contains commands to log into the OpenShift Cluster and the internal registry, create a new project, build the container image and then push it into the registry. 2-deploy.sh shell script that contains commands to create the OpenLibertyApplication custom resource based off of the image build and pushed by 1-build.sh. 3-cleanup.sh shell script that will clean up the OpenLibertyApplication created by the previous scripts and delete the new project created by 1-build.sh. Dockerfile referenced by 1-build.sh to build the container image. app-mod-withroute_cr.yaml referenced by 2-deploy.sh to create the OpenLibertyApplication custom resource. env environment variables sourced by the shell scripts for various commands. modresorts-1.0.war a collection of JAR-files, JavaServer Pages, Java Servlets, Java classes, etc\u2026 that together constitute the sample Mod Resorts application. server.xml used in conjunction with the .war file to create the web application.","title":"Cloning the GitHub Repository and Reviewing its Contents"},{"location":"lab007/lab007-4/","text":"Using the Open Liberty Operator to Install an Application \u00b6 Edit the environment variables file to match your user number (NN) . sed -i 's/NN/YOUR_USER_NUMBER/g' env Important Make sure that you replace YOUR_USER_NUMBER in the command above. Run the command cat env to check that the two instances of NN were properly replaced with your user number . With your modified environment variables file env, you\u2019re ready to run the shell script 1-build.sh that will use Buildah to build a container image from the Dockerfile, and Podman to push the image into OpenShift\u2019s internal registry. Before running this script, take a look at the steps it will go through. View the script contents using the command : cat 1-build.sh Example Output #!/bin/text unset KUBECONFIG . ./env echo \"Logging into OpenShift\" oc login $OPENSHIFT_API_URL \\ --username=$OPENSHIFT_USERNAME \\ --password=$OPENSHIFT_PASSWORD \\ --insecure-skip-tls-verify=true echo \"Logging into OpenShift image registry\" podman login \\ --username $OPENSHIFT_USERNAME \\ --password $(oc whoami -t) \\ --tls-verify=false \\ $OPENSHIFT_REGISTRY_URL echo \"Switch to $OPENSHIFT_PROJECT\" oc project $OPENSHIFT_PROJECT echo \"Building the container image\" buildah build-using-dockerfile \\ -t ${OPENSHIFT_REGISTRY_URL}/$OPENSHIFT_PROJECT/app-modernization:v1.0.0 \\ . echo \"Pushing the container image to the OpenShift image registry\" podman push --tls-verify=false \\ ${OPENSHIFT_REGISTRY_URL}/${OPENSHIFT_PROJECT}/app-modernization:v1.0.0 This shell script: Logs you into the OpenShift cluster. Logs you into the OpenShift cluster\u2019s internal image registry. Switches to your project, if not currently working in it. Builds the container image using the Dockerfile contained in your working directory. Pushes the new container image from step 4 to the cluster\u2019s internal image registry. Note Note that you are welcome to enter each command manually and individually, but the scripting is there to minimize the opportunity for typos and other errors. If you do enter each command manually, make sure to replace each variable in BLUE with the actual value itself. Also, notice that the forward slash simply breaks a single command into multiple lines. You might notice that the Dockerfile is doing the brunt of the work in this script to build the container image itself. Take a look at this too, using the command : cat Dockerfile Example Output FROM quay.io/mmondics/open-liberty:latest COPY --chown = 1001 :0 modresorts-1.0.war /config/dropins COPY --chown = 1001 :0 server.xml /config/ This Dockerfile pulls the Open Liberty Java EE 8 image from Quay.io then adds the modresorts-1.0.war binary and server.xml configuration file to the base image. Back in the ol-app-install working directory, you can now run the script that brings all of these pieces together. Run the script using the command : ./1-build.sh Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./1-build.sh Logging into Openshift Login successful. You have access to 170 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". Logging into Openshift image registry Login Succeeded! Switch to user01-project Already on project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\". Building the container image STEP 1: FROM quay.io/mmondics/open-liberty:javaee8-ubi-min STEP 2: COPY --chown=1001:0 modresorts-1.0.war /config/dropins STEP 3: COPY --chown=1001:0 server.xml /config/ STEP 4: COMMIT default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization:v1.0.0 Getting image source signatures Copying blob d20db2e30c33 skipped: already exists Copying blob 4ec5f0a55d74 skipped: already exists ... cut from screenshot ... Copying config c6e6f3bf6b done Writing manifest to image destination Copying config c6e6f3bf6b done Writing manifest to image destination Storing signatures user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ Your container image is now built and pushed into OpenShift\u2019s internal registry. View your new image in the registry using the command : podman images Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization v1.0.0 5970fc63cb46 16 seconds ago 472 MB With the app-modernization image in the OpenShift internal registry, it can now be used to deploy an application into the cluster. You have one more file to edit before deploying the Mod Resorts sample application. Edit the custom resource file using the command : sed -i 's/NN/YOUR_USER_NUMBER/g' app-mod-withroute_cr.yaml Important Make sure that you replace YOUR_USER_NUMBER in the command above. Run the command : cat app-mod-withroute_cr.yaml And make sure that the two instances of NN were properly replaced with your user number. Expand for More Information What exactly is this YAML file? A Custom Resource Definition (CRD) object defines a new, unique object in the cluster and lets the Kubernetes API server handle its entire lifecycle. Custom Resource (CR) objects are created from CRDs that have been added to the cluster by a cluster administrator, allowing all cluster users to add the new resource type into projects. So in this case, a CRD was created ahead of time of the kind: OpenLibertyApplication, and you are creating a CR from that CRD. With the modified app-mod-withroute_cr.yaml file, you\u2019re ready to run the second script, 2-deploy.sh . Before running it, take a look at what all it will do. Run the command : cat 2-deploy.sh Example Output #!/bin/text unset KUBECONFIG . ./env echo \"Logging into OpenShift\" oc login $OPENSHIFT_API_URL \\ --username=$OPENSHIFT_USERNAME \\ --password=$OPENSHIFT_PASSWORD \\ --insecure-skip-tls-verify=true echo \"Creating OpenLiberty Custom Resource\" oc -n $OPENSHIFT_PROJECT create -f app-mod-withroute_cr.yaml This shell script: Logs you into the OpenShift cluster. Creates an object from the Custom Resource (CR) YAML file you just edited. Run this shell script with the command : ./2-deploy.sh Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./2-deploy.sh Logging into Openshift Login successful. You have access to 170 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". Creating Openliberty Custom Resource Definition openlibertyapplication.openliberty.io/appmod created Your Custom Resource named appmod of kind OpenLibertyApplication has been created in your project, userNN-project . The creation of this CR resulted in the Open Liberty Operator deploying a pod, deployment, replicaset, and a service that is exposed as a route. View all of the created objects using the command : oc get all Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ oc get all NAME READY STATUS RESTARTS AGE pod/appmod-5959fb64b5-fqkvg 1/1 Running 0 16s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/appmod ClusterIP 172.30.26.44 <none> 9080/TCP 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/appmod 1/1 1 1 16s NAME DESIRED CURRENT READY AGE replicaset.apps/appmod-5959fb64b5 1 1 1 16s NAME IMAGE REPOSITORY TAGS UPDATED imagestream.image.openshift.io/app-modernization default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization v1.0.0 29 seconds ago NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/appmod modresort-user01.apps.atsocppa.dmz /resorts appmod 9080-tcp None","title":"Using the Open Liberty Operator to Install an Application"},{"location":"lab007/lab007-4/#using-the-open-liberty-operator-to-install-an-application","text":"Edit the environment variables file to match your user number (NN) . sed -i 's/NN/YOUR_USER_NUMBER/g' env Important Make sure that you replace YOUR_USER_NUMBER in the command above. Run the command cat env to check that the two instances of NN were properly replaced with your user number . With your modified environment variables file env, you\u2019re ready to run the shell script 1-build.sh that will use Buildah to build a container image from the Dockerfile, and Podman to push the image into OpenShift\u2019s internal registry. Before running this script, take a look at the steps it will go through. View the script contents using the command : cat 1-build.sh Example Output #!/bin/text unset KUBECONFIG . ./env echo \"Logging into OpenShift\" oc login $OPENSHIFT_API_URL \\ --username=$OPENSHIFT_USERNAME \\ --password=$OPENSHIFT_PASSWORD \\ --insecure-skip-tls-verify=true echo \"Logging into OpenShift image registry\" podman login \\ --username $OPENSHIFT_USERNAME \\ --password $(oc whoami -t) \\ --tls-verify=false \\ $OPENSHIFT_REGISTRY_URL echo \"Switch to $OPENSHIFT_PROJECT\" oc project $OPENSHIFT_PROJECT echo \"Building the container image\" buildah build-using-dockerfile \\ -t ${OPENSHIFT_REGISTRY_URL}/$OPENSHIFT_PROJECT/app-modernization:v1.0.0 \\ . echo \"Pushing the container image to the OpenShift image registry\" podman push --tls-verify=false \\ ${OPENSHIFT_REGISTRY_URL}/${OPENSHIFT_PROJECT}/app-modernization:v1.0.0 This shell script: Logs you into the OpenShift cluster. Logs you into the OpenShift cluster\u2019s internal image registry. Switches to your project, if not currently working in it. Builds the container image using the Dockerfile contained in your working directory. Pushes the new container image from step 4 to the cluster\u2019s internal image registry. Note Note that you are welcome to enter each command manually and individually, but the scripting is there to minimize the opportunity for typos and other errors. If you do enter each command manually, make sure to replace each variable in BLUE with the actual value itself. Also, notice that the forward slash simply breaks a single command into multiple lines. You might notice that the Dockerfile is doing the brunt of the work in this script to build the container image itself. Take a look at this too, using the command : cat Dockerfile Example Output FROM quay.io/mmondics/open-liberty:latest COPY --chown = 1001 :0 modresorts-1.0.war /config/dropins COPY --chown = 1001 :0 server.xml /config/ This Dockerfile pulls the Open Liberty Java EE 8 image from Quay.io then adds the modresorts-1.0.war binary and server.xml configuration file to the base image. Back in the ol-app-install working directory, you can now run the script that brings all of these pieces together. Run the script using the command : ./1-build.sh Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./1-build.sh Logging into Openshift Login successful. You have access to 170 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". Logging into Openshift image registry Login Succeeded! Switch to user01-project Already on project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\". Building the container image STEP 1: FROM quay.io/mmondics/open-liberty:javaee8-ubi-min STEP 2: COPY --chown=1001:0 modresorts-1.0.war /config/dropins STEP 3: COPY --chown=1001:0 server.xml /config/ STEP 4: COMMIT default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization:v1.0.0 Getting image source signatures Copying blob d20db2e30c33 skipped: already exists Copying blob 4ec5f0a55d74 skipped: already exists ... cut from screenshot ... Copying config c6e6f3bf6b done Writing manifest to image destination Copying config c6e6f3bf6b done Writing manifest to image destination Storing signatures user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ Your container image is now built and pushed into OpenShift\u2019s internal registry. View your new image in the registry using the command : podman images Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization v1.0.0 5970fc63cb46 16 seconds ago 472 MB With the app-modernization image in the OpenShift internal registry, it can now be used to deploy an application into the cluster. You have one more file to edit before deploying the Mod Resorts sample application. Edit the custom resource file using the command : sed -i 's/NN/YOUR_USER_NUMBER/g' app-mod-withroute_cr.yaml Important Make sure that you replace YOUR_USER_NUMBER in the command above. Run the command : cat app-mod-withroute_cr.yaml And make sure that the two instances of NN were properly replaced with your user number. Expand for More Information What exactly is this YAML file? A Custom Resource Definition (CRD) object defines a new, unique object in the cluster and lets the Kubernetes API server handle its entire lifecycle. Custom Resource (CR) objects are created from CRDs that have been added to the cluster by a cluster administrator, allowing all cluster users to add the new resource type into projects. So in this case, a CRD was created ahead of time of the kind: OpenLibertyApplication, and you are creating a CR from that CRD. With the modified app-mod-withroute_cr.yaml file, you\u2019re ready to run the second script, 2-deploy.sh . Before running it, take a look at what all it will do. Run the command : cat 2-deploy.sh Example Output #!/bin/text unset KUBECONFIG . ./env echo \"Logging into OpenShift\" oc login $OPENSHIFT_API_URL \\ --username=$OPENSHIFT_USERNAME \\ --password=$OPENSHIFT_PASSWORD \\ --insecure-skip-tls-verify=true echo \"Creating OpenLiberty Custom Resource\" oc -n $OPENSHIFT_PROJECT create -f app-mod-withroute_cr.yaml This shell script: Logs you into the OpenShift cluster. Creates an object from the Custom Resource (CR) YAML file you just edited. Run this shell script with the command : ./2-deploy.sh Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./2-deploy.sh Logging into Openshift Login successful. You have access to 170 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". Creating Openliberty Custom Resource Definition openlibertyapplication.openliberty.io/appmod created Your Custom Resource named appmod of kind OpenLibertyApplication has been created in your project, userNN-project . The creation of this CR resulted in the Open Liberty Operator deploying a pod, deployment, replicaset, and a service that is exposed as a route. View all of the created objects using the command : oc get all Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ oc get all NAME READY STATUS RESTARTS AGE pod/appmod-5959fb64b5-fqkvg 1/1 Running 0 16s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/appmod ClusterIP 172.30.26.44 <none> 9080/TCP 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/appmod 1/1 1 1 16s NAME DESIRED CURRENT READY AGE replicaset.apps/appmod-5959fb64b5 1 1 1 16s NAME IMAGE REPOSITORY TAGS UPDATED imagestream.image.openshift.io/app-modernization default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization v1.0.0 29 seconds ago NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/appmod modresort-user01.apps.atsocppa.dmz /resorts appmod 9080-tcp None","title":"Using the Open Liberty Operator to Install an Application"},{"location":"lab007/lab007-5/","text":"Access the Application in a Browser \u00b6 Your application is running and accessible via the exposed route. In a web browser, navigate to the route : http://modresort-userNN.apps.atsocppa.dmz/resorts/ Where NN is your user number. The demo application used (mod resorts) is admittedly a simple use-case with no dependencies on external resources, so you will notice that most of the links do not function. In real-world applications, there will be dependencies on external resources which can be integrated using various OpenShift and Kubernetes objects such as ConfigMaps, Volume mounts, secrets, etc.","title":"Access the Application in a Browser"},{"location":"lab007/lab007-5/#access-the-application-in-a-browser","text":"Your application is running and accessible via the exposed route. In a web browser, navigate to the route : http://modresort-userNN.apps.atsocppa.dmz/resorts/ Where NN is your user number. The demo application used (mod resorts) is admittedly a simple use-case with no dependencies on external resources, so you will notice that most of the links do not function. In real-world applications, there will be dependencies on external resources which can be integrated using various OpenShift and Kubernetes objects such as ConfigMaps, Volume mounts, secrets, etc.","title":"Access the Application in a Browser"},{"location":"lab007/lab007-6/","text":"Cleaning Up \u00b6 When you\u2019re ready to wrap up this lab, return to your terminal to run the last shell script. Run the cleanup script with the command : ./3-cleanup.sh Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./3-cleanup.sh Logging into OpenShift Login successful. You have access to 169 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". Logging into OpenShift image registry Login Succeeded! Deleting Openliberty app openlibertyapplication.openliberty.io \"appmod\" deleted Deleting imagestream imagestream.image.openshift.io \"app-modernization\" deleted Your OpenLiberty app consisting of a pod, deployment, service, route, and the imagestream used to create the pod have all been deleted.","title":"Cleaning Up"},{"location":"lab007/lab007-6/#cleaning-up","text":"When you\u2019re ready to wrap up this lab, return to your terminal to run the last shell script. Run the cleanup script with the command : ./3-cleanup.sh Example Output user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./3-cleanup.sh Logging into OpenShift Login successful. You have access to 169 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". Logging into OpenShift image registry Login Succeeded! Deleting Openliberty app openlibertyapplication.openliberty.io \"appmod\" deleted Deleting imagestream imagestream.image.openshift.io \"app-modernization\" deleted Your OpenLiberty app consisting of a pod, deployment, service, route, and the imagestream used to create the pod have all been deleted.","title":"Cleaning Up"},{"location":"lab008/lab008-1/","text":"Deploying an Application with Quarkus Red Hat Runtime \u00b6 Note For more Quarkus guides, see the official site here: https://quarkus.io/guides/ Quarkus is a full-stack, Kubernetes-native Java framework optimized specifically for containers and Kubernetes environments. It is designed to work with popular Java standards, frameworks, and libraries like Eclipse MicroProfile, Spring Apache Kafka, RESTEasy, and many more. Quarkus was designed for developers with the intent to be easy to use with features that work well with little to no configuration . It includes many features for developers, such as live coding so you can immediately check the effect of code changes and quickly troubleshoot them. Quarkus was built around a container-first philosophy, meaning it\u2019s optimized for lower memory usage and faster startup times. Quarkus builds applications to consume 1/10 th the memory when compared to traditional Java, and has a much faster startup time (as much as 300 times faster), both of which greatly reduce the cost of cloud resources. In this lab, you will explore these features by: Creating a new Quarkus project Configuring the Quarkus application for OpenShift Deploying the Quarkus application to OpenShift","title":"Overview of the Quarkus Red Hat Runtime"},{"location":"lab008/lab008-1/#deploying-an-application-with-quarkus-red-hat-runtime","text":"Note For more Quarkus guides, see the official site here: https://quarkus.io/guides/ Quarkus is a full-stack, Kubernetes-native Java framework optimized specifically for containers and Kubernetes environments. It is designed to work with popular Java standards, frameworks, and libraries like Eclipse MicroProfile, Spring Apache Kafka, RESTEasy, and many more. Quarkus was designed for developers with the intent to be easy to use with features that work well with little to no configuration . It includes many features for developers, such as live coding so you can immediately check the effect of code changes and quickly troubleshoot them. Quarkus was built around a container-first philosophy, meaning it\u2019s optimized for lower memory usage and faster startup times. Quarkus builds applications to consume 1/10 th the memory when compared to traditional Java, and has a much faster startup time (as much as 300 times faster), both of which greatly reduce the cost of cloud resources. In this lab, you will explore these features by: Creating a new Quarkus project Configuring the Quarkus application for OpenShift Deploying the Quarkus application to OpenShift","title":"Deploying an Application with Quarkus Red Hat Runtime"},{"location":"lab008/lab008-2/","text":"Log into OpenShift Using the ClI \u00b6 In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server : ssh userNN@192.168.176.61 Where NN is your user number. When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab008/lab008-2/#log-into-openshift-using-the-cli","text":"In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server : ssh userNN@192.168.176.61 Where NN is your user number. When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the ClI"},{"location":"lab008/lab008-3/","text":"Creating and Reviewing the Quarkus Project \u00b6 In the terminal session, you should have been automatically placed in your home directory /home/userNN (where NN is your user number). Run the command pwd to check your current working directory . Example Output user01@lab061:~$ pwd /home/user01 If you are in any other directory, change into the correct home directory using the command : cd /home/userNN Where NN is your user number. Example Output user01@lab061:~$ cd /home/user01 user01@lab061:~$ pwd /home/user01 We will start off by creating a Maven Project. Maven is a powerful project management tool based on POM (Project Object Model). It is a tool used by Java developers to simplify and add structure to their day-to-day work by implementing dependency and documentation into their Java applications. You can read more about Maven on their official site here: https://maven.apache.org/index.html . The following command uses the Maven Quarkus Plugin to create a basic Maven project for you in the openshift-quickstart subdirectory. It generates: The Maven structure including the pom.xml An org.acme.rest.GreetingResource resource exposed on /greeting An associated unit test A landing page that is accessible on http://localhost:8080 after starting the application Example Dockerfiles for both native and jvm modes The application configuration file Note that the forward slash simply breaks the command into multiple lines for readability. Also note that if you do not specify the variables for projectGroupId, projectArtifactId, etc., the Maven installer will prompt you for them. In your home directory, run the following command : mvn io.quarkus:quarkus-maven-plugin:1.8.3.Final:create \\ -DprojectGroupId=org.acme \\ -DprojectArtifactId=openshift-quickstart \\ -DclassName=\"org.acme.rest.GreetingResource\" \\ -Dpath=\"/greeting\" \\ -Dextensions=\"resteasy\" Example Output user01@lab061:~$ user01@lab061:~$ mvn io.quarkus:quarkus-maven-plugin:1.8.3.Final:create \\ > -DprojectGroupId=org.acme \\ > -DprojectArtifactId=openshift-quickstart \\ > -DclassName=\"org.acme.rest.GreetingResource\" \\ > -Dpath=\"/greeting\" \\ > -Dextensions=\u201dresteasy\u201d [INFO] Scanning for projects... [INFO] [INFO] ------------------< org.apache.maven:standalone-pom >------------------- [INFO] Building Maven Stub Project (No POM) 1 [INFO] --------------------------------[ pom ]--------------------------------- [INFO] [INFO] --- quarkus-maven-plugin:1.8.3.Final:create (default-cli) @ standalone-pom --- [INFO] [INFO] Maven Wrapper version 0.5.6 has been successfully set up for your project. [INFO] Using Apache Maven: 3.6.3 [INFO] Repo URL in properties file: https://repo.maven.apache.org/maven2 [INFO] [INFO] ======================================================================================== [INFO] Your new application has been created in /home/user01/openshift-quickstart [INFO] Navigate into this directory and launch your application with mvn quarkus:dev [INFO] Your application will be accessible on http://localhost:8080 [INFO] ======================================================================================== [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 12.644 s [INFO] Finished at: 2020-10-19T11:35:57-04:00 [INFO] ------------------------------------------------------------------------ As the installation says, an application has been created under the openshift-quickstart sub-directory. Change into this directory using the command : cd openshift-quickstart Then view its structure using the command : tree Example Output user01@lab061:~$ cd openshift-quickstart user01@lab061:~/openshift-quickstart$ tree total 24 . |-- README.md |-- mvnw |-- mvnw.cmd |-- pom.xml `-- src |-- main | |-- docker | | |-- Dockerfile.fast-jar | | |-- Dockerfile.jvm | | `-- Dockerfile.native | |-- java | | `-- org | | `-- acme | | `-- rest | | `-- GreetingResource.java | `-- resources | |-- META-INF | | `-- resources | | `-- index.html | `-- application.properties `-- test `-- java `-- org `-- acme `-- rest |-- GreetingResourceTest.java `-- NativeGreetingResourceIT.java 15 directories, 12 files You can see that the command created the pom.xml file, Dockerfiles for both JVM and native modes, your GreetingResource.java file exposed at /greeting , and the associated test resources. Let\u2019s take a look at the pom.xml file that was created as a part of the installation. View the file using the command : cat pom.xml Example Output user01@lab061:~/openshift-quickstart$ cat pom.xml ...omitted... <dependencyManagement> <dependencies> <dependency> <groupId> ${quarkus.platform.group-id} </groupId> <artifactId> ${quarkus.platform.artifact-id} </artifactId> <version> ${quarkus.platform.version} </version> <type> pom </type> <scope> import </scope> </dependency> </dependencies> </dependencyManagement> ...omitted... <build> <plugins> <plugin> <groupId> io.quarkus </groupId> <artifactId> quarkus-maven-plugin </artifactId> <version> ${quarkus-plugin.version} </version> <executions> <execution> <goals> <goal> generate-code </goal> <goal> generate-code-tests </goal> <goal> build </goal> </goals> </execution> </executions> </plugin> <plugin> <artifactId> maven-compiler-plugin </artifactId> <version> ${compiler-plugin.version} </version> </plugin> <plugin> <artifactId> maven-surefire-plugin </artifactId> <version> ${surefire-plugin.version} </version> <configuration> <systemPropertyVariables> <java.util.logging.manager> org.jboss.logmanager.LogManager </java.util.logging.manager> <maven.home> ${maven.home} </maven.home> </systemPropertyVariables> </configuration> </plugin> </plugins> </build> The snippets above show the import of the Quarkus BOM, which allows you to omit the version on the different Quarkus dependencies. In addition, you can see the quarkus-maven-plugin responsible for the packaging of the application and providing the development mode. Next look at the dependencies section. Example Output <dependencies> <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-resteasy </artifactId> </dependency> <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-junit5 </artifactId> <scope> test </scope> </dependency> <dependency> <groupId> io.rest-assured </groupId> <artifactId> rest-assured </artifactId> <scope> test </scope> </dependency> You can see we are using Quarkus extensions which allow the development and testing of REST applications: During the installation, the openshift-quickstart/src/main/java/org/acme/rest/GreetingResource.java file was created. This is a simple REST endpoint, returning \u201chello\u201d to requests at /greeting . From the openshift-quickstart directory, view this file with the command : cat src/main/java/org/acme/rest/GreetingResource.java Example Output package org.acme.rest ; import javax.ws.rs.GET ; import javax.ws.rs.Path ; import javax.ws.rs.Produces ; import javax.ws.rs.core.MediaType ; @Path ( \"/greeting\" ) public class GreetingResource { @GET @Produces ( MediaType . TEXT_PLAIN ) public String hello () { return \"hello\" ; } } You can see that this file is simply telling the application to return \u201chello\u201d at the /greeting path.","title":"Creating and Reviewing the Quarkus Project"},{"location":"lab008/lab008-3/#creating-and-reviewing-the-quarkus-project","text":"In the terminal session, you should have been automatically placed in your home directory /home/userNN (where NN is your user number). Run the command pwd to check your current working directory . Example Output user01@lab061:~$ pwd /home/user01 If you are in any other directory, change into the correct home directory using the command : cd /home/userNN Where NN is your user number. Example Output user01@lab061:~$ cd /home/user01 user01@lab061:~$ pwd /home/user01 We will start off by creating a Maven Project. Maven is a powerful project management tool based on POM (Project Object Model). It is a tool used by Java developers to simplify and add structure to their day-to-day work by implementing dependency and documentation into their Java applications. You can read more about Maven on their official site here: https://maven.apache.org/index.html . The following command uses the Maven Quarkus Plugin to create a basic Maven project for you in the openshift-quickstart subdirectory. It generates: The Maven structure including the pom.xml An org.acme.rest.GreetingResource resource exposed on /greeting An associated unit test A landing page that is accessible on http://localhost:8080 after starting the application Example Dockerfiles for both native and jvm modes The application configuration file Note that the forward slash simply breaks the command into multiple lines for readability. Also note that if you do not specify the variables for projectGroupId, projectArtifactId, etc., the Maven installer will prompt you for them. In your home directory, run the following command : mvn io.quarkus:quarkus-maven-plugin:1.8.3.Final:create \\ -DprojectGroupId=org.acme \\ -DprojectArtifactId=openshift-quickstart \\ -DclassName=\"org.acme.rest.GreetingResource\" \\ -Dpath=\"/greeting\" \\ -Dextensions=\"resteasy\" Example Output user01@lab061:~$ user01@lab061:~$ mvn io.quarkus:quarkus-maven-plugin:1.8.3.Final:create \\ > -DprojectGroupId=org.acme \\ > -DprojectArtifactId=openshift-quickstart \\ > -DclassName=\"org.acme.rest.GreetingResource\" \\ > -Dpath=\"/greeting\" \\ > -Dextensions=\u201dresteasy\u201d [INFO] Scanning for projects... [INFO] [INFO] ------------------< org.apache.maven:standalone-pom >------------------- [INFO] Building Maven Stub Project (No POM) 1 [INFO] --------------------------------[ pom ]--------------------------------- [INFO] [INFO] --- quarkus-maven-plugin:1.8.3.Final:create (default-cli) @ standalone-pom --- [INFO] [INFO] Maven Wrapper version 0.5.6 has been successfully set up for your project. [INFO] Using Apache Maven: 3.6.3 [INFO] Repo URL in properties file: https://repo.maven.apache.org/maven2 [INFO] [INFO] ======================================================================================== [INFO] Your new application has been created in /home/user01/openshift-quickstart [INFO] Navigate into this directory and launch your application with mvn quarkus:dev [INFO] Your application will be accessible on http://localhost:8080 [INFO] ======================================================================================== [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 12.644 s [INFO] Finished at: 2020-10-19T11:35:57-04:00 [INFO] ------------------------------------------------------------------------ As the installation says, an application has been created under the openshift-quickstart sub-directory. Change into this directory using the command : cd openshift-quickstart Then view its structure using the command : tree Example Output user01@lab061:~$ cd openshift-quickstart user01@lab061:~/openshift-quickstart$ tree total 24 . |-- README.md |-- mvnw |-- mvnw.cmd |-- pom.xml `-- src |-- main | |-- docker | | |-- Dockerfile.fast-jar | | |-- Dockerfile.jvm | | `-- Dockerfile.native | |-- java | | `-- org | | `-- acme | | `-- rest | | `-- GreetingResource.java | `-- resources | |-- META-INF | | `-- resources | | `-- index.html | `-- application.properties `-- test `-- java `-- org `-- acme `-- rest |-- GreetingResourceTest.java `-- NativeGreetingResourceIT.java 15 directories, 12 files You can see that the command created the pom.xml file, Dockerfiles for both JVM and native modes, your GreetingResource.java file exposed at /greeting , and the associated test resources. Let\u2019s take a look at the pom.xml file that was created as a part of the installation. View the file using the command : cat pom.xml Example Output user01@lab061:~/openshift-quickstart$ cat pom.xml ...omitted... <dependencyManagement> <dependencies> <dependency> <groupId> ${quarkus.platform.group-id} </groupId> <artifactId> ${quarkus.platform.artifact-id} </artifactId> <version> ${quarkus.platform.version} </version> <type> pom </type> <scope> import </scope> </dependency> </dependencies> </dependencyManagement> ...omitted... <build> <plugins> <plugin> <groupId> io.quarkus </groupId> <artifactId> quarkus-maven-plugin </artifactId> <version> ${quarkus-plugin.version} </version> <executions> <execution> <goals> <goal> generate-code </goal> <goal> generate-code-tests </goal> <goal> build </goal> </goals> </execution> </executions> </plugin> <plugin> <artifactId> maven-compiler-plugin </artifactId> <version> ${compiler-plugin.version} </version> </plugin> <plugin> <artifactId> maven-surefire-plugin </artifactId> <version> ${surefire-plugin.version} </version> <configuration> <systemPropertyVariables> <java.util.logging.manager> org.jboss.logmanager.LogManager </java.util.logging.manager> <maven.home> ${maven.home} </maven.home> </systemPropertyVariables> </configuration> </plugin> </plugins> </build> The snippets above show the import of the Quarkus BOM, which allows you to omit the version on the different Quarkus dependencies. In addition, you can see the quarkus-maven-plugin responsible for the packaging of the application and providing the development mode. Next look at the dependencies section. Example Output <dependencies> <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-resteasy </artifactId> </dependency> <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-junit5 </artifactId> <scope> test </scope> </dependency> <dependency> <groupId> io.rest-assured </groupId> <artifactId> rest-assured </artifactId> <scope> test </scope> </dependency> You can see we are using Quarkus extensions which allow the development and testing of REST applications: During the installation, the openshift-quickstart/src/main/java/org/acme/rest/GreetingResource.java file was created. This is a simple REST endpoint, returning \u201chello\u201d to requests at /greeting . From the openshift-quickstart directory, view this file with the command : cat src/main/java/org/acme/rest/GreetingResource.java Example Output package org.acme.rest ; import javax.ws.rs.GET ; import javax.ws.rs.Path ; import javax.ws.rs.Produces ; import javax.ws.rs.core.MediaType ; @Path ( \"/greeting\" ) public class GreetingResource { @GET @Produces ( MediaType . TEXT_PLAIN ) public String hello () { return \"hello\" ; } } You can see that this file is simply telling the application to return \u201chello\u201d at the /greeting path.","title":"Creating and Reviewing the Quarkus Project"},{"location":"lab008/lab008-4/","text":"Configure the Application for OpenShift \u00b6 One of the great things about Quarkus is the plethora of extensions it provides out of the box. Quarkus extensions are comparable to Maven dependencies that allow for much easier use and integration into 3 rd party projects. We will be using the Quarkus OpenShift extension . The OpenShift extension is actually a wrapper that brings together the kubernetes and container-image-s2i extensions with defaults specific to OpenShift. In your terminal session, add the OpenShift extension to you application with the command : ./mvnw quarkus:add-extension -Dextensions=\"openshift\" Example Output user01@lab061:~/openshift-quickstart$ ./mvnw quarkus:add-extension -Dextensions=\"openshift\" [INFO] Scanning for projects... [INFO] [INFO] -------------------< org.acme:openshift-quickstart >-------------------- [INFO] Building openshift-quickstart 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- quarkus-maven-plugin:1.9.0.CR1:add-extension (default-cli) @ openshift-quickstart --- ? Extension io.quarkus:quarkus-openshift has been installed [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 11.356 s [INFO] Finished at: 2020-10-23T15:42:02-04:00 [INFO] ------------------------------------------------------------------------ This command added the following dependency to your pom.xml file: Example Output <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-openshift </artifactId> </dependency> This dependency is the generic quarkus.openshift extension, but it can be further customized with essentially any further nested dependency for OpenShift or Kubernetes objects you need. Some examples are in the table below, and the full list is here: https://quarkus.io/guides/kubernetes#openshift . Expand for more Information Property Type quarkus.openshift.version String quarkus.openshift.env-vars Map quarkus.openshift.replicas int quarkus.openshift.service-account String quarkus.openshift.host String quarkus.openshift.ports Map quarkus.openshift.pvc-volumes Map quarkus.openshift.image-pull-policy ImagePullPolicy quarkus.openshift.image-pull-secrets String[] quarkus.openshift.liveness-probe Probe quarkus.openshift.readiness-probe Probe quarkus.openshift.expose boolean You will be using a few of these customizations when you deploy the application to OpenShift in the next step.","title":"Configure the Application for OpenShift"},{"location":"lab008/lab008-4/#configure-the-application-for-openshift","text":"One of the great things about Quarkus is the plethora of extensions it provides out of the box. Quarkus extensions are comparable to Maven dependencies that allow for much easier use and integration into 3 rd party projects. We will be using the Quarkus OpenShift extension . The OpenShift extension is actually a wrapper that brings together the kubernetes and container-image-s2i extensions with defaults specific to OpenShift. In your terminal session, add the OpenShift extension to you application with the command : ./mvnw quarkus:add-extension -Dextensions=\"openshift\" Example Output user01@lab061:~/openshift-quickstart$ ./mvnw quarkus:add-extension -Dextensions=\"openshift\" [INFO] Scanning for projects... [INFO] [INFO] -------------------< org.acme:openshift-quickstart >-------------------- [INFO] Building openshift-quickstart 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- quarkus-maven-plugin:1.9.0.CR1:add-extension (default-cli) @ openshift-quickstart --- ? Extension io.quarkus:quarkus-openshift has been installed [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 11.356 s [INFO] Finished at: 2020-10-23T15:42:02-04:00 [INFO] ------------------------------------------------------------------------ This command added the following dependency to your pom.xml file: Example Output <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-openshift </artifactId> </dependency> This dependency is the generic quarkus.openshift extension, but it can be further customized with essentially any further nested dependency for OpenShift or Kubernetes objects you need. Some examples are in the table below, and the full list is here: https://quarkus.io/guides/kubernetes#openshift . Expand for more Information Property Type quarkus.openshift.version String quarkus.openshift.env-vars Map quarkus.openshift.replicas int quarkus.openshift.service-account String quarkus.openshift.host String quarkus.openshift.ports Map quarkus.openshift.pvc-volumes Map quarkus.openshift.image-pull-policy ImagePullPolicy quarkus.openshift.image-pull-secrets String[] quarkus.openshift.liveness-probe Probe quarkus.openshift.readiness-probe Probe quarkus.openshift.expose boolean You will be using a few of these customizations when you deploy the application to OpenShift in the next step.","title":"Configure the Application for OpenShift"},{"location":"lab008/lab008-5/","text":"Deploy the Application onto OpenShift \u00b6 Let\u2019s now take our local application and use the Quarkus extension we just added to build and deploy a containerized application onto OpenShift. In the openshift-quickstart directory, run the command : ./mvnw clean package \\ -Dquarkus.kubernetes.deploy=true \\ -Dquarkus.kubernetes-client.trust-certs=true \\ -Dquarkus.openshift.expose=true The -Dquarkus flags in this command are telling Maven to deploy the application into the Kubernetes (OpenShift) cluster, trust the certificates, and expose the application service as a route, eliminating the need to run an oc expose svc to make the service endpoint accessible outside of the cluster. Information This command may take a few minutes to complete. Example Output user01@lab061:~/openshift-quickstart$ ./mvnw clean package \\ -Dquarkus.kubernetes.deploy=true \\ -Dquarkus.kubernetes-client.trust-certs=true \\ -Dquarkus.openshift.expose=true [INFO] Scanning for projects... [INFO] [INFO] -------------------< org.acme:openshift-quickstart >-------------------- [INFO] Building openshift-quickstart 1.0.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] ...omitted... [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] The deployed application can be accessed at: http://openshift-quickstart-user01-project.apps.atsocppa.dmz [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 94655ms [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 03:19 min [INFO] Finished at: 2021-03-09T12:12:14-05:00 [INFO] ------------------------------------------------------------------------ The previous command builds a jar file locally, connects to the OpenShift cluster you previously logged into, triggers a container image build, pushes that container image into the OpenShift internal registry, generates OpenShift/Kubernetes resources including a Service, Route, DeploymentConfig, and your running application pod. View all of the created objects with the following command : oc get all Example Output user01@lab061: ~/openshift-quickstart$ oc get all NAME READY STATUS RESTARTS AGE pod/openshift-quickstart-1-build 0/1 Completed 0 9m10s pod/openshift-quickstart-1-deploy 0/1 Completed 0 8m13s pod/openshift-quickstart-1-ndp8h 1/1 Running 0 8m10s NAME DESIRED CURRENT READY AGE replicationcontroller/openshift-quickstart-1 1 1 1 8m13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/openshift-quickstart ClusterIP 172.30.28.241 <none> 8080/TCP 8m14s NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/openshift-quickstart 1 1 1 image(openshift-quickstart:1.0-SNAPSHOT) NAME TYPE FROM LATEST buildconfig.build.openshift.io/openshift-quickstart Source Binary 1 NAME TYPE FROM STATUS STARTED DURATION build.build.openshift.io/openshift-quickstart-1 Source Binary Complete 9 minutes ago 55s NAME IMAGE REPOSITORY TAGS UPDATED imagestream.image.openshift.io/openjdk-11 default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/openjdk-11 1.3,1.3-3,1.3-3.1591609340 + 18 more... 9 minutes ago imagestream.image.openshift.io/openshift-quickstart default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/openshift-quickstart 1.0-SNAPSHOT 8 minutes ago NAME HOST/PORT PATH SERVICES PORT TERMINATION route.route.openshift.io/openshift-quickstart openshift-quickstart-user01-project.apps.atsocppa.dmz / openshift-quickstart 8080 Each of these objects were created because of the -Dquarkus.kubernetes.deploy=true and -Dquarkus.openshift.expose=true flags provided in the previous command. There are many more OpenShift objects and object properties that can be created by passing different flags, such as liveliness probes, environment variables, secrets, persistent storage, and more. If you have one running pod, your application has successfully deployed and is accessible at the route. In a web browser, navigate to your route : Hint It will be http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/ where NN is your user number. Your Quarkus application is now deployed as a container in OpenShift. Earlier we looked at the GreetingResource.java REST endpoint and its return of \u201chello\u201d in the command line. We can do the same thing in the web browser. Add /greeting to the end of your route . Hint http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/greeting where NN is your user number. In this lab, you have created a Quarkus application locally, containerized the application and deployed it onto an OpenShift cluster running on IBM Z, and accessed it from a public route. The speed, agility, and ease with which we\u2019re able to edit and redeploy applications using the Quarkus runtime creates tremendous value in time savings, allowing developers and operations staff to minimize downtime and keep applications up to date. Further, the simplicity of integration using Quarkus extensions creates great opportunity for customization and implementations tailored to fit a variety of needs.","title":"Deploy the Application onto OpenShift"},{"location":"lab008/lab008-5/#deploy-the-application-onto-openshift","text":"Let\u2019s now take our local application and use the Quarkus extension we just added to build and deploy a containerized application onto OpenShift. In the openshift-quickstart directory, run the command : ./mvnw clean package \\ -Dquarkus.kubernetes.deploy=true \\ -Dquarkus.kubernetes-client.trust-certs=true \\ -Dquarkus.openshift.expose=true The -Dquarkus flags in this command are telling Maven to deploy the application into the Kubernetes (OpenShift) cluster, trust the certificates, and expose the application service as a route, eliminating the need to run an oc expose svc to make the service endpoint accessible outside of the cluster. Information This command may take a few minutes to complete. Example Output user01@lab061:~/openshift-quickstart$ ./mvnw clean package \\ -Dquarkus.kubernetes.deploy=true \\ -Dquarkus.kubernetes-client.trust-certs=true \\ -Dquarkus.openshift.expose=true [INFO] Scanning for projects... [INFO] [INFO] -------------------< org.acme:openshift-quickstart >-------------------- [INFO] Building openshift-quickstart 1.0.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] ...omitted... [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] The deployed application can be accessed at: http://openshift-quickstart-user01-project.apps.atsocppa.dmz [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 94655ms [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 03:19 min [INFO] Finished at: 2021-03-09T12:12:14-05:00 [INFO] ------------------------------------------------------------------------ The previous command builds a jar file locally, connects to the OpenShift cluster you previously logged into, triggers a container image build, pushes that container image into the OpenShift internal registry, generates OpenShift/Kubernetes resources including a Service, Route, DeploymentConfig, and your running application pod. View all of the created objects with the following command : oc get all Example Output user01@lab061: ~/openshift-quickstart$ oc get all NAME READY STATUS RESTARTS AGE pod/openshift-quickstart-1-build 0/1 Completed 0 9m10s pod/openshift-quickstart-1-deploy 0/1 Completed 0 8m13s pod/openshift-quickstart-1-ndp8h 1/1 Running 0 8m10s NAME DESIRED CURRENT READY AGE replicationcontroller/openshift-quickstart-1 1 1 1 8m13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/openshift-quickstart ClusterIP 172.30.28.241 <none> 8080/TCP 8m14s NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/openshift-quickstart 1 1 1 image(openshift-quickstart:1.0-SNAPSHOT) NAME TYPE FROM LATEST buildconfig.build.openshift.io/openshift-quickstart Source Binary 1 NAME TYPE FROM STATUS STARTED DURATION build.build.openshift.io/openshift-quickstart-1 Source Binary Complete 9 minutes ago 55s NAME IMAGE REPOSITORY TAGS UPDATED imagestream.image.openshift.io/openjdk-11 default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/openjdk-11 1.3,1.3-3,1.3-3.1591609340 + 18 more... 9 minutes ago imagestream.image.openshift.io/openshift-quickstart default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/openshift-quickstart 1.0-SNAPSHOT 8 minutes ago NAME HOST/PORT PATH SERVICES PORT TERMINATION route.route.openshift.io/openshift-quickstart openshift-quickstart-user01-project.apps.atsocppa.dmz / openshift-quickstart 8080 Each of these objects were created because of the -Dquarkus.kubernetes.deploy=true and -Dquarkus.openshift.expose=true flags provided in the previous command. There are many more OpenShift objects and object properties that can be created by passing different flags, such as liveliness probes, environment variables, secrets, persistent storage, and more. If you have one running pod, your application has successfully deployed and is accessible at the route. In a web browser, navigate to your route : Hint It will be http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/ where NN is your user number. Your Quarkus application is now deployed as a container in OpenShift. Earlier we looked at the GreetingResource.java REST endpoint and its return of \u201chello\u201d in the command line. We can do the same thing in the web browser. Add /greeting to the end of your route . Hint http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/greeting where NN is your user number. In this lab, you have created a Quarkus application locally, containerized the application and deployed it onto an OpenShift cluster running on IBM Z, and accessed it from a public route. The speed, agility, and ease with which we\u2019re able to edit and redeploy applications using the Quarkus runtime creates tremendous value in time savings, allowing developers and operations staff to minimize downtime and keep applications up to date. Further, the simplicity of integration using Quarkus extensions creates great opportunity for customization and implementations tailored to fit a variety of needs.","title":"Deploy the Application onto OpenShift"},{"location":"lab008/lab008-6/","text":"Cleaning Up \u00b6 Double check that you are in your own userNN-project by issuing the command : oc project Example Output user01@lab061 ~/openshift-quickstart $ oc project Using project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\". Once you\u2019re sure you\u2019re in your own project, issue the following command to delete all objects associated with your application labeled app.kubernetes.io/name=openshift-quickstart. oc delete all --selector app.kubernetes.io/name=openshift-quickstart Example Output user01@lab061:~/openshift-quickstart $ oc delete all --selector app.kubernetes.io/name=openshift-quickstart pod \"openshift-quickstart-1-ztfq9\" deleted replicationcontroller \"openshift-quickstart-1\" deleted service \"openshift-quickstart\" deleted deploymentconfig.apps.openshift.io \"openshift-quickstart\" deleted buildconfig.build.openshift.io \"openshift-quickstart\" deleted build.build.openshift.io \"openshift-quickstart-1\" deleted imagestream.image.openshift.io \"openjdk-11\" deleted imagestream.image.openshift.io \"openshift-quickstart\" deleted route.route.openshift.io \"openshift-quickstart\" deleted To check that all of your mongo application resources were deleted, run the command : oc get all Example Output user01@lab061:~/openshift-quickstart $ oc get all No resources found. user00@lab061:~$ Note If there are leftover resources from other labs that you would like to delete, run the command: oc delete all --all","title":"Cleaning Up"},{"location":"lab008/lab008-6/#cleaning-up","text":"Double check that you are in your own userNN-project by issuing the command : oc project Example Output user01@lab061 ~/openshift-quickstart $ oc project Using project \"user01-project\" on server \"https://api.atsocppa.dmz:6443\". Once you\u2019re sure you\u2019re in your own project, issue the following command to delete all objects associated with your application labeled app.kubernetes.io/name=openshift-quickstart. oc delete all --selector app.kubernetes.io/name=openshift-quickstart Example Output user01@lab061:~/openshift-quickstart $ oc delete all --selector app.kubernetes.io/name=openshift-quickstart pod \"openshift-quickstart-1-ztfq9\" deleted replicationcontroller \"openshift-quickstart-1\" deleted service \"openshift-quickstart\" deleted deploymentconfig.apps.openshift.io \"openshift-quickstart\" deleted buildconfig.build.openshift.io \"openshift-quickstart\" deleted build.build.openshift.io \"openshift-quickstart-1\" deleted imagestream.image.openshift.io \"openjdk-11\" deleted imagestream.image.openshift.io \"openshift-quickstart\" deleted route.route.openshift.io \"openshift-quickstart\" deleted To check that all of your mongo application resources were deleted, run the command : oc get all Example Output user01@lab061:~/openshift-quickstart $ oc get all No resources found. user00@lab061:~$ Note If there are leftover resources from other labs that you would like to delete, run the command: oc delete all --all","title":"Cleaning Up"},{"location":"lab009/lab009-0/","text":"Using OpenShift Pipelines \u00b6 Red Hat OpenShift Pipelines is a cloud-native, continuous integration and continuous delivery (CI/CD) solution based on Kubernetes resources. It uses Tekton building blocks to automate deployments across multiple platforms by abstracting away the underlying implementation details. Tekton introduces a number of standard custom resource definitions (CRDs) for defining CI/CD pipelines that are portable across Kubernetes distributions. Key features \u00b6 Red Hat OpenShift Pipelines is a serverless CI/CD system that runs pipelines with all the required dependencies in isolated containers . Red Hat OpenShift Pipelines are designed for decentralized teams that work on microservice-based architecture. Red Hat OpenShift Pipelines use standard CI/CD pipeline definitions that are easy to extend and integrate with the existing Kubernetes tools, enabling you to scale on-demand. You can use Red Hat OpenShift Pipelines to build images with Kubernetes tools such as Source-to-Image (S2I), Buildah, Buildpacks, and Kaniko that are portable across any Kubernetes platform. You can use the OpenShift Container Platform Developer console to create Tekton resources, view logs of pipeline runs, and manage pipelines in your OpenShift Container Platform namespaces. What is Tekton? \u00b6 Tekton is an open source project that provides a framework to create cloud-native CI/CD pipelines quickly. As a Kubernetes-native framework, Tekton makes it easier to deploy across multiple cloud providers or hybrid environments. By leveraging the Custom Resource Definitions (CRDs) in Kubernetes, Tekton uses the Kubernetes control plane to run pipeline tasks. By using standard industry specifications, Tekton will work well with existing CI/CD tools such as Jenkins, Jenkins X, Skaffold, Knative, and now OpenShift. Source of images and information on this page: https://cloud.redhat.com/learn/topics/ci-cd","title":"Introduction to OpenShift Pipelines"},{"location":"lab009/lab009-0/#using-openshift-pipelines","text":"Red Hat OpenShift Pipelines is a cloud-native, continuous integration and continuous delivery (CI/CD) solution based on Kubernetes resources. It uses Tekton building blocks to automate deployments across multiple platforms by abstracting away the underlying implementation details. Tekton introduces a number of standard custom resource definitions (CRDs) for defining CI/CD pipelines that are portable across Kubernetes distributions.","title":"Using OpenShift Pipelines"},{"location":"lab009/lab009-0/#key-features","text":"Red Hat OpenShift Pipelines is a serverless CI/CD system that runs pipelines with all the required dependencies in isolated containers . Red Hat OpenShift Pipelines are designed for decentralized teams that work on microservice-based architecture. Red Hat OpenShift Pipelines use standard CI/CD pipeline definitions that are easy to extend and integrate with the existing Kubernetes tools, enabling you to scale on-demand. You can use Red Hat OpenShift Pipelines to build images with Kubernetes tools such as Source-to-Image (S2I), Buildah, Buildpacks, and Kaniko that are portable across any Kubernetes platform. You can use the OpenShift Container Platform Developer console to create Tekton resources, view logs of pipeline runs, and manage pipelines in your OpenShift Container Platform namespaces.","title":"Key features"},{"location":"lab009/lab009-0/#what-is-tekton","text":"Tekton is an open source project that provides a framework to create cloud-native CI/CD pipelines quickly. As a Kubernetes-native framework, Tekton makes it easier to deploy across multiple cloud providers or hybrid environments. By leveraging the Custom Resource Definitions (CRDs) in Kubernetes, Tekton uses the Kubernetes control plane to run pipeline tasks. By using standard industry specifications, Tekton will work well with existing CI/CD tools such as Jenkins, Jenkins X, Skaffold, Knative, and now OpenShift. Source of images and information on this page: https://cloud.redhat.com/learn/topics/ci-cd","title":"What is Tekton?"},{"location":"lab009/lab009-1/","text":"Using OpenShift Pipelines \u00b6 In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session . ssh into the Linux Guest server : ssh userNN@192.168.176.61 Where NN is your user number. When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab009/lab009-1/#using-openshift-pipelines","text":"In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session . ssh into the Linux Guest server : ssh userNN@192.168.176.61 Where NN is your user number. When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Using OpenShift Pipelines"},{"location":"lab009/lab009-2/","text":"Cloning the GitHub Repository and Viewing its Contents \u00b6 In the terminal session, you should have been automatically placed in your home directory /home/userNN (where NN is your user number). Run the command pwd to check your current working directory . Example Output user01@lab061:~$ pwd /home/user01 If you are in any other directory, change into the correct home directory using the command : cd /home/userNN (Where NN is your user number). Example Output user01@lab061:~$ cd /home/user01 user01@lab061:~$ pwd /home/user01 In your home directory, clone the OpenShift Pipelines repository using the command : git clone https://github.com/mmondics/openshift-pipelines-s390x Example Output user01@lab061:~$ git clone https://github.com/mmondics/openshift-pipelines-s390x Cloning into 'openshift-pipelines-s390x'... remote: Enumerating objects: 25, done. remote: Counting objects: 100% (25/25), done. remote: Compressing objects: 100% (21/21), done. remote: Total 25 (delta 5), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (25/25), done. Checking connectivity... done This will create a new directory called openliberty-pipelines-s390x . Change into this directory using the command : cd openshift-pipelines-s390x List its contents using the command : ls -l Example Output user01@lab061:~$ cd openliberty-operator-ocpz user01@lab061:~/openliberty-operator-ocpz$ ls -l total 16 -rw-r--r-- 1 user00 users 48 Mar 16 14:20 README.md drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 pipeline -rw-r--r-- 1 user00 users 251 Mar 22 13:23 pipeline-cleanup.sh drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 resources drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 tasks If you navigate to the GitHub repository in a web browser https://github.com/mmondics/openshift-pipelines-s390x , you will notice that the sub-directories in your Linux session reflect the folders contained in the repository. File Description README.MD Contains the content displayed on the GitHub page for this repository. You can read through this README file if you want to get more information about this lab. pipeline Directory containing the YAML file that will be used to create a Pipeline. pipeline-cleanup.sh Shell script that will delete all objects created in this lab. resources Directory containing the YAML file that will create a PersistentVolumeClaim in the cluster. tasks Directory containing YAML files to create various Tasks that make up a pipeline.","title":"Cloning the GitHub Repository and Viewing its Contents"},{"location":"lab009/lab009-2/#cloning-the-github-repository-and-viewing-its-contents","text":"In the terminal session, you should have been automatically placed in your home directory /home/userNN (where NN is your user number). Run the command pwd to check your current working directory . Example Output user01@lab061:~$ pwd /home/user01 If you are in any other directory, change into the correct home directory using the command : cd /home/userNN (Where NN is your user number). Example Output user01@lab061:~$ cd /home/user01 user01@lab061:~$ pwd /home/user01 In your home directory, clone the OpenShift Pipelines repository using the command : git clone https://github.com/mmondics/openshift-pipelines-s390x Example Output user01@lab061:~$ git clone https://github.com/mmondics/openshift-pipelines-s390x Cloning into 'openshift-pipelines-s390x'... remote: Enumerating objects: 25, done. remote: Counting objects: 100% (25/25), done. remote: Compressing objects: 100% (21/21), done. remote: Total 25 (delta 5), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (25/25), done. Checking connectivity... done This will create a new directory called openliberty-pipelines-s390x . Change into this directory using the command : cd openshift-pipelines-s390x List its contents using the command : ls -l Example Output user01@lab061:~$ cd openliberty-operator-ocpz user01@lab061:~/openliberty-operator-ocpz$ ls -l total 16 -rw-r--r-- 1 user00 users 48 Mar 16 14:20 README.md drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 pipeline -rw-r--r-- 1 user00 users 251 Mar 22 13:23 pipeline-cleanup.sh drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 resources drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 tasks If you navigate to the GitHub repository in a web browser https://github.com/mmondics/openshift-pipelines-s390x , you will notice that the sub-directories in your Linux session reflect the folders contained in the repository. File Description README.MD Contains the content displayed on the GitHub page for this repository. You can read through this README file if you want to get more information about this lab. pipeline Directory containing the YAML file that will be used to create a Pipeline. pipeline-cleanup.sh Shell script that will delete all objects created in this lab. resources Directory containing the YAML file that will create a PersistentVolumeClaim in the cluster. tasks Directory containing YAML files to create various Tasks that make up a pipeline.","title":"Cloning the GitHub Repository and Viewing its Contents"},{"location":"lab009/lab009-3/","text":"Understanding and Deploying Tasks \u00b6 A Task defines a series of steps that run in a desired order and complete a set amount of build work. Every Task runs as a Pod on your OpenShift cluster with each step as its own container. Tasks have one single responsibility so they can be reused across different Pipelines or in multiple places within a single Pipeline. The repository you pulled includes the YAML files needed to create three Tasks. Let\u2019s take a look at one of them. From the openshift-pipelines-s390x directory, run the command : cat tasks/hello.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ cat tasks/hello.yaml apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: steps: - name: say-hello image: registry.access.redhat.com/ubi8/ubi command: - /bin/text args: ['-c', 'echo Hello World'] This file will create a Kubernetes Task object called hello that is made up of one step. That step has its own name, image, command, and args associated with it. As explained above, once created, this Task will create one Pod that includes one Container. Create the Task using the command : oc create -f tasks/hello.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/hello.yaml task.tekton.dev/hello created The Task is now created in your project and can be run using Tekton, the CI/CD tool that OpenShift Pipelines are based on. Run the hello task using the command : tkn task start --showlog hello Example Output user01@lab061:~/openshift-pipelines-s390x$ tkn task start --showlog hello TaskRun started: hello-run-xvr92 Waiting for logs to be available... [say-hello] Hello World Running the tkn task start command created a new Kubernetes resource called a TaskRun. TaskRuns are automatically created for each Task that is run in a Pipeline, but as you can see, they can also be manually created by running a Task. This can be useful for debugging a single Task in a Pipeline. Your Pipeline will consist of three Tasks total. Create the remaining Tasks using the commands : oc create -f tasks/apply_manifest_task.yaml and oc create -f tasks/update_deployment_task.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/apply_manifest_task.yaml task.tekton.dev/apply-manifests created user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/update_deployment_task.yaml task.tekton.dev/update-deployment created You have now created three Tasks that will be plumbed together to create a Pipeline. To see them, run the command : tkn task ls Example Output user01@lab061:~/openshift-pipelines-s390x$ tkn task ls NAME DESCRIPTION AGE apply-manifests 7 minutes ago hello 8 minutes ago update-deployment 7 minutes ago You will also need a Workspace in which your will run all of the Tasks associated with your Pipeline. This will be a shared space across each Task, TaskRun, Pipeline, and PipelineRun that you associate with the Workspace. With a Workspace, you can store Task inputs and outputs, share data among Tasks, use it as a mount point for credentials held in Secrets, create a cache of build artifacts that speed up jobs, and more. In our case, we will be using a PersistentVolumeClaim as our Workspace. Create the PVC using the command : oc create -f resources/persistent_volume_claim.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ oc create -f resources/persistent_volume_claim.yaml persistentvolumeclaim/source-pvc created In the next section, you will create a Pipeline that uses the Tasks and Workspace you just created to pull the source code of an application from GitHub and then builds and deploys it in a container on OpenShift.","title":"Understanding and Deploying Tasks"},{"location":"lab009/lab009-3/#understanding-and-deploying-tasks","text":"A Task defines a series of steps that run in a desired order and complete a set amount of build work. Every Task runs as a Pod on your OpenShift cluster with each step as its own container. Tasks have one single responsibility so they can be reused across different Pipelines or in multiple places within a single Pipeline. The repository you pulled includes the YAML files needed to create three Tasks. Let\u2019s take a look at one of them. From the openshift-pipelines-s390x directory, run the command : cat tasks/hello.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ cat tasks/hello.yaml apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: steps: - name: say-hello image: registry.access.redhat.com/ubi8/ubi command: - /bin/text args: ['-c', 'echo Hello World'] This file will create a Kubernetes Task object called hello that is made up of one step. That step has its own name, image, command, and args associated with it. As explained above, once created, this Task will create one Pod that includes one Container. Create the Task using the command : oc create -f tasks/hello.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/hello.yaml task.tekton.dev/hello created The Task is now created in your project and can be run using Tekton, the CI/CD tool that OpenShift Pipelines are based on. Run the hello task using the command : tkn task start --showlog hello Example Output user01@lab061:~/openshift-pipelines-s390x$ tkn task start --showlog hello TaskRun started: hello-run-xvr92 Waiting for logs to be available... [say-hello] Hello World Running the tkn task start command created a new Kubernetes resource called a TaskRun. TaskRuns are automatically created for each Task that is run in a Pipeline, but as you can see, they can also be manually created by running a Task. This can be useful for debugging a single Task in a Pipeline. Your Pipeline will consist of three Tasks total. Create the remaining Tasks using the commands : oc create -f tasks/apply_manifest_task.yaml and oc create -f tasks/update_deployment_task.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/apply_manifest_task.yaml task.tekton.dev/apply-manifests created user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/update_deployment_task.yaml task.tekton.dev/update-deployment created You have now created three Tasks that will be plumbed together to create a Pipeline. To see them, run the command : tkn task ls Example Output user01@lab061:~/openshift-pipelines-s390x$ tkn task ls NAME DESCRIPTION AGE apply-manifests 7 minutes ago hello 8 minutes ago update-deployment 7 minutes ago You will also need a Workspace in which your will run all of the Tasks associated with your Pipeline. This will be a shared space across each Task, TaskRun, Pipeline, and PipelineRun that you associate with the Workspace. With a Workspace, you can store Task inputs and outputs, share data among Tasks, use it as a mount point for credentials held in Secrets, create a cache of build artifacts that speed up jobs, and more. In our case, we will be using a PersistentVolumeClaim as our Workspace. Create the PVC using the command : oc create -f resources/persistent_volume_claim.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ oc create -f resources/persistent_volume_claim.yaml persistentvolumeclaim/source-pvc created In the next section, you will create a Pipeline that uses the Tasks and Workspace you just created to pull the source code of an application from GitHub and then builds and deploys it in a container on OpenShift.","title":"Understanding and Deploying Tasks"},{"location":"lab009/lab009-4/","text":"Understanding and Deploying Pipelines \u00b6 A Pipeline consists of a series of Tasks that are executed to construct complex workflows that automate the build, deployment, and delivery of applications. It is a collection of PipelineResources, parameters, and one or more Tasks. Below is a diagram of the Pipeline you will be creating. The repository you pulled provides the YAML file necessary to generate this Pipeline. Take a look at the YAML by using the command : cat pipeline/pipeline.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ cat tasks/hello.yaml apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: build-and-deploy spec: workspaces: - name: shared-workspace params: - name: deployment-name type: string description: name of the deployment to be patched - name: git-url type: string description: url of the git repo for the code of deployment - name: git-revision type: string description: revision to be used from repo of the code for deployment default: \"master\" - name: IMAGE type: string description: image to be build from the code tasks: - name: fetch-repository taskRef: name: git-clone kind: ClusterTask workspaces: - name: output workspace: shared-workspace params: - name: url value: $(params.git-url) - name: subdirectory value: \"\" - name: deleteExisting value: \"true\" - name: revision value: $(params.git-revision) - name: build-image taskRef: name: buildah kind: ClusterTask params: - name: TLSVERIFY value: \"false\" - name: IMAGE value: $(params.IMAGE) workspaces: - name: source workspace: shared-workspace runAfter: - fetch-repository - name: apply-manifests taskRef: name: apply-manifests workspaces: - name: source workspace: shared-workspace runAfter: - build-image - name: update-deployment taskRef: name: update-deployment params: - name: deployment value: $(params.deployment-name) - name: IMAGE value: $(params.IMAGE) runAfter: - apply-manifests The Tasks included in this pipeline and their responsibilities are as follows: fetch-repository clones the source code of the application from a GitHub repository based on the git-url and git-revision parameters. build-image builds the container image of the application using Buildah. apply-manifests deploys the application to OpenShift by running the oc apply command on the new container image with the provided parameters. update-deployment will update the application in OpenShift with the oc patch command when changes are needed. You will notice that there are no references to the GitHub repository or the image registry that will be pushed to in the pipeline. This is because Pipelines are designed to be generic and re-used in different situations or to deploy different applications. Pipelines abstract away the specific parameters that can be passed into the Pipeline. When triggering the Pipeline, you will provide different GitHub repositories and images to be used when executed. Also notice that the execution order of Tasks can be determined by dependencies defined between Tasks via inputs and outputs, or explicitly ordered via runAfter. Create the Pipeline with the command : oc create -f pipeline/pipeline.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ oc create -f pipeline/pipeline.yaml pipeline.tekton.dev/build-and-deploy created Although we are using pre-built YAML files to simplify the creation of these resources, everything in this lab could also be done in the OpenShift console in a browser. Take a look at the graphical representation of your Pipeline by accessing the cluster at the URL: https://console-openshift-console.apps.atsocppa.dmz/ Username: userNN (where NN is your user number) Password: p@ssw0rd Navigate to the Developer Perspective -> Pipelines -> select your userNN Project . Click your new Pipeline called build-and-deploy . The framework of your Pipeline has been created, and you can see the four Tasks that make up your Pipeline. Information If you remember making the apply-manifests and update-deployment Tasks, but not the \u201cfetch-repository\u201d and \u201cbuild-image\u201d Tasks -- you aren\u2019t wrong. These are ClusterTasks that come pre-built into OpenShift. In the next section you will trigger a PipelineRun to execute your Pipeline and the Tasks it contains.","title":"Understanding and Deploying Pipelines"},{"location":"lab009/lab009-4/#understanding-and-deploying-pipelines","text":"A Pipeline consists of a series of Tasks that are executed to construct complex workflows that automate the build, deployment, and delivery of applications. It is a collection of PipelineResources, parameters, and one or more Tasks. Below is a diagram of the Pipeline you will be creating. The repository you pulled provides the YAML file necessary to generate this Pipeline. Take a look at the YAML by using the command : cat pipeline/pipeline.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ cat tasks/hello.yaml apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: build-and-deploy spec: workspaces: - name: shared-workspace params: - name: deployment-name type: string description: name of the deployment to be patched - name: git-url type: string description: url of the git repo for the code of deployment - name: git-revision type: string description: revision to be used from repo of the code for deployment default: \"master\" - name: IMAGE type: string description: image to be build from the code tasks: - name: fetch-repository taskRef: name: git-clone kind: ClusterTask workspaces: - name: output workspace: shared-workspace params: - name: url value: $(params.git-url) - name: subdirectory value: \"\" - name: deleteExisting value: \"true\" - name: revision value: $(params.git-revision) - name: build-image taskRef: name: buildah kind: ClusterTask params: - name: TLSVERIFY value: \"false\" - name: IMAGE value: $(params.IMAGE) workspaces: - name: source workspace: shared-workspace runAfter: - fetch-repository - name: apply-manifests taskRef: name: apply-manifests workspaces: - name: source workspace: shared-workspace runAfter: - build-image - name: update-deployment taskRef: name: update-deployment params: - name: deployment value: $(params.deployment-name) - name: IMAGE value: $(params.IMAGE) runAfter: - apply-manifests The Tasks included in this pipeline and their responsibilities are as follows: fetch-repository clones the source code of the application from a GitHub repository based on the git-url and git-revision parameters. build-image builds the container image of the application using Buildah. apply-manifests deploys the application to OpenShift by running the oc apply command on the new container image with the provided parameters. update-deployment will update the application in OpenShift with the oc patch command when changes are needed. You will notice that there are no references to the GitHub repository or the image registry that will be pushed to in the pipeline. This is because Pipelines are designed to be generic and re-used in different situations or to deploy different applications. Pipelines abstract away the specific parameters that can be passed into the Pipeline. When triggering the Pipeline, you will provide different GitHub repositories and images to be used when executed. Also notice that the execution order of Tasks can be determined by dependencies defined between Tasks via inputs and outputs, or explicitly ordered via runAfter. Create the Pipeline with the command : oc create -f pipeline/pipeline.yaml Example Output user01@lab061:~/openshift-pipelines-s390x$ oc create -f pipeline/pipeline.yaml pipeline.tekton.dev/build-and-deploy created Although we are using pre-built YAML files to simplify the creation of these resources, everything in this lab could also be done in the OpenShift console in a browser. Take a look at the graphical representation of your Pipeline by accessing the cluster at the URL: https://console-openshift-console.apps.atsocppa.dmz/ Username: userNN (where NN is your user number) Password: p@ssw0rd Navigate to the Developer Perspective -> Pipelines -> select your userNN Project . Click your new Pipeline called build-and-deploy . The framework of your Pipeline has been created, and you can see the four Tasks that make up your Pipeline. Information If you remember making the apply-manifests and update-deployment Tasks, but not the \u201cfetch-repository\u201d and \u201cbuild-image\u201d Tasks -- you aren\u2019t wrong. These are ClusterTasks that come pre-built into OpenShift. In the next section you will trigger a PipelineRun to execute your Pipeline and the Tasks it contains.","title":"Understanding and Deploying Pipelines"},{"location":"lab009/lab009-5/","text":"Running the Pipeline \u00b6 Let\u2019s use this Pipeline to create an application. To demonstrate the re-usability of OpenShift Pipelines, we will be creating both a frontend and a backend with the same Pipeline you created in the previous step. We\u2019ll also demonstrate the flexibility provided by OpenShift Pipelines that lets you use them from either the web console or the command line. Let\u2019s create the backend application with the Tekton CLI in your terminal. Now that you have all of the building blocks in place, you can start the Pipeline with the following command. The command will run the Pipeline and pass in parameters to: Use the shared workspace and the PersistentVolumeClaim you created Create the deployment named vote-api Build a container image from the source code at the given GitHub repository Push that container image into the OpenShift internal registry and tag it for your project Show the log so you can follow its progress Note that the forward slash simply breaks the command into multiple lines for readability. Creating the Backend Application through the CLI \u00b6 Run the following command : tkn pipeline start build-and-deploy \\ -w name=shared-workspace,claimName=source-pvc \\ -p deployment-name=vote-api \\ -p git-url=https://github.com/mmondics/pipelines-vote-api.git \\ -p IMAGE=image-registry.openshift-image-registry.svc:5000/userNN-project/pipelines-vote-api --showlog Important Make sure you change the one instance of NN to your team number in the command above. Expand for Example Output user01@lab061:~/openshift-pipelines-s390x$ tkn pipeline start build-and-deploy \\ > -w name=shared-workspace,claimName=source-pvc \\ > -p deployment-name=vote-api \\ > -p git-url=https://github.com/mmondics/vote-api.git \\ > -p IMAGE=image-registry.openshift-image-registry.svc:5000/userNN-project/vote-api \\ > --showlog PipelineRun started: build-and-deploy-run-75zqv Waiting for logs to be available... [fetch-respository : clone] + CHECKOUT_DIR=/workspace/output/ [fetch-respository : clone] + [[ true == \\t\\r\\u\\e ]] [fetch-respository : clone] + cleandir [fetch-respository : clone] + [[ -d /workspace/output/ ]] [fetch-respository : clone] + rm -rf /workspace/output//Dockerfile /workspace/output//LICENSE /workspace/output//README.md /workspace/output//go.mod /workspace/output//go.sum /workspace/output//k8s /workspace/output//main.go /workspace/output//vendor [fetch-respository : clone] + rm -rf /workspace/output//.git /workspace/output//.gitignore [fetch-respository : clone] + rm -rf '/workspace/output//..?*' [fetch-respository : clone] + test -z '' [fetch-respository : clone] + test -z '' [fetch-respository : clone] + test -z '' [fetch-respository : clone] + /ko-app/git-init -url https://github.com/mmondics/vote-api.git -revision master -refspec '' -path /workspace/output/ -sslVerify=true -submodules=true -depth 1 [fetch-respository : clone] {\"level\":\"info\",\"ts\":1616101272.5251348,\"caller\":\"git/git.go:165\",\"msg\":\"Successfully cloned https://github.com/mmondics/vote-api.git @ a08f579f6135293358b9423a3370e725ae1380cc (grafted, HEAD, origin/master) in path /workspace/output/\"} [fetch-respository : clone] {\"level\":\"info\",\"ts\":1616101272.6701891,\"caller\":\"git/git.go:203\",\"msg\":\"Successfully initialized and updated submodules in path /workspace/output/\"} [fetch-respository : clone] + cd /workspace/output/ [fetch-respository : clone] ++ git rev-parse HEAD [fetch-respository : clone] + RESULT_SHA=a08f579f6135293358b9423a3370e725ae1380cc [fetch-respository : clone] + EXIT_CODE=0 [fetch-respository : clone] + '[' 0 '!=' 0 ']' [fetch-respository : clone] + echo -n a08f579f6135293358b9423a3370e725ae1380cc [fetch-respository : clone] + echo -n https://github.com/mmondics/vote-api.git [build-image : build] + buildah --storage-driver=vfs bud --format=oci --tls-verify=false --no-cache -f ./Dockerfile -t image-registry.openshift-image-registry.svc:5000/user00-project/vote-api . [build-image : build] STEP 1: FROM image-registry.openshift-image-registry.svc:5000/openshift/golang:latest AS builder [build-image : build] Getting image source signatures [build-image : build] Copying blob sha256:ff637d5a66cba4903fc7d9343b0f9dbb4e1bf8ada19bd3934ea0edfb85dc4 [build-image : build] Copying blob sha256:f7fb0662b957bcb1b5007f9b5502af4da4c13e17b7bc2eff4f02c3e5ec08e [build-image : build] Copying blob sha256:35aab756d1a095511ab75eeca5aa77a37fa62a258f3fa5bcfb37ad604e369 [build-image : build] Copying blob sha256:7cc70ce0e0ee7fe5f8ea22894ad8c2f962f1dfdd00d05de91a32181c89179 [build-image : build] Copying blob sha256:73986f838dc404255946f6aa282b0aeabc420faa4f21b572e1de735498edf [build-image : build] Copying config sha256:9e8f033b036bdb224dc931cfcaaf532da6a6ae7d779e8a09c52eed12305 [build-image : build] Writing manifest to image destination [build-image : build] Storing signatures [build-image : build] STEP 2: WORKDIR /build [build-image : build] STEP 3: ADD . /build/ [build-image : build] STEP 4: RUN export GARCH=\"$(uname -m)\" && if [[ ${GARCH} == \"s390x\" ]]; then export GARCH=\"s390x\"; fi && GOOS=linux GOARCH=${GARCH} CGO_ENABLED=0 go build -mod=vendor -o api-server [build-image : build] STEP 5: FROM scratch [build-image : build] STEP 6: WORKDIR /app [build-image : build] STEP 7: COPY --from=builder /build/api-server /app/api-server [build-image : build] STEP 8: CMD [ \"/app/api-server\" ] [build-image : build] STEP 9: COMMIT image-registry.openshift-image-registry.svc:5000/user00-project/vote-api [build-image : build] --> 36faca61f94 [build-image : build] 36faca61f941af886128abd8792753095eaac7c1041084e222f426243ed50ecc [build-image : push] + buildah --storage-driver=vfs push --tls-verify=false --digestfile /workspace/source/image-digest image-registry.openshift-image-registry.svc:5000/user00-project/vote-api docker://image-registry.openshift-image-registry.svc:5000/user00-project/vote-api [build-image : push] + buildah --storage-driver=vfs push --tls-verify=false --digestfile /workspace/source/image-digest image-registry.openshift-image-registry.svc:5000/user00-project/vote-api docker://image-registry.openshift-image-registry.svc:5000/user00-project/vote-api [build-image : push] Getting image source signatures [build-image : push] Copying blob sha256:9eda1116f7414b98e397f94cc650fd50890c2d97fa47925e02b83df7726119 [build-image : push] Copying config sha256:36faca61f941af886128abd8792753095eaac7c1041084e222f426243ed5 [build-image : push] Writing manifest to image destination [build-image : push] Storing signatures [build-image : digest-to-results] + cat /workspace/source/image-digest [build-image : digest-to-results] + tee /tekton/results/IMAGE_DIGEST [build-image : digest-to-results] sha256:a7d730f92530c2f10891c55ba86a44e4fcc907436831c99733779ffb0d0fe8 [apply-manifests : apply] Applying manifests in k8s directory [apply-manifests : apply] deployment.apps \"vote-api\" created [apply-manifests : apply] service \"vote-api\" created [apply-manifests : apply] ----------------------------------- [update-deployment : patch] deployment.apps \"vote-api\" patched If you see the final deployment.apps \u201cvote-api\u201d patched line, your PipelineRun was successful and your backend application is now deployed in OpenShift. Look at your running application Pod by issuing the command : oc get pod Example Output user01@lab061:~/openshift-pipelines-s390x$ oc get pod NAME READY STATUS RESTARTS AGE build-and-deploy-run-sgtc7-apply-manifests-9p6mv-pod-95jhw 0/1 Completed 0 9m52s build-and-deploy-run-sgtc7-build-image-6kh6n-pod-pkvgx 0/3 Completed 0 12m build-and-deploy-run-sgtc7-fetch-repository-flxfx-pod-p6nzh 0/1 Completed 0 13m build-and-deploy-run-sgtc7-update-deployment-hgxrz-pod-htqpf 0/1 Completed 0 9m33s vote-api-6765569bfb-v4jlh 1/1 Running 0 9m20s You should see one running Pod and four completed Pods. The running Pod is your application that the Pipeline pulled from GitHub, containerized, pushed into the internal OpenShift repository, and started. The completed Pods were created to complete the Tasks defined in the Pipeline, and each is made up of one container per step in the Task. Looking at the READY column, you can see that most of the Pods have one container, with the exception of the build-image Pod that has three. The Tekton CLI also provides a way to check your Pipelines and PipelineRuns by running : tkn pipeline ls Example Output user01@lab061:~/openshift-pipelines-s390x$ tkn pipeline ls NAME AGE LAST RUN STARTED DURATION STATUS build-and-deploy 4 minutes ago build-and-deploy-run-2q5fp 4 minutes ago 3 minutes Succeeded Since we have successfully run the Pipeline in the CLI, let\u2019s trigger a run from the console in the next section. Creating the Frontend Application through the Console \u00b6 Let\u2019s create the frontend portion of our application by running the Pipeline from the OpenShift console. If you\u2019ve closed out of the OpenShift console in your web browser, go back to https://console-openshift-console.apps.atsocppa.dmz/ Navigate to the Developer Perspective -> Pipelines -> and select your userNN Project . The main Pipelines page displays the same information returned from the tkn pipeline ls command. Click your build-and-deploy Pipeline and then click the Actions -> Start button . This will open a new window that prompts you for the parameters with which to start your second PipelineRun. This window is the GUI equivalent to the multi-line tkn pipeline start command that we entered in the CLI PipelineRun. Enter the following parameters : deployment name: vote-ui git-url: https://github.com/mmondics/pipelines-vote-ui.git git-revision: master IMAGE: image-registry.openshift-image-registry.svc:5000/userNN-project/vote-ui shared-workspace: PersistentVolumeClaim -> source-pvc Important Make sure you change the one instance of NN in the IMAGE field to your user number. Then click start . You will be taken to the page for your PipelineRun and shown the graphical representation of the running Pipeline. Click the logs tab to follow what\u2019s happening in more detail like you saw in the CLI. When you see the PipelineRun has Succeeded and the deployment.apps \u201cvote-ui\u201d has been patched , your frontend application is also up and running. With both your backend and frontend applications are running, in the next section we\u2019ll access it in a browser.","title":"Running the Pipeline"},{"location":"lab009/lab009-5/#running-the-pipeline","text":"Let\u2019s use this Pipeline to create an application. To demonstrate the re-usability of OpenShift Pipelines, we will be creating both a frontend and a backend with the same Pipeline you created in the previous step. We\u2019ll also demonstrate the flexibility provided by OpenShift Pipelines that lets you use them from either the web console or the command line. Let\u2019s create the backend application with the Tekton CLI in your terminal. Now that you have all of the building blocks in place, you can start the Pipeline with the following command. The command will run the Pipeline and pass in parameters to: Use the shared workspace and the PersistentVolumeClaim you created Create the deployment named vote-api Build a container image from the source code at the given GitHub repository Push that container image into the OpenShift internal registry and tag it for your project Show the log so you can follow its progress Note that the forward slash simply breaks the command into multiple lines for readability.","title":"Running the Pipeline"},{"location":"lab009/lab009-5/#creating-the-backend-application-through-the-cli","text":"Run the following command : tkn pipeline start build-and-deploy \\ -w name=shared-workspace,claimName=source-pvc \\ -p deployment-name=vote-api \\ -p git-url=https://github.com/mmondics/pipelines-vote-api.git \\ -p IMAGE=image-registry.openshift-image-registry.svc:5000/userNN-project/pipelines-vote-api --showlog Important Make sure you change the one instance of NN to your team number in the command above. Expand for Example Output user01@lab061:~/openshift-pipelines-s390x$ tkn pipeline start build-and-deploy \\ > -w name=shared-workspace,claimName=source-pvc \\ > -p deployment-name=vote-api \\ > -p git-url=https://github.com/mmondics/vote-api.git \\ > -p IMAGE=image-registry.openshift-image-registry.svc:5000/userNN-project/vote-api \\ > --showlog PipelineRun started: build-and-deploy-run-75zqv Waiting for logs to be available... [fetch-respository : clone] + CHECKOUT_DIR=/workspace/output/ [fetch-respository : clone] + [[ true == \\t\\r\\u\\e ]] [fetch-respository : clone] + cleandir [fetch-respository : clone] + [[ -d /workspace/output/ ]] [fetch-respository : clone] + rm -rf /workspace/output//Dockerfile /workspace/output//LICENSE /workspace/output//README.md /workspace/output//go.mod /workspace/output//go.sum /workspace/output//k8s /workspace/output//main.go /workspace/output//vendor [fetch-respository : clone] + rm -rf /workspace/output//.git /workspace/output//.gitignore [fetch-respository : clone] + rm -rf '/workspace/output//..?*' [fetch-respository : clone] + test -z '' [fetch-respository : clone] + test -z '' [fetch-respository : clone] + test -z '' [fetch-respository : clone] + /ko-app/git-init -url https://github.com/mmondics/vote-api.git -revision master -refspec '' -path /workspace/output/ -sslVerify=true -submodules=true -depth 1 [fetch-respository : clone] {\"level\":\"info\",\"ts\":1616101272.5251348,\"caller\":\"git/git.go:165\",\"msg\":\"Successfully cloned https://github.com/mmondics/vote-api.git @ a08f579f6135293358b9423a3370e725ae1380cc (grafted, HEAD, origin/master) in path /workspace/output/\"} [fetch-respository : clone] {\"level\":\"info\",\"ts\":1616101272.6701891,\"caller\":\"git/git.go:203\",\"msg\":\"Successfully initialized and updated submodules in path /workspace/output/\"} [fetch-respository : clone] + cd /workspace/output/ [fetch-respository : clone] ++ git rev-parse HEAD [fetch-respository : clone] + RESULT_SHA=a08f579f6135293358b9423a3370e725ae1380cc [fetch-respository : clone] + EXIT_CODE=0 [fetch-respository : clone] + '[' 0 '!=' 0 ']' [fetch-respository : clone] + echo -n a08f579f6135293358b9423a3370e725ae1380cc [fetch-respository : clone] + echo -n https://github.com/mmondics/vote-api.git [build-image : build] + buildah --storage-driver=vfs bud --format=oci --tls-verify=false --no-cache -f ./Dockerfile -t image-registry.openshift-image-registry.svc:5000/user00-project/vote-api . [build-image : build] STEP 1: FROM image-registry.openshift-image-registry.svc:5000/openshift/golang:latest AS builder [build-image : build] Getting image source signatures [build-image : build] Copying blob sha256:ff637d5a66cba4903fc7d9343b0f9dbb4e1bf8ada19bd3934ea0edfb85dc4 [build-image : build] Copying blob sha256:f7fb0662b957bcb1b5007f9b5502af4da4c13e17b7bc2eff4f02c3e5ec08e [build-image : build] Copying blob sha256:35aab756d1a095511ab75eeca5aa77a37fa62a258f3fa5bcfb37ad604e369 [build-image : build] Copying blob sha256:7cc70ce0e0ee7fe5f8ea22894ad8c2f962f1dfdd00d05de91a32181c89179 [build-image : build] Copying blob sha256:73986f838dc404255946f6aa282b0aeabc420faa4f21b572e1de735498edf [build-image : build] Copying config sha256:9e8f033b036bdb224dc931cfcaaf532da6a6ae7d779e8a09c52eed12305 [build-image : build] Writing manifest to image destination [build-image : build] Storing signatures [build-image : build] STEP 2: WORKDIR /build [build-image : build] STEP 3: ADD . /build/ [build-image : build] STEP 4: RUN export GARCH=\"$(uname -m)\" && if [[ ${GARCH} == \"s390x\" ]]; then export GARCH=\"s390x\"; fi && GOOS=linux GOARCH=${GARCH} CGO_ENABLED=0 go build -mod=vendor -o api-server [build-image : build] STEP 5: FROM scratch [build-image : build] STEP 6: WORKDIR /app [build-image : build] STEP 7: COPY --from=builder /build/api-server /app/api-server [build-image : build] STEP 8: CMD [ \"/app/api-server\" ] [build-image : build] STEP 9: COMMIT image-registry.openshift-image-registry.svc:5000/user00-project/vote-api [build-image : build] --> 36faca61f94 [build-image : build] 36faca61f941af886128abd8792753095eaac7c1041084e222f426243ed50ecc [build-image : push] + buildah --storage-driver=vfs push --tls-verify=false --digestfile /workspace/source/image-digest image-registry.openshift-image-registry.svc:5000/user00-project/vote-api docker://image-registry.openshift-image-registry.svc:5000/user00-project/vote-api [build-image : push] + buildah --storage-driver=vfs push --tls-verify=false --digestfile /workspace/source/image-digest image-registry.openshift-image-registry.svc:5000/user00-project/vote-api docker://image-registry.openshift-image-registry.svc:5000/user00-project/vote-api [build-image : push] Getting image source signatures [build-image : push] Copying blob sha256:9eda1116f7414b98e397f94cc650fd50890c2d97fa47925e02b83df7726119 [build-image : push] Copying config sha256:36faca61f941af886128abd8792753095eaac7c1041084e222f426243ed5 [build-image : push] Writing manifest to image destination [build-image : push] Storing signatures [build-image : digest-to-results] + cat /workspace/source/image-digest [build-image : digest-to-results] + tee /tekton/results/IMAGE_DIGEST [build-image : digest-to-results] sha256:a7d730f92530c2f10891c55ba86a44e4fcc907436831c99733779ffb0d0fe8 [apply-manifests : apply] Applying manifests in k8s directory [apply-manifests : apply] deployment.apps \"vote-api\" created [apply-manifests : apply] service \"vote-api\" created [apply-manifests : apply] ----------------------------------- [update-deployment : patch] deployment.apps \"vote-api\" patched If you see the final deployment.apps \u201cvote-api\u201d patched line, your PipelineRun was successful and your backend application is now deployed in OpenShift. Look at your running application Pod by issuing the command : oc get pod Example Output user01@lab061:~/openshift-pipelines-s390x$ oc get pod NAME READY STATUS RESTARTS AGE build-and-deploy-run-sgtc7-apply-manifests-9p6mv-pod-95jhw 0/1 Completed 0 9m52s build-and-deploy-run-sgtc7-build-image-6kh6n-pod-pkvgx 0/3 Completed 0 12m build-and-deploy-run-sgtc7-fetch-repository-flxfx-pod-p6nzh 0/1 Completed 0 13m build-and-deploy-run-sgtc7-update-deployment-hgxrz-pod-htqpf 0/1 Completed 0 9m33s vote-api-6765569bfb-v4jlh 1/1 Running 0 9m20s You should see one running Pod and four completed Pods. The running Pod is your application that the Pipeline pulled from GitHub, containerized, pushed into the internal OpenShift repository, and started. The completed Pods were created to complete the Tasks defined in the Pipeline, and each is made up of one container per step in the Task. Looking at the READY column, you can see that most of the Pods have one container, with the exception of the build-image Pod that has three. The Tekton CLI also provides a way to check your Pipelines and PipelineRuns by running : tkn pipeline ls Example Output user01@lab061:~/openshift-pipelines-s390x$ tkn pipeline ls NAME AGE LAST RUN STARTED DURATION STATUS build-and-deploy 4 minutes ago build-and-deploy-run-2q5fp 4 minutes ago 3 minutes Succeeded Since we have successfully run the Pipeline in the CLI, let\u2019s trigger a run from the console in the next section.","title":"Creating the Backend Application through the CLI"},{"location":"lab009/lab009-5/#creating-the-frontend-application-through-the-console","text":"Let\u2019s create the frontend portion of our application by running the Pipeline from the OpenShift console. If you\u2019ve closed out of the OpenShift console in your web browser, go back to https://console-openshift-console.apps.atsocppa.dmz/ Navigate to the Developer Perspective -> Pipelines -> and select your userNN Project . The main Pipelines page displays the same information returned from the tkn pipeline ls command. Click your build-and-deploy Pipeline and then click the Actions -> Start button . This will open a new window that prompts you for the parameters with which to start your second PipelineRun. This window is the GUI equivalent to the multi-line tkn pipeline start command that we entered in the CLI PipelineRun. Enter the following parameters : deployment name: vote-ui git-url: https://github.com/mmondics/pipelines-vote-ui.git git-revision: master IMAGE: image-registry.openshift-image-registry.svc:5000/userNN-project/vote-ui shared-workspace: PersistentVolumeClaim -> source-pvc Important Make sure you change the one instance of NN in the IMAGE field to your user number. Then click start . You will be taken to the page for your PipelineRun and shown the graphical representation of the running Pipeline. Click the logs tab to follow what\u2019s happening in more detail like you saw in the CLI. When you see the PipelineRun has Succeeded and the deployment.apps \u201cvote-ui\u201d has been patched , your frontend application is also up and running. With both your backend and frontend applications are running, in the next section we\u2019ll access it in a browser.","title":"Creating the Frontend Application through the Console"},{"location":"lab009/lab009-6/","text":"Accessing the Pipeline in a Browser \u00b6 Your application is accessible via its route. In the OpenShift console, navigate to the Topology page in the Developer Perspective and make sure you\u2019re in your userNN-project . You should see two Icons with solid blue bars indicating your application pods are running without error. On the vote-ui icon, click the button in the top right corner to navigate to the application\u2019s exposed route . This will open a new browser tab for your frontend application UI. Click the box for your desired option . By casting your vote with the vote-ui frontend, you are invoking a REST API call and sending a POST request that is stored in the vote-api backend application. You can see this POST request reflected in the vote-api Pod logs. In your terminal session find the name of your vote-api Pod using the command : oc get pods | grep Running Example Output user01@lab061:~/openshift-pipelines-s390x$ oc get pods | grep Running vote-api-6765569bfb-p2bhh 1/1 Running 0 65m vote-ui-6846f88f6f-rzzgt 1/1 Running 0 18m Copy the full name for your vote-api Pod and view its logs with the command : oc logs pod/vote-api-XXXXXXXXXX-YYYYY Important Your randomly-generated Pod names will differ. Example Output user01@lab061:~/openshift-pipelines-s390x$ oc logs pod/vote-api-6765569bfb-p2bhh [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached. [GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] GET /vote --> main.setupRouter.func1 (3 handlers) [GIN-debug] POST /vote --> main.setupRouter.func2 (3 handlers) [GIN-debug] Listening and serving HTTP on :9000 [GIN] 2021/03/22 - 16:18:18 | 200 | 179.658\u00b5s | 10.131.1.157 | POST /vote [GIN] 2021/03/22 - 16:18:48 | 200 | 107.379\u00b5s | 10.131.1.157 | POST /vote You can see your POST requests at the /vote endpoint at the bottom, and more detail is stored in NFS by the PersistentVolumeClaim you created earlier. In this lab, you have: Created Tasks that have specific responsibilities in the building and deploying of a containerized application onto an OpenShift on IBM Z cluster Created a Pipeline that combines these Tasks to one end-to-end process Ran the Pipeline twice -- once from the command line, and once from the OpenShift console -- to create a backend and a frontend application. Used the created applications to invoke a REST API call that is stored persistently in NFS storage.","title":"Access the Application in a Browser"},{"location":"lab009/lab009-6/#accessing-the-pipeline-in-a-browser","text":"Your application is accessible via its route. In the OpenShift console, navigate to the Topology page in the Developer Perspective and make sure you\u2019re in your userNN-project . You should see two Icons with solid blue bars indicating your application pods are running without error. On the vote-ui icon, click the button in the top right corner to navigate to the application\u2019s exposed route . This will open a new browser tab for your frontend application UI. Click the box for your desired option . By casting your vote with the vote-ui frontend, you are invoking a REST API call and sending a POST request that is stored in the vote-api backend application. You can see this POST request reflected in the vote-api Pod logs. In your terminal session find the name of your vote-api Pod using the command : oc get pods | grep Running Example Output user01@lab061:~/openshift-pipelines-s390x$ oc get pods | grep Running vote-api-6765569bfb-p2bhh 1/1 Running 0 65m vote-ui-6846f88f6f-rzzgt 1/1 Running 0 18m Copy the full name for your vote-api Pod and view its logs with the command : oc logs pod/vote-api-XXXXXXXXXX-YYYYY Important Your randomly-generated Pod names will differ. Example Output user01@lab061:~/openshift-pipelines-s390x$ oc logs pod/vote-api-6765569bfb-p2bhh [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached. [GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] GET /vote --> main.setupRouter.func1 (3 handlers) [GIN-debug] POST /vote --> main.setupRouter.func2 (3 handlers) [GIN-debug] Listening and serving HTTP on :9000 [GIN] 2021/03/22 - 16:18:18 | 200 | 179.658\u00b5s | 10.131.1.157 | POST /vote [GIN] 2021/03/22 - 16:18:48 | 200 | 107.379\u00b5s | 10.131.1.157 | POST /vote You can see your POST requests at the /vote endpoint at the bottom, and more detail is stored in NFS by the PersistentVolumeClaim you created earlier. In this lab, you have: Created Tasks that have specific responsibilities in the building and deploying of a containerized application onto an OpenShift on IBM Z cluster Created a Pipeline that combines these Tasks to one end-to-end process Ran the Pipeline twice -- once from the command line, and once from the OpenShift console -- to create a backend and a frontend application. Used the created applications to invoke a REST API call that is stored persistently in NFS storage.","title":"Accessing the Pipeline in a Browser"},{"location":"lab009/lab009-7/","text":"Cleaning Up \u00b6 When you\u2019re ready to finish the lab and delete the objects you created, return to your terminal and double check that you\u2019re in your own userNN-project with : oc project Example Output user01@lab061:~/openshift-pipelines-s390x$ oc project Using project \"user00-project\" on server \"https://api.atsocpd2.dmz:6443\". Run the uninstall script : ./pipeline-cleanup.sh Example Output user01@lab061:~/openshift-pipelines-s390x$ ./pipeline-cleanup.sh Running oc delete all --all pod \"build-and-deploy-run-6hgg7-apply-manifests-ksns5-pod-zvnq8\" deleted pod \"build-and-deploy-run-6hgg7-build-image-4bstq-pod-sd2xv\" deleted pod \"build-and-deploy-run-6hgg7-fetch-repository-jgqml-pod-ld4nj\" deleted pod \"build-and-deploy-run-6hgg7-update-deployment-qhmxh-pod-c56gj\" deleted pod \"build-and-deploy-run-fnx5s-apply-manifests-k8v7f-pod-88fpm\" deleted pod \"build-and-deploy-run-fnx5s-build-image-4xknh-pod-kq4zk\" deleted pod \"build-and-deploy-run-fnx5s-fetch-repository-m5pmr-pod-8m5gt\" deleted pod \"build-and-deploy-run-fnx5s-update-deployment-fgq9s-pod-2s4vw\" deleted pod \"hello-run-l2skb-pod-vzvrj\" deleted pod \"vote-api-6765569bfb-p59vj\" deleted pod \"vote-ui-6846f88f6f-z7zp9\" deleted service \"vote-api\" deleted service \"vote-ui\" deleted deployment.apps \"vote-api\" deleted deployment.apps \"vote-ui\" deleted replicaset.apps \"vote-ui-566848fff4\" deleted replicaset.apps \"vote-ui-6846f88f6f\" deleted imagestream.image.openshift.io \"vote-api\" deleted imagestream.image.openshift.io \"vote-ui\" deleted route.route.openshift.io \"vote-ui\" deleted Deleting Pipeline & resources pipeline.tekton.dev \"build-and-deploy\" deleted pipelinerun.tekton.dev \"build-and-deploy-run-6hgg7\" deleted pipelinerun.tekton.dev \"build-and-deploy-run-fnx5s\" deleted task.tekton.dev \"apply-manifests\" deleted task.tekton.dev \"hello\" deleted task.tekton.dev \"update-deployment\" deleted taskrun.tekton.dev \"hello-run-l2skb\" deleted Deleting PVC persistentvolumeclaim \"source-pvc\" deleted Removing Images","title":"Cleaning Up"},{"location":"lab009/lab009-7/#cleaning-up","text":"When you\u2019re ready to finish the lab and delete the objects you created, return to your terminal and double check that you\u2019re in your own userNN-project with : oc project Example Output user01@lab061:~/openshift-pipelines-s390x$ oc project Using project \"user00-project\" on server \"https://api.atsocpd2.dmz:6443\". Run the uninstall script : ./pipeline-cleanup.sh Example Output user01@lab061:~/openshift-pipelines-s390x$ ./pipeline-cleanup.sh Running oc delete all --all pod \"build-and-deploy-run-6hgg7-apply-manifests-ksns5-pod-zvnq8\" deleted pod \"build-and-deploy-run-6hgg7-build-image-4bstq-pod-sd2xv\" deleted pod \"build-and-deploy-run-6hgg7-fetch-repository-jgqml-pod-ld4nj\" deleted pod \"build-and-deploy-run-6hgg7-update-deployment-qhmxh-pod-c56gj\" deleted pod \"build-and-deploy-run-fnx5s-apply-manifests-k8v7f-pod-88fpm\" deleted pod \"build-and-deploy-run-fnx5s-build-image-4xknh-pod-kq4zk\" deleted pod \"build-and-deploy-run-fnx5s-fetch-repository-m5pmr-pod-8m5gt\" deleted pod \"build-and-deploy-run-fnx5s-update-deployment-fgq9s-pod-2s4vw\" deleted pod \"hello-run-l2skb-pod-vzvrj\" deleted pod \"vote-api-6765569bfb-p59vj\" deleted pod \"vote-ui-6846f88f6f-z7zp9\" deleted service \"vote-api\" deleted service \"vote-ui\" deleted deployment.apps \"vote-api\" deleted deployment.apps \"vote-ui\" deleted replicaset.apps \"vote-ui-566848fff4\" deleted replicaset.apps \"vote-ui-6846f88f6f\" deleted imagestream.image.openshift.io \"vote-api\" deleted imagestream.image.openshift.io \"vote-ui\" deleted route.route.openshift.io \"vote-ui\" deleted Deleting Pipeline & resources pipeline.tekton.dev \"build-and-deploy\" deleted pipelinerun.tekton.dev \"build-and-deploy-run-6hgg7\" deleted pipelinerun.tekton.dev \"build-and-deploy-run-fnx5s\" deleted task.tekton.dev \"apply-manifests\" deleted task.tekton.dev \"hello\" deleted task.tekton.dev \"update-deployment\" deleted taskrun.tekton.dev \"hello-run-l2skb\" deleted Deleting PVC persistentvolumeclaim \"source-pvc\" deleted Removing Images","title":"Cleaning Up"},{"location":"lab010/lab010-1/","text":"OpenShift Service Mesh \u00b6 Red Hat OpenShift Service Mesh (OSSM) provides a platform for behavioral insight and operational control over your networked microservices in a service mesh. With OSSM, you can connect, secure, and monitor microservices in your OpenShift Container Platform environment. A Service Mesh is the network of microservices that make up applications and the interactions between those microservices. When a Service Mesh grows in size and complexity, it can become harder to understand and manage. Take, for example, an application made up of 5 microservices that is managed from the OpenShift Console and/or Command Line. Based on the open source Istio project, OSSM adds a transparent layer on existing microservice applications without requiring any changes to the application code. You add OSSM support to services by deploying a sidecar proxy to relevant microservices in the mesh that intercepts all network communication between microservices. See the figure below. OpenShift Service Mesh gives you an easy way to create a network of deployed services that provide: Discovery Load balancing Service-to-service authentication Failure recovery Metrics Monitoring OpenShift Service Mesh also provides more complex operational functions including: A/B testing Canary releases Rate limiting Access control End-to-end authentication In this lab, we will be exploring many of the OSSM features above.","title":"OpenShift Service Mesh"},{"location":"lab010/lab010-1/#openshift-service-mesh","text":"Red Hat OpenShift Service Mesh (OSSM) provides a platform for behavioral insight and operational control over your networked microservices in a service mesh. With OSSM, you can connect, secure, and monitor microservices in your OpenShift Container Platform environment. A Service Mesh is the network of microservices that make up applications and the interactions between those microservices. When a Service Mesh grows in size and complexity, it can become harder to understand and manage. Take, for example, an application made up of 5 microservices that is managed from the OpenShift Console and/or Command Line. Based on the open source Istio project, OSSM adds a transparent layer on existing microservice applications without requiring any changes to the application code. You add OSSM support to services by deploying a sidecar proxy to relevant microservices in the mesh that intercepts all network communication between microservices. See the figure below. OpenShift Service Mesh gives you an easy way to create a network of deployed services that provide: Discovery Load balancing Service-to-service authentication Failure recovery Metrics Monitoring OpenShift Service Mesh also provides more complex operational functions including: A/B testing Canary releases Rate limiting Access control End-to-end authentication In this lab, we will be exploring many of the OSSM features above.","title":"OpenShift Service Mesh"},{"location":"lab010/lab010-10/","text":"Wrap Up & Clean Up \u00b6 In this lab, you have explored many of the features that come as part of OpenShift Service Mesh. OSSM is an extremely powerful OpenShift add-on, and we were not able to fit all of its features into this lab. You can find more information & tutorials at the following documentation links: OpenShift Service Mesh: https://docs.openshift.com/container-platform/4.7/service_mesh/v2x/ossm-about.html Istio: https://istio.io/latest/docs/ Kiali: https://kiali.io/documentation/ Jaeger: https://www.jaegertracing.io/docs/1.24/ When you\u2019re ready to clean up and finish this lab, run the following script to delete all the resources from your OpenShift project . ./cleanup.sh Example Output user01@lab061:~/istio-s390x$ ./cleanup.sh serviceaccount \"bookinfo-details\" deleted serviceaccount \"bookinfo-productpage\" deleted serviceaccount \"bookinfo-ratings\" deleted serviceaccount \"bookinfo-reviews\" deleted deployment.apps \"details-v1\" deleted deployment.apps \"productpage-v1\" deleted deployment.apps \"ratings-v1\" deleted deployment.apps \"reviews-v1\" deleted deployment.apps \"reviews-v2\" deleted deployment.apps \"reviews-v3\" deleted service \"details\" deleted service \"productpage\" deleted service \"ratings\" deleted service \"reviews\" deleted gateway.networking.istio.io \"bookinfo-gateway\" deleted virtualservice.networking.istio.io \"bookinfo\" deleted destinationrule.networking.istio.io \"details\" deleted destinationrule.networking.istio.io \"productpage\" deleted destinationrule.networking.istio.io \"ratings\" deleted destinationrule.networking.istio.io \"reviews\" deleted Cleanup Complete","title":"Wrap Up & Clean Up"},{"location":"lab010/lab010-10/#wrap-up-clean-up","text":"In this lab, you have explored many of the features that come as part of OpenShift Service Mesh. OSSM is an extremely powerful OpenShift add-on, and we were not able to fit all of its features into this lab. You can find more information & tutorials at the following documentation links: OpenShift Service Mesh: https://docs.openshift.com/container-platform/4.7/service_mesh/v2x/ossm-about.html Istio: https://istio.io/latest/docs/ Kiali: https://kiali.io/documentation/ Jaeger: https://www.jaegertracing.io/docs/1.24/ When you\u2019re ready to clean up and finish this lab, run the following script to delete all the resources from your OpenShift project . ./cleanup.sh Example Output user01@lab061:~/istio-s390x$ ./cleanup.sh serviceaccount \"bookinfo-details\" deleted serviceaccount \"bookinfo-productpage\" deleted serviceaccount \"bookinfo-ratings\" deleted serviceaccount \"bookinfo-reviews\" deleted deployment.apps \"details-v1\" deleted deployment.apps \"productpage-v1\" deleted deployment.apps \"ratings-v1\" deleted deployment.apps \"reviews-v1\" deleted deployment.apps \"reviews-v2\" deleted deployment.apps \"reviews-v3\" deleted service \"details\" deleted service \"productpage\" deleted service \"ratings\" deleted service \"reviews\" deleted gateway.networking.istio.io \"bookinfo-gateway\" deleted virtualservice.networking.istio.io \"bookinfo\" deleted destinationrule.networking.istio.io \"details\" deleted destinationrule.networking.istio.io \"productpage\" deleted destinationrule.networking.istio.io \"ratings\" deleted destinationrule.networking.istio.io \"reviews\" deleted Cleanup Complete","title":"Wrap Up &amp; Clean Up"},{"location":"lab010/lab010-2/","text":"OpenShift Service Mesh Architecture \u00b6 OpenShift Service Mesh is logically split into a data plane and a control plane : The data plane is a set of intelligent proxies deployed as sidecars. These proxies intercept and control all inbound and outbound network communication between microservices in the service mesh. Envoy proxy intercepts all inbound and outbound traffic for all services in the service mesh. Envoy is deployed as a sidecar to the relevant service in the same pod. The control plane manages and configures Istiod to enforce proxies to route traffic. Istiod provides service discovery, configuration and certificate management. It converts high-level routing rules to Envoy configurations and propagates them to the sidecars at runtime.","title":"OpenShift Service Mesh Architecture"},{"location":"lab010/lab010-2/#openshift-service-mesh-architecture","text":"OpenShift Service Mesh is logically split into a data plane and a control plane : The data plane is a set of intelligent proxies deployed as sidecars. These proxies intercept and control all inbound and outbound network communication between microservices in the service mesh. Envoy proxy intercepts all inbound and outbound traffic for all services in the service mesh. Envoy is deployed as a sidecar to the relevant service in the same pod. The control plane manages and configures Istiod to enforce proxies to route traffic. Istiod provides service discovery, configuration and certificate management. It converts high-level routing rules to Envoy configurations and propagates them to the sidecars at runtime.","title":"OpenShift Service Mesh Architecture"},{"location":"lab010/lab010-3/","text":"Log into OpenShift Using the CLI \u00b6 In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server : ssh userNN@192.168.176.61 Where NN is your user number. When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab010/lab010-3/#log-into-openshift-using-the-cli","text":"In this section, you will be connecting to a \u201cLinux Guest\u201d server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line oc installed, so you don\u2019t have to install it on your RHEL VM terminal. Open a Terminal session ssh into the Linux Guest server : ssh userNN@192.168.176.61 Where NN is your user number. When prompted, enter your password: p@ssw0rd and hit enter . Example Output In Firefox, navigate to the following URL to request an API token: https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request Enter your OpenShift credentials when prompted . Username: userNN Password: p@ssw0rd Click the \u201cDisplay Token\u201d hyperlink . Copy the contents of the first text box beginning with \u201coc login\u201d and ending with \u201c6443\u201d. Paste this command back in your terminal session and press enter . oc login --token=<YOUR_TOKEN_HERE> --server=https://api.atsocppa.dmz:6443 Important If you\u2019re prompted to use an insecure connection, type Y and hit enter. Example Output user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443 Logged into \"https://api.atsocppa.dmz:6443\" as \"user01\" using the token provided. You have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"user01-project\". You are now logged into the cluster via the command line, and you are told which project you are using. If you\u2019re in a project other than userNN-project, use the following command to move into it: oc project userNN-project , where NN is your user number.","title":"Log into OpenShift Using the CLI"},{"location":"lab010/lab010-4/","text":"Cloning the GitHub Repository and Reviewing its Contents \u00b6 In the terminal session, you should have been automatically placed in your home directory /home/userNN (where NN is your user number). Run the command pwd to check your current working directory . Example Output user01@lab061:~$ pwd /home/user01 If you are in any other directory, change into the correct home directory using the command : cd /home/userNN Where NN is your user number. Example Output user01@lab061:~$ cd /home/user01 user01@lab061:~$ pwd /home/user01 In your home directory, clone the OpenShift Service Mesh repository using the command : git clone https://github.com/mmondics/istio-s390x -b ocp-wildfire Example Output user01@lab061:~$ git clone https://github.com/mmondics/istio-s390x -b ocp-wildfire Cloning into 'istio-s390x'... remote: Enumerating objects: 171, done. remote: Counting objects: 100% (171/171), done. remote: Compressing objects: 100% (98/98), done. remote: Total 171 (delta 61), reused 159 (delta 52), pack-reused 0 Receiving objects: 100% (171/171), 332.76 KiB | 0 bytes/s, done. Resolving deltas: 100% (61/61), done. Checking connectivity... done. This will create a new directory called istio-s390x. Change into this directory using the command : cd istio-s390x Then list its contents using the command : ls -l Example Output user01@lab061:~$ cd Istio-s390x user01@lab061:~/istio-s390x$ total 32 -rw-r--r-- 1 user01 users 1306 Jun 24 12:54 README.md -rwxr-xr-x 1 user01 users 4029 Jun 24 12:54 build_push_update_images.sh drwxr-xr-x 2 user01 users 4096 Jun 24 12:54 networking drwxr-xr-x 3 user01 users 4096 Jun 24 12:54 platform drwxr-xr-x 2 user01 users 4096 Jun 24 12:54 policy drwxr-xr-x 8 user01 users 4096 Jun 24 12:54 src -rw-r--r-- 1 user01 users 6329 Jun 24 12:54 swagger.yaml If you navigate to the GitHub in a web browser https://github.com/mmondics/istio-s390x/tree/ocp-wildfire , you will notice that the sub-directories in your Linux session reflect the folders contained in the repository. File Description README.md Contains the content displayed on the GitHub page for this repository. You can read through this README file if you want to get more information about this lab. build_push_update_images.sh Directory containing a shell script that was used to create the container images used in this lab. You will not be using this script, but it is here for anyone who wishes to update images to newer versions in the future. networking Directory container various YAML files for networking components such as gateways, virtualservices, destinationrules, and more. platform Directory containing various YAML files that will create the application deployments, services, serviceaccounts, and more. policy Directory containing a YAML file that will create envoyfilters in order to dynamically rate-limit the traffic to the service mesh application. src Directory containing the source files used to build each container image used in this lab. These source files will not be used in this lab. swagger.yaml A YAML file that defines and documents the structure of the APIs used in this lab. You will not be interacting with this file directly.","title":"Cloning the GitHub Repository and Reviewing its Contents"},{"location":"lab010/lab010-4/#cloning-the-github-repository-and-reviewing-its-contents","text":"In the terminal session, you should have been automatically placed in your home directory /home/userNN (where NN is your user number). Run the command pwd to check your current working directory . Example Output user01@lab061:~$ pwd /home/user01 If you are in any other directory, change into the correct home directory using the command : cd /home/userNN Where NN is your user number. Example Output user01@lab061:~$ cd /home/user01 user01@lab061:~$ pwd /home/user01 In your home directory, clone the OpenShift Service Mesh repository using the command : git clone https://github.com/mmondics/istio-s390x -b ocp-wildfire Example Output user01@lab061:~$ git clone https://github.com/mmondics/istio-s390x -b ocp-wildfire Cloning into 'istio-s390x'... remote: Enumerating objects: 171, done. remote: Counting objects: 100% (171/171), done. remote: Compressing objects: 100% (98/98), done. remote: Total 171 (delta 61), reused 159 (delta 52), pack-reused 0 Receiving objects: 100% (171/171), 332.76 KiB | 0 bytes/s, done. Resolving deltas: 100% (61/61), done. Checking connectivity... done. This will create a new directory called istio-s390x. Change into this directory using the command : cd istio-s390x Then list its contents using the command : ls -l Example Output user01@lab061:~$ cd Istio-s390x user01@lab061:~/istio-s390x$ total 32 -rw-r--r-- 1 user01 users 1306 Jun 24 12:54 README.md -rwxr-xr-x 1 user01 users 4029 Jun 24 12:54 build_push_update_images.sh drwxr-xr-x 2 user01 users 4096 Jun 24 12:54 networking drwxr-xr-x 3 user01 users 4096 Jun 24 12:54 platform drwxr-xr-x 2 user01 users 4096 Jun 24 12:54 policy drwxr-xr-x 8 user01 users 4096 Jun 24 12:54 src -rw-r--r-- 1 user01 users 6329 Jun 24 12:54 swagger.yaml If you navigate to the GitHub in a web browser https://github.com/mmondics/istio-s390x/tree/ocp-wildfire , you will notice that the sub-directories in your Linux session reflect the folders contained in the repository. File Description README.md Contains the content displayed on the GitHub page for this repository. You can read through this README file if you want to get more information about this lab. build_push_update_images.sh Directory containing a shell script that was used to create the container images used in this lab. You will not be using this script, but it is here for anyone who wishes to update images to newer versions in the future. networking Directory container various YAML files for networking components such as gateways, virtualservices, destinationrules, and more. platform Directory containing various YAML files that will create the application deployments, services, serviceaccounts, and more. policy Directory containing a YAML file that will create envoyfilters in order to dynamically rate-limit the traffic to the service mesh application. src Directory containing the source files used to build each container image used in this lab. These source files will not be used in this lab. swagger.yaml A YAML file that defines and documents the structure of the APIs used in this lab. You will not be interacting with this file directly.","title":"Cloning the GitHub Repository and Reviewing its Contents"},{"location":"lab010/lab010-5/","text":"Deploying an Application on the Service Mesh \u00b6 The first thing we will do is deploy our application - Bookinfo . Bookinfo is a sample application provided by Istio, the upstream project from which OpenShift Service Mesh is built. The application displays information about a book, similar to a single catalog entry of an online bookstore. Displayed on the page is a description of the book, book details (ISBN, number of pages, and so on), and a few book reviews. The Bookinfo application is broken into four separate microservices: productpage . The productpage microservice calls the details and reviews microservices to populate the page. details . The details microservice contains book information. reviews . The reviews microservice contains book reviews. It also calls the ratings microservice. ratings . The ratings microservice contains book ranking information that accompanies a book review. There are 3 versions of the reviews microservice: Version v1 doesn\u2019t call the ratings service. Version v2 calls the ratings service and displays each rating as 1 to 5 black stars. Version v3 calls the ratings service and displays each rating as 1 to 5 red stars. The end-to-end architecture of the application is shown below. Information This application is polyglot , i.e., the microservices are written in different languages. It\u2019s worth noting that these microservices have no dependencies on the Istio or OSSM, but make an interesting Service Mesh example, particularly because of the multitude of microservices, languages and versions for the reviews microservice. To run Bookinfo on the Service Mesh requires no changes to the application itself . You simply need to run the application in a Service Mesh-enabled environment, with Envoy sidecars injected alongside each microservice. Our OpenShift environment is Service Mesh enabled. OSSM is already installed in our OpenShift cluster and is configured to watch for Service Mesh-enabled deployments to appear in your project, userNN-project. The Service Mesh knows which project to watch through an instance of the Istio Service Mesh Member Roll custom resource. Your userNN ID is not able to access this custom resource, but it is displayed below for your reference. Example Output user01@lab061:~/istio-s390x$ oc describe ServiceMeshMemberRoll/default Name: default Namespace: istio-system Labels: <none> Annotations: <none> API Version: maistra.io/v1 Kind: ServiceMeshMemberRoll Spec: Members: user01-project user02-project user03-project user04-project user05-project Status: Annotations: Configured Member Count: 5/5 Conditions: Last Transition Time: 2021-06-28T19:04:47Z Message: All namespaces have been configured successfully Reason: Configured Status: True Type: Ready Configured Members: user01-project user02-project user03-project user04-project user05-project Mesh Generation: 1 Mesh Reconciled Version: 2.0.6-2.el8-1 Observed Generation: 6 Pending Members: Events: <none> Note The screenshot above has been trimmed down for brevity. User projects up to user30-project are configured. This custom resource will watch each member project and automatically inject Envoy sidecars when a deployment is created with the annotation sidecar.istio.io/inject: \"true\" . The deployments contained in the GitHub repository you pulled already have this annotation configured. From the Istio-s390x directory, view the application deployments with the command : cat platform/kube/bookinfo.yaml You will find that this YAML file contains a Service, a ServiceAccount, and a Deployment for each of the microservices described previously. Find a section of the YAML file that has kind: Deployment, and you will see the sidecar.istio.io/inject: \"true\" annotation in its spec section. Example Output apiVersion: apps/v1 kind: Deployment metadata: name: details-v1 labels: app: details version: v1 spec: replicas: 1 selector: matchLabels: app: details version: v1 template: metadata: annotations: sidecar.istio.io/inject: \"true\" labels: app: details version: v1 spec: serviceAccountName: bookinfo-details containers: - name: details image: quay.io/mmondics/examples-bookinfo-details-v1:1.16.2 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 Once created, this YAML file will set up the entire Bookinfo application for you and the Envoy proxy sidecars will be automatically injected for the Istio control plane to interact with. Create the application by running the following command : oc create -f platform/kube/bookinfo.yaml Example Output user01@lab061:~/istio-s390x$ oc create -f platform/kube/bookinfo.yaml oc create -f platform/kube/bookinfo.yaml service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created Although not yet accessible, all of the application components should be up and running within a few seconds. Check that your four services were created with : oc get services Example Output user01@lab061:~/istio-s390x$ oc get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 172.30.237.245 <none> 9080/TCP 10s productpage ClusterIP 172.30.73.57 <none> 9080/TCP 10s ratings ClusterIP 172.30.241.30 <none> 9080/TCP 10s reviews ClusterIP 172.30.34.195 <none> 9080/TCP 10s And check that your six pods have been created with : oc get pods Example Output user01@lab061:~/istio-s390x$ oc get services NAME READY STATUS RESTARTS AGE details-v1-78cb8b797f-ndqdz 2/2 Running 0 45s productpage-v1-568ddd75bf-rmqtf 2/2 Running 0 45s ratings-v1-768dc65999-q5dft 2/2 Running 0 45s reviews-v1-6cf69f46c9-wbtqm 2/2 Running 0 45s reviews-v2-64fd74bbd7-hx4rd 2/2 Running 0 46s reviews-v3-8cb65c475-wr22r 2/2 Running 0 46s Each pod should have a STATUS : Running and should show Ready : 2/2. The 2/2 indicates that the pod includes two containers, and both are ready. In our case, each pod has one container for its application, and one container for the Envoy proxy, or sidecar. Optional You can list the containers in your productpage pod with the command: oc get pod productpage-v1-xxxxx -o jsonpath={.spec.containers[*].name} (where xxxxx is your randomly generated string of characters returned by oc get pods) user01@lab061:~/istio-s390x$ oc get pod productpage-v1-78797f -o jsonpath={.spec.containers[*].name} productpage istio-proxyuser01@lab061:~/istio-s390x$ Your two container names will be printed at the beginning of the next line - productpage, and Istio-proxy. productpage is the end user\u2019s primary ingress into the Bookinfo application. This is the microservice that will pull the data from the ratings, reviews, and details microservices and display them when requested by a user. Check that the productpage microservice is running correctly by issuing the command : oc exec \"$(oc get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')\" -c ratings -- curl -sS productpage:9080/productpage | grep -o \"<title>.*</title>\" Example Output user01@lab061:~/istio-s390x$ oc exec \"$(oc get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')\" -c ratings -- curl -sS productpage:9080/productpage | grep -o \"<title>.*</title>\" <title>Simple Bookstore App</title> If you see <title>Simple Bookstore App</title> returned, your Bookinfo application is working correctly, yet still inaccessible. In the next section, we will create a Gateway to provide ingress into the Service Mesh application.","title":"Deploying an Application on the Service Mesh"},{"location":"lab010/lab010-5/#deploying-an-application-on-the-service-mesh","text":"The first thing we will do is deploy our application - Bookinfo . Bookinfo is a sample application provided by Istio, the upstream project from which OpenShift Service Mesh is built. The application displays information about a book, similar to a single catalog entry of an online bookstore. Displayed on the page is a description of the book, book details (ISBN, number of pages, and so on), and a few book reviews. The Bookinfo application is broken into four separate microservices: productpage . The productpage microservice calls the details and reviews microservices to populate the page. details . The details microservice contains book information. reviews . The reviews microservice contains book reviews. It also calls the ratings microservice. ratings . The ratings microservice contains book ranking information that accompanies a book review. There are 3 versions of the reviews microservice: Version v1 doesn\u2019t call the ratings service. Version v2 calls the ratings service and displays each rating as 1 to 5 black stars. Version v3 calls the ratings service and displays each rating as 1 to 5 red stars. The end-to-end architecture of the application is shown below. Information This application is polyglot , i.e., the microservices are written in different languages. It\u2019s worth noting that these microservices have no dependencies on the Istio or OSSM, but make an interesting Service Mesh example, particularly because of the multitude of microservices, languages and versions for the reviews microservice. To run Bookinfo on the Service Mesh requires no changes to the application itself . You simply need to run the application in a Service Mesh-enabled environment, with Envoy sidecars injected alongside each microservice. Our OpenShift environment is Service Mesh enabled. OSSM is already installed in our OpenShift cluster and is configured to watch for Service Mesh-enabled deployments to appear in your project, userNN-project. The Service Mesh knows which project to watch through an instance of the Istio Service Mesh Member Roll custom resource. Your userNN ID is not able to access this custom resource, but it is displayed below for your reference. Example Output user01@lab061:~/istio-s390x$ oc describe ServiceMeshMemberRoll/default Name: default Namespace: istio-system Labels: <none> Annotations: <none> API Version: maistra.io/v1 Kind: ServiceMeshMemberRoll Spec: Members: user01-project user02-project user03-project user04-project user05-project Status: Annotations: Configured Member Count: 5/5 Conditions: Last Transition Time: 2021-06-28T19:04:47Z Message: All namespaces have been configured successfully Reason: Configured Status: True Type: Ready Configured Members: user01-project user02-project user03-project user04-project user05-project Mesh Generation: 1 Mesh Reconciled Version: 2.0.6-2.el8-1 Observed Generation: 6 Pending Members: Events: <none> Note The screenshot above has been trimmed down for brevity. User projects up to user30-project are configured. This custom resource will watch each member project and automatically inject Envoy sidecars when a deployment is created with the annotation sidecar.istio.io/inject: \"true\" . The deployments contained in the GitHub repository you pulled already have this annotation configured. From the Istio-s390x directory, view the application deployments with the command : cat platform/kube/bookinfo.yaml You will find that this YAML file contains a Service, a ServiceAccount, and a Deployment for each of the microservices described previously. Find a section of the YAML file that has kind: Deployment, and you will see the sidecar.istio.io/inject: \"true\" annotation in its spec section. Example Output apiVersion: apps/v1 kind: Deployment metadata: name: details-v1 labels: app: details version: v1 spec: replicas: 1 selector: matchLabels: app: details version: v1 template: metadata: annotations: sidecar.istio.io/inject: \"true\" labels: app: details version: v1 spec: serviceAccountName: bookinfo-details containers: - name: details image: quay.io/mmondics/examples-bookinfo-details-v1:1.16.2 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 Once created, this YAML file will set up the entire Bookinfo application for you and the Envoy proxy sidecars will be automatically injected for the Istio control plane to interact with. Create the application by running the following command : oc create -f platform/kube/bookinfo.yaml Example Output user01@lab061:~/istio-s390x$ oc create -f platform/kube/bookinfo.yaml oc create -f platform/kube/bookinfo.yaml service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created Although not yet accessible, all of the application components should be up and running within a few seconds. Check that your four services were created with : oc get services Example Output user01@lab061:~/istio-s390x$ oc get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 172.30.237.245 <none> 9080/TCP 10s productpage ClusterIP 172.30.73.57 <none> 9080/TCP 10s ratings ClusterIP 172.30.241.30 <none> 9080/TCP 10s reviews ClusterIP 172.30.34.195 <none> 9080/TCP 10s And check that your six pods have been created with : oc get pods Example Output user01@lab061:~/istio-s390x$ oc get services NAME READY STATUS RESTARTS AGE details-v1-78cb8b797f-ndqdz 2/2 Running 0 45s productpage-v1-568ddd75bf-rmqtf 2/2 Running 0 45s ratings-v1-768dc65999-q5dft 2/2 Running 0 45s reviews-v1-6cf69f46c9-wbtqm 2/2 Running 0 45s reviews-v2-64fd74bbd7-hx4rd 2/2 Running 0 46s reviews-v3-8cb65c475-wr22r 2/2 Running 0 46s Each pod should have a STATUS : Running and should show Ready : 2/2. The 2/2 indicates that the pod includes two containers, and both are ready. In our case, each pod has one container for its application, and one container for the Envoy proxy, or sidecar. Optional You can list the containers in your productpage pod with the command: oc get pod productpage-v1-xxxxx -o jsonpath={.spec.containers[*].name} (where xxxxx is your randomly generated string of characters returned by oc get pods) user01@lab061:~/istio-s390x$ oc get pod productpage-v1-78797f -o jsonpath={.spec.containers[*].name} productpage istio-proxyuser01@lab061:~/istio-s390x$ Your two container names will be printed at the beginning of the next line - productpage, and Istio-proxy. productpage is the end user\u2019s primary ingress into the Bookinfo application. This is the microservice that will pull the data from the ratings, reviews, and details microservices and display them when requested by a user. Check that the productpage microservice is running correctly by issuing the command : oc exec \"$(oc get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')\" -c ratings -- curl -sS productpage:9080/productpage | grep -o \"<title>.*</title>\" Example Output user01@lab061:~/istio-s390x$ oc exec \"$(oc get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')\" -c ratings -- curl -sS productpage:9080/productpage | grep -o \"<title>.*</title>\" <title>Simple Bookstore App</title> If you see <title>Simple Bookstore App</title> returned, your Bookinfo application is working correctly, yet still inaccessible. In the next section, we will create a Gateway to provide ingress into the Service Mesh application.","title":"Deploying an Application on the Service Mesh"},{"location":"lab010/lab010-6/","text":"Understanding and Deploying a Service Mesh Gateway and VirtualService \u00b6 Gateways are used to manage inbound and outbound traffic for your mesh, letting you specify which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads. Unlike other mechanisms for controlling traffic entering your systems, such as the Kubernetes Ingress APIs, Istio gateways let you use the full power and flexibility of Istio\u2019s traffic routing. You can do this because Istio\u2019s Gateway resource just lets you configure layer 4-6 load balancing properties such as ports to expose, TLS settings, and so on. Then instead of adding application-layer traffic routing (L7) to the same API resource, you bind a regular Istio virtual service to the gateway. This lets you basically manage gateway traffic like any other data plane traffic in an Istio mesh. Gateways are primarily used to manage ingress traffic, but you can also configure egress gateways. An egress gateway lets you configure a dedicated exit node for the traffic leaving the mesh, letting you limit which services can or should access external networks, or to enable secure control of egress traffic to add security to your mesh, for example. We will only be deploying an ingress Gateway in this lab. Along with a Gateway, we will need a VirtualService . A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. You will find a YAML file for a Gateway and VirtualService in your terminal session. From the istio-s390x directory, view the YAML file with the command : cat networking/bookinfo-gateway.yaml Example Output apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : bookinfo-gateway spec : selector : istio : ingressgateway # use default controller servers : - port : number : 80 name : http protocol : HTTP hosts : - \"userNN-project.istio.apps.atsocppa.dmz\" --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : bookinfo spec : hosts : - \"userNN-project.istio.apps.atsocppa.dmz\" gateways : - bookinfo-gateway http : - match : - uri : exact : /productpage - uri : prefix : /static - uri : exact : /login - uri : exact : /logout - uri : prefix : /api/v1/products route : - destination : host : productpage port : number : 9080 You will notice that there are a few instances of userNN in the YAML file that must be edited to match your user number. You can quickly change these by entering the following command. Expand for more information Extra information for those interested\u2026 The hosts field lists the VirtualService\u2019s hosts - in other words, the user-addressable destination or destinations that these routing rules apply to. This is the address or addresses used when sending requests to the service. The virtual service hostname can be an IP address, a DNS name, or, depending on the platform, a short name (such as a Kubernetes service short name) that resolves to a fully qualified domain name (FQDN). You can also use wildcard (\u201d*\u201d) prefixes, letting you create a single set of routing rules for all matching services. The http section contains the virtual service\u2019s routing rules, describing match conditions and actions for routing HTTP/1.1, HTTP2, and gRPC traffic sent to the destination(s) specified in the hosts field (you can also use tcp and tls sections to configure routing rules for TCP and unterminated TLS traffic). A routing rule consists of the destination where you want the traffic to go and zero or more match conditions, depending on your use case. The route section\u2019s destination field specifies the actual destination for traffic that matches this condition. Unlike the VirtuallService\u2019s host(s), the destination\u2019s host must be a real destination that exists in Istio\u2019s service registry or Envoy won\u2019t know where to send traffic to it. This can be a mesh service with proxies or a non-mesh service added using a service entry. In this case we\u2019re running on Kubernetes and the host name is a Kubernetes service name. Make sure that you change to the correct number , i.e. 01 for user01 sed -i 's/NN/<YOUR_USER_NUMBER>/g' networking/bookinfo-gateway.yaml Create the Gateway and VirtualService with the following command : oc create -f networking/bookinfo-gateway.yaml Example Output user01@lab061:~/istio-s390x$ oc create -f networking/bookinfo-gateway.yaml gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created And view the new objects with the command : oc get gateway,virtualservice Note This command will not work if there is a space after the comma. Example Output user01@lab061:~/istio-s390x$ oc get gateway,virtualservice NAME AGE gateway/bookinfo-gateway 42s NAME GATEWAYS HOST virtualservice/bookinfo [\"bookinfo-gateway\"] [\"user15-project.istio.apps.atsocppa.dmz\"] With this Gateway and VirtualService, you are now able to access the application. First, identify your Gateway URL by entering the following command in your terminal : export GATEWAY_URL=$(oc get virtualservice bookinfo -o jsonpath='{.spec.hosts[0]}') Next, enter the following command to print your productpage URL : echo \"http://$GATEWAY_URL/productpage\" Example Output user01@lab061:~/istio-s390x$ export GATEWAY_URL=$(oc get virtualservice bookinfo -o jsonpath='{.spec.hosts[0]}') user01@lab061:~/istio-s390x$ echo \"http://$GATEWAY_URL/productpage\" user01@lab061:~/istio-s390x$ http://user15-project.istio.apps.atsocppa.dmz/productpage Copy the URL that is returned, and paste it into a web browser Hint The URL will be similar to http://userNN-project.istio.apps.atsocppa.dmz/productpage , (where NN is your user number). If all is working correctly, the overall productpage is shown, which displays information about William Shakespeare\u2019s play, The Comedy of Errors. The details on the left are returned by the details microservice, the text for the two reviews is provided by the ratings microservice, and the star ratings (or lack thereof) are returned by one of the three reviews microservices, depending on which was called by the ratings microservice. You now have an application made up of six microservices written in four different languages deployed on the OpenShift Service Mesh and can now take full advantage of its features. We will look at a subset of them in the following sections.","title":"Understanding and Deploying a Service Mesh Gateway and VirtualService"},{"location":"lab010/lab010-6/#understanding-and-deploying-a-service-mesh-gateway-and-virtualservice","text":"Gateways are used to manage inbound and outbound traffic for your mesh, letting you specify which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads. Unlike other mechanisms for controlling traffic entering your systems, such as the Kubernetes Ingress APIs, Istio gateways let you use the full power and flexibility of Istio\u2019s traffic routing. You can do this because Istio\u2019s Gateway resource just lets you configure layer 4-6 load balancing properties such as ports to expose, TLS settings, and so on. Then instead of adding application-layer traffic routing (L7) to the same API resource, you bind a regular Istio virtual service to the gateway. This lets you basically manage gateway traffic like any other data plane traffic in an Istio mesh. Gateways are primarily used to manage ingress traffic, but you can also configure egress gateways. An egress gateway lets you configure a dedicated exit node for the traffic leaving the mesh, letting you limit which services can or should access external networks, or to enable secure control of egress traffic to add security to your mesh, for example. We will only be deploying an ingress Gateway in this lab. Along with a Gateway, we will need a VirtualService . A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. You will find a YAML file for a Gateway and VirtualService in your terminal session. From the istio-s390x directory, view the YAML file with the command : cat networking/bookinfo-gateway.yaml Example Output apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : bookinfo-gateway spec : selector : istio : ingressgateway # use default controller servers : - port : number : 80 name : http protocol : HTTP hosts : - \"userNN-project.istio.apps.atsocppa.dmz\" --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : bookinfo spec : hosts : - \"userNN-project.istio.apps.atsocppa.dmz\" gateways : - bookinfo-gateway http : - match : - uri : exact : /productpage - uri : prefix : /static - uri : exact : /login - uri : exact : /logout - uri : prefix : /api/v1/products route : - destination : host : productpage port : number : 9080 You will notice that there are a few instances of userNN in the YAML file that must be edited to match your user number. You can quickly change these by entering the following command. Expand for more information Extra information for those interested\u2026 The hosts field lists the VirtualService\u2019s hosts - in other words, the user-addressable destination or destinations that these routing rules apply to. This is the address or addresses used when sending requests to the service. The virtual service hostname can be an IP address, a DNS name, or, depending on the platform, a short name (such as a Kubernetes service short name) that resolves to a fully qualified domain name (FQDN). You can also use wildcard (\u201d*\u201d) prefixes, letting you create a single set of routing rules for all matching services. The http section contains the virtual service\u2019s routing rules, describing match conditions and actions for routing HTTP/1.1, HTTP2, and gRPC traffic sent to the destination(s) specified in the hosts field (you can also use tcp and tls sections to configure routing rules for TCP and unterminated TLS traffic). A routing rule consists of the destination where you want the traffic to go and zero or more match conditions, depending on your use case. The route section\u2019s destination field specifies the actual destination for traffic that matches this condition. Unlike the VirtuallService\u2019s host(s), the destination\u2019s host must be a real destination that exists in Istio\u2019s service registry or Envoy won\u2019t know where to send traffic to it. This can be a mesh service with proxies or a non-mesh service added using a service entry. In this case we\u2019re running on Kubernetes and the host name is a Kubernetes service name. Make sure that you change to the correct number , i.e. 01 for user01 sed -i 's/NN/<YOUR_USER_NUMBER>/g' networking/bookinfo-gateway.yaml Create the Gateway and VirtualService with the following command : oc create -f networking/bookinfo-gateway.yaml Example Output user01@lab061:~/istio-s390x$ oc create -f networking/bookinfo-gateway.yaml gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created And view the new objects with the command : oc get gateway,virtualservice Note This command will not work if there is a space after the comma. Example Output user01@lab061:~/istio-s390x$ oc get gateway,virtualservice NAME AGE gateway/bookinfo-gateway 42s NAME GATEWAYS HOST virtualservice/bookinfo [\"bookinfo-gateway\"] [\"user15-project.istio.apps.atsocppa.dmz\"] With this Gateway and VirtualService, you are now able to access the application. First, identify your Gateway URL by entering the following command in your terminal : export GATEWAY_URL=$(oc get virtualservice bookinfo -o jsonpath='{.spec.hosts[0]}') Next, enter the following command to print your productpage URL : echo \"http://$GATEWAY_URL/productpage\" Example Output user01@lab061:~/istio-s390x$ export GATEWAY_URL=$(oc get virtualservice bookinfo -o jsonpath='{.spec.hosts[0]}') user01@lab061:~/istio-s390x$ echo \"http://$GATEWAY_URL/productpage\" user01@lab061:~/istio-s390x$ http://user15-project.istio.apps.atsocppa.dmz/productpage Copy the URL that is returned, and paste it into a web browser Hint The URL will be similar to http://userNN-project.istio.apps.atsocppa.dmz/productpage , (where NN is your user number). If all is working correctly, the overall productpage is shown, which displays information about William Shakespeare\u2019s play, The Comedy of Errors. The details on the left are returned by the details microservice, the text for the two reviews is provided by the ratings microservice, and the star ratings (or lack thereof) are returned by one of the three reviews microservices, depending on which was called by the ratings microservice. You now have an application made up of six microservices written in four different languages deployed on the OpenShift Service Mesh and can now take full advantage of its features. We will look at a subset of them in the following sections.","title":"Understanding and Deploying a Service Mesh Gateway and VirtualService"},{"location":"lab010/lab010-7/","text":"Traffic Management \u00b6 Istio\u2019s traffic routing rules let you easily control the flow of traffic and API calls between services. Istio simplifies configuration of service-level properties like circuit breakers, timeouts, and retries, and makes it easy to set up important tasks like A/B testing, canary rollouts, and staged rollouts with percentage-based traffic splits. It also provides out-of-box failure recovery features that help make your application more robust against failures of dependent services or the network. Refresh the productpage a few times and you will see either black stars, red stars, or no stars. This is because, by default, Istio uses a round-robin load balancing policy, where each service instance in the instance pool gets a request in turn. Istio also supports the following models, which you can specify in destination rules for requests to a particular service or service subset. Random: Requests are forwarded at random to instances in the pool. Weighted: Requests are forwarded to instances in the pool according to a specific percentage. Least requests: Requests are forwarded to instances with the least number of requests. Along with VirtualServices, DestinationRules are a key part of Istio\u2019s traffic routing functionality. You can think of virtual services as how you route your traffic to a given destination, and then you use destination rules to configure what happens to traffic for that destination. Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic\u2019s \u201creal\u201d destination. There is a DestinationRule YAML file provided in the networking directory. Take a look at it with : cat networking/destination-rule-all.yaml Example Output apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : productpage spec : host : productpage subsets : - name : v1 labels : version : v1 --- apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : reviews spec : host : reviews subsets : - name : v1 labels : version : v1 - name : v2 labels : version : v2 - name : v3 labels : version : v3 --- More cut from screenshot This will be our baseline DestinationRules with no special routing or load balancing included. These DesintationRules simply describe the various versions of each microservce (v1, v2, v3). Create the DestinationRules with the command : oc create -f networking/destination-rule-all.yaml Example Output user01@lab061:~/istio-s390x$ oc create -f networking/destination-rule-all.yaml destinationrule.networking.istio.io/productpage created destinationrule.networking.istio.io/reviews created destinationrule.networking.istio.io/ratings created destinationrule.networking.istio.io/details created Now that OpenShift knows which versions of each microservice are available, we can use Istio to control the version routing. For example, each of the microservices in Bookinfo application include a version v1. We can deploy a new VirtualService that routes all traffic to the v1 microservices. Look at the v1-specific VirtualService with the command : cat networking/virtual-service-all-v1.yaml Example Output apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : productpage spec : hosts : - productpage http : - route : - destination : host : productpage subset : v1 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : reviews spec : hosts : - reviews http : - route : - destination : host : reviews subset : v1 --- More cut from screenshot And notice that v1 is the only version specified for each microservice. Create the new VirtualService with the command : oc apply -f networking/virtual-service-all-v1.yaml Example Output user01@lab061:~/istio-s390x$ oc apply -f networking/virtual-service-all-v1.yaml virtualservice.networking.istio.io/productpage created virtualservice.networking.istio.io/reviews created virtualservice.networking.istio.io/ratings created virtualservice.networking.istio.io/details created You have just configured Istio to route all traffic to the v1 version of each microservice, most importantly the reviews microservice that decides which stars are displayed on the productpage. Back in your web browser, refresh the productpage a few times . You should notice now that no matter how many times you refresh, no stars will be displayed, because the new VirtualService only allows you to reach v1 of reviews. You can also control the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service reviews:v2. Run the following command to enable user-based routing : oc apply -f networking/virtual-service-reviews-test-v2.yaml In your web browser, log into the productpage as user: Jason with password: Jason . Refresh the productpage a few times again and notice that Jason is only able to reach v2 of the reviews microservice, which displays black stars. On the productpage, sign out, then sign in with a different user: Richard with password: Richard . Refresh the productpage a few times again and notice that Richard can only reach v1 of the reviews microservice, which does not display stars . In this task, you used Istio to send 100% of the traffic to the v1 version of each of the Bookinfo services. You then set a rule to selectively send traffic to version v2 of the reviews service based on a custom end-user header (for Jason) added to the request by the productpage service.","title":"Traffic Management"},{"location":"lab010/lab010-7/#traffic-management","text":"Istio\u2019s traffic routing rules let you easily control the flow of traffic and API calls between services. Istio simplifies configuration of service-level properties like circuit breakers, timeouts, and retries, and makes it easy to set up important tasks like A/B testing, canary rollouts, and staged rollouts with percentage-based traffic splits. It also provides out-of-box failure recovery features that help make your application more robust against failures of dependent services or the network. Refresh the productpage a few times and you will see either black stars, red stars, or no stars. This is because, by default, Istio uses a round-robin load balancing policy, where each service instance in the instance pool gets a request in turn. Istio also supports the following models, which you can specify in destination rules for requests to a particular service or service subset. Random: Requests are forwarded at random to instances in the pool. Weighted: Requests are forwarded to instances in the pool according to a specific percentage. Least requests: Requests are forwarded to instances with the least number of requests. Along with VirtualServices, DestinationRules are a key part of Istio\u2019s traffic routing functionality. You can think of virtual services as how you route your traffic to a given destination, and then you use destination rules to configure what happens to traffic for that destination. Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic\u2019s \u201creal\u201d destination. There is a DestinationRule YAML file provided in the networking directory. Take a look at it with : cat networking/destination-rule-all.yaml Example Output apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : productpage spec : host : productpage subsets : - name : v1 labels : version : v1 --- apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : reviews spec : host : reviews subsets : - name : v1 labels : version : v1 - name : v2 labels : version : v2 - name : v3 labels : version : v3 --- More cut from screenshot This will be our baseline DestinationRules with no special routing or load balancing included. These DesintationRules simply describe the various versions of each microservce (v1, v2, v3). Create the DestinationRules with the command : oc create -f networking/destination-rule-all.yaml Example Output user01@lab061:~/istio-s390x$ oc create -f networking/destination-rule-all.yaml destinationrule.networking.istio.io/productpage created destinationrule.networking.istio.io/reviews created destinationrule.networking.istio.io/ratings created destinationrule.networking.istio.io/details created Now that OpenShift knows which versions of each microservice are available, we can use Istio to control the version routing. For example, each of the microservices in Bookinfo application include a version v1. We can deploy a new VirtualService that routes all traffic to the v1 microservices. Look at the v1-specific VirtualService with the command : cat networking/virtual-service-all-v1.yaml Example Output apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : productpage spec : hosts : - productpage http : - route : - destination : host : productpage subset : v1 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : reviews spec : hosts : - reviews http : - route : - destination : host : reviews subset : v1 --- More cut from screenshot And notice that v1 is the only version specified for each microservice. Create the new VirtualService with the command : oc apply -f networking/virtual-service-all-v1.yaml Example Output user01@lab061:~/istio-s390x$ oc apply -f networking/virtual-service-all-v1.yaml virtualservice.networking.istio.io/productpage created virtualservice.networking.istio.io/reviews created virtualservice.networking.istio.io/ratings created virtualservice.networking.istio.io/details created You have just configured Istio to route all traffic to the v1 version of each microservice, most importantly the reviews microservice that decides which stars are displayed on the productpage. Back in your web browser, refresh the productpage a few times . You should notice now that no matter how many times you refresh, no stars will be displayed, because the new VirtualService only allows you to reach v1 of reviews. You can also control the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service reviews:v2. Run the following command to enable user-based routing : oc apply -f networking/virtual-service-reviews-test-v2.yaml In your web browser, log into the productpage as user: Jason with password: Jason . Refresh the productpage a few times again and notice that Jason is only able to reach v2 of the reviews microservice, which displays black stars. On the productpage, sign out, then sign in with a different user: Richard with password: Richard . Refresh the productpage a few times again and notice that Richard can only reach v1 of the reviews microservice, which does not display stars . In this task, you used Istio to send 100% of the traffic to the v1 version of each of the Bookinfo services. You then set a rule to selectively send traffic to version v2 of the reviews service based on a custom end-user header (for Jason) added to the request by the productpage service.","title":"Traffic Management"},{"location":"lab010/lab010-8/","text":"Application Observability with Kiali \u00b6 Kiali provides visibility into your service mesh by showing you the microservices in your service mesh, and how they are connected. Kiali provides an interactive graph view of your namespace in real time that provides visibility into features like circuit breakers, request rates, latency, and even graphs of traffic flows. Kiali offers insights about components at different levels, from Applications to Services and Workloads, and can display the interactions with contextual information and charts on the selected graph node or edge. Kiali also provides the ability to validate your Istio configurations, such as gateways, destination rules, virtual services, mesh policies, and more. Navigate to the Kiali console located at: https://kiali-istio-system.apps.atsocppa.dmz Log in with your OpenShift credentials . View the overview of your mesh in the Overview page that appears immediately after you log in . The Overview page displays all the namespaces that have services in the mesh. The namespaces shown are those previously discussed in the Service Mesh Member Roll. Validating Istio Configuration with Kiali \u00b6 Kiali can validate your Istio resources to ensure they follow proper conventions and semantics. Any problems detected in the configuration of your Istio resources can be flagged as errors or warnings depending on the severity of the incorrect configuration. You might have noticed on the Kiali overview page that there is an error somewhere within your userNN-project Istio configuration. Let\u2019s find and fix that error. In the left-side menu of the Kiali console, navigate to Istio Config, and then filter to your userNN-project namespace . You should notice an error icon in the Configuration column for your details microservice. Click the details hyperlink to drill down into the microservice and scroll to the bottom of the YAML file . There is an error with v2 of the details DestinationRule. For more specificity, hover over the red X to the left of the YAML file . You will see the error code KIA0203 This subset\u2019s labels are not found in any matching host. Essentially, the details DestinationRule is failing to find the v2 host for the details microservice. If you look back at our Bookinfo application architecture, there is only supposed to be one version of the details microservice - v1 is the only version that exists. Correct the error by deleting lines 30-32 of the YAML file, resulting in the following : Click on the Istio Config tab in the left-side menu again to navigate back to the configuration page for your project . You should no longer have any configuration issues in your Istio configuration. Validating Service Mesh Application Configuration with Kiali \u00b6 Along with identifying issues with the Istio configuration, Kiali can also identify issues with the applications running on the Service Mesh . In your terminal session, introduce an invalid configuration of a service port name with the following command : oc patch service details --type json -p \\ '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"foo\"}]' Example Output user01@lab061:~/istio-s390x$ oc patch service details --type json -p \\ > '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"foo\"}]' service/details patched If you see the service/details patched message as in the image above, your patch was successful. Back in the Kiali web console, navigate to the Services page from the left side menu . You will notice that you now have an error icon under the Configuration column for the details service. Click the details hyperlink and then click the Network option under Service Info . Hover over the error icon to display a tool tip describing the error. This error is telling you that your port name does not follow the correct syntax. Back in your terminal session, run the following command to correct the port name : oc patch service details--type json -p \\ '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"http\"}]' Example Output user01@lab061:~/istio-s390x$ oc patch service details --type json -p \\ > '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"http\"}]' service/details patched Back in the Kiali console, click the blue refresh button and see that your error has been fixed . Now that our Service Mesh is configured correctly, along with the application running on top of it, let\u2019s explore some of the other features that Kiali provides. Viewing Your Service Mesh Applications with Kiali \u00b6 To view your namespace graph, Select the Graph option in the left side menu and select your project in the namespace dropdown . The page looks similar to: The graph represents traffic flowing through the service mesh for a period of time, generated using Istio telemetry. Let\u2019s generate some traffic into our Bookinfo application. In your terminal session, run the following command to continually send http request to the productpage : watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage Example Output user01@lab061:~/istio-s390x$ watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage Every 1.0s: curl -o /dev/null -s -w %{http_code} user01-project.istio.apps.atsocppa.dmz/productpage 200 If you see a 200 return code as in the image above, you\u2019re now sending requests to the productpage every second. Important Note: you will want to leave this watch command running until otherwise directed. Open a second terminal session and connect to the environment as directed previously in this lab . To view a summary of metrics, select any node or edge in the graph to display its metric details in the summary details panel on the right . For example, click on the triangle representing the productpage service , and you should see a 100% success rate for both inbound and outbound traffic. On this same page, click the Display dropdown and select Requests Percentage and Service Nodes, if they aren\u2019t already selected . This will let you view the percentage of traffic to each workload in near real-time. For example, since there are three versions of the reviews microservice and traffic is being distributed in a round-robin fashion, you should see close to 33% traffic going from the productpage to each version of reviews. Your percentages are likely not exactly 33.3%, as in the image above. More often, they will vary between 20% and 40%. Managing Service Mesh Applications with Kiali \u00b6 Kiali does not only allow visibility into your service mesh application, Kiali can also be used to directly interact with the application. Click the reviews service represented by a triangle. In the right-side menu that pops up, click on the hyperlink for the reviews service . You are taken to the Services page (instead of Graph, where you previously were). This shows expanded information about the reviews service, its properties and versions, metrics about its traffic, and more. Click the Actions dropdown in the top right of the page to see the traffic management options you have - Request Routing, Fault Injection, Traffic Shifting, and Request Timeouts . These should all be grayed out right now because these are managed through DestinationRules, and we already created one for the reviews service in a previous step. Delete the Traffic Routing from this Actions dropdown and confirm that you want to delete the DestinationRule: \u2018reviews\u2019 . Click the Actions dropdown again and you will notice that the options can now be selected and click the Traffic Shifting option . From this page, you can create a new Traffic Shifting rule to manage how much traffic should be directed to each version of the reviews microservice. Slide the slider for reviews-v3 all the way to the left for 0%, and then make reviews-v1 and reviews-v2 50% and click create . Your page should look like the image below. Navigate back to the Graph page from the left side menu . Over the next few minutes (and depending on the graph\u2019s refresh rate that you can edit in the top right of the Graph page) you will see that the percentage of traffic going to v3 of reviews will decrease towards 0%, while the traffic going to v1 and v2 will increase towards 50%. In your web browser, navigate back to your bookinfo productpage and refresh the page a few times . No matter how many times you refresh, you will not see the red stars again. That is because no traffic can reach v3 of the reviews microservice, which is the version that provides red stars. As you can tell from the past few sections, you can control Service Mesh applications either from the Command Line by creating VirtualServices and DestinationRules, or by using the Kiali GUI console. Using the Kiali console simply generates the VirtualServices and DestinationRules for you, however the Command Line offers greater flexibility, more control, and the ability to automate the creation of these rules. We will now move on from Kiali to Jaeger, the tool that OSSM uses for distributed tracing. At this point, feel free to explore Kiali and the other functions it provides. There are many things Kiali can do that we will not be covering in this lab. You can find more information in the Kiali documentation here: https://kiali.io/documentation/latest/features/","title":"Application Observability with Kiali"},{"location":"lab010/lab010-8/#application-observability-with-kiali","text":"Kiali provides visibility into your service mesh by showing you the microservices in your service mesh, and how they are connected. Kiali provides an interactive graph view of your namespace in real time that provides visibility into features like circuit breakers, request rates, latency, and even graphs of traffic flows. Kiali offers insights about components at different levels, from Applications to Services and Workloads, and can display the interactions with contextual information and charts on the selected graph node or edge. Kiali also provides the ability to validate your Istio configurations, such as gateways, destination rules, virtual services, mesh policies, and more. Navigate to the Kiali console located at: https://kiali-istio-system.apps.atsocppa.dmz Log in with your OpenShift credentials . View the overview of your mesh in the Overview page that appears immediately after you log in . The Overview page displays all the namespaces that have services in the mesh. The namespaces shown are those previously discussed in the Service Mesh Member Roll.","title":"Application Observability with Kiali"},{"location":"lab010/lab010-8/#validating-istio-configuration-with-kiali","text":"Kiali can validate your Istio resources to ensure they follow proper conventions and semantics. Any problems detected in the configuration of your Istio resources can be flagged as errors or warnings depending on the severity of the incorrect configuration. You might have noticed on the Kiali overview page that there is an error somewhere within your userNN-project Istio configuration. Let\u2019s find and fix that error. In the left-side menu of the Kiali console, navigate to Istio Config, and then filter to your userNN-project namespace . You should notice an error icon in the Configuration column for your details microservice. Click the details hyperlink to drill down into the microservice and scroll to the bottom of the YAML file . There is an error with v2 of the details DestinationRule. For more specificity, hover over the red X to the left of the YAML file . You will see the error code KIA0203 This subset\u2019s labels are not found in any matching host. Essentially, the details DestinationRule is failing to find the v2 host for the details microservice. If you look back at our Bookinfo application architecture, there is only supposed to be one version of the details microservice - v1 is the only version that exists. Correct the error by deleting lines 30-32 of the YAML file, resulting in the following : Click on the Istio Config tab in the left-side menu again to navigate back to the configuration page for your project . You should no longer have any configuration issues in your Istio configuration.","title":"Validating Istio Configuration with Kiali"},{"location":"lab010/lab010-8/#validating-service-mesh-application-configuration-with-kiali","text":"Along with identifying issues with the Istio configuration, Kiali can also identify issues with the applications running on the Service Mesh . In your terminal session, introduce an invalid configuration of a service port name with the following command : oc patch service details --type json -p \\ '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"foo\"}]' Example Output user01@lab061:~/istio-s390x$ oc patch service details --type json -p \\ > '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"foo\"}]' service/details patched If you see the service/details patched message as in the image above, your patch was successful. Back in the Kiali web console, navigate to the Services page from the left side menu . You will notice that you now have an error icon under the Configuration column for the details service. Click the details hyperlink and then click the Network option under Service Info . Hover over the error icon to display a tool tip describing the error. This error is telling you that your port name does not follow the correct syntax. Back in your terminal session, run the following command to correct the port name : oc patch service details--type json -p \\ '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"http\"}]' Example Output user01@lab061:~/istio-s390x$ oc patch service details --type json -p \\ > '[{\"op\":\"replace\",\"path\":\"/spec/ports/0/name\", \"value\":\"http\"}]' service/details patched Back in the Kiali console, click the blue refresh button and see that your error has been fixed . Now that our Service Mesh is configured correctly, along with the application running on top of it, let\u2019s explore some of the other features that Kiali provides.","title":"Validating Service Mesh Application Configuration with Kiali"},{"location":"lab010/lab010-8/#viewing-your-service-mesh-applications-with-kiali","text":"To view your namespace graph, Select the Graph option in the left side menu and select your project in the namespace dropdown . The page looks similar to: The graph represents traffic flowing through the service mesh for a period of time, generated using Istio telemetry. Let\u2019s generate some traffic into our Bookinfo application. In your terminal session, run the following command to continually send http request to the productpage : watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage Example Output user01@lab061:~/istio-s390x$ watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage Every 1.0s: curl -o /dev/null -s -w %{http_code} user01-project.istio.apps.atsocppa.dmz/productpage 200 If you see a 200 return code as in the image above, you\u2019re now sending requests to the productpage every second. Important Note: you will want to leave this watch command running until otherwise directed. Open a second terminal session and connect to the environment as directed previously in this lab . To view a summary of metrics, select any node or edge in the graph to display its metric details in the summary details panel on the right . For example, click on the triangle representing the productpage service , and you should see a 100% success rate for both inbound and outbound traffic. On this same page, click the Display dropdown and select Requests Percentage and Service Nodes, if they aren\u2019t already selected . This will let you view the percentage of traffic to each workload in near real-time. For example, since there are three versions of the reviews microservice and traffic is being distributed in a round-robin fashion, you should see close to 33% traffic going from the productpage to each version of reviews. Your percentages are likely not exactly 33.3%, as in the image above. More often, they will vary between 20% and 40%.","title":"Viewing Your Service Mesh Applications with Kiali"},{"location":"lab010/lab010-8/#managing-service-mesh-applications-with-kiali","text":"Kiali does not only allow visibility into your service mesh application, Kiali can also be used to directly interact with the application. Click the reviews service represented by a triangle. In the right-side menu that pops up, click on the hyperlink for the reviews service . You are taken to the Services page (instead of Graph, where you previously were). This shows expanded information about the reviews service, its properties and versions, metrics about its traffic, and more. Click the Actions dropdown in the top right of the page to see the traffic management options you have - Request Routing, Fault Injection, Traffic Shifting, and Request Timeouts . These should all be grayed out right now because these are managed through DestinationRules, and we already created one for the reviews service in a previous step. Delete the Traffic Routing from this Actions dropdown and confirm that you want to delete the DestinationRule: \u2018reviews\u2019 . Click the Actions dropdown again and you will notice that the options can now be selected and click the Traffic Shifting option . From this page, you can create a new Traffic Shifting rule to manage how much traffic should be directed to each version of the reviews microservice. Slide the slider for reviews-v3 all the way to the left for 0%, and then make reviews-v1 and reviews-v2 50% and click create . Your page should look like the image below. Navigate back to the Graph page from the left side menu . Over the next few minutes (and depending on the graph\u2019s refresh rate that you can edit in the top right of the Graph page) you will see that the percentage of traffic going to v3 of reviews will decrease towards 0%, while the traffic going to v1 and v2 will increase towards 50%. In your web browser, navigate back to your bookinfo productpage and refresh the page a few times . No matter how many times you refresh, you will not see the red stars again. That is because no traffic can reach v3 of the reviews microservice, which is the version that provides red stars. As you can tell from the past few sections, you can control Service Mesh applications either from the Command Line by creating VirtualServices and DestinationRules, or by using the Kiali GUI console. Using the Kiali console simply generates the VirtualServices and DestinationRules for you, however the Command Line offers greater flexibility, more control, and the ability to automate the creation of these rules. We will now move on from Kiali to Jaeger, the tool that OSSM uses for distributed tracing. At this point, feel free to explore Kiali and the other functions it provides. There are many things Kiali can do that we will not be covering in this lab. You can find more information in the Kiali documentation here: https://kiali.io/documentation/latest/features/","title":"Managing Service Mesh Applications with Kiali"},{"location":"lab010/lab010-9/","text":"Distributed Tracing with Jaeger \u00b6 Distributed Tracing is the process of tracking the performance of individual microservices in an application by tracing the path of the service calls in the application. Each time a user takes action in an application, a request is executed that might require many microservices to interact to produce a response. Jaeger is an open source distributed tracing system used by OpenShift Service Mesh. With Jaeger, you can perform a trace that follows the path of a request through various microservices which make up an application. For our Bookinfo application, traces are generated when HTTP requests are made to the productpage microservice. This starts a cascade of requests to the other microservices in the Bookinfo mesh. Before we start looking at traces, check that your watch command is still running in your terminal session . Example Output Every 1.0s: curl -o /dev/null -s -w %{http_code} user01-project.istio.apps.atsocppa.dmz/productpage 200 If you see the 200 status code return and the time in the top right is current, you are still sending requests to your productpage and will be able to generate traces. Note If your watch command has been stopped for whatever reason, start it again with the command: watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage Navigate to the Jaeger console located at: https://jaeger-istio-system.apps.atsocppa.dmz Login with your OpenShift credentials . In the Service dropdown, select productpage.userNN-project . If others are doing this same lab, it might be easier to search for your userNN-project. Scroll to the bottom-left of the page and click the Find Traces button . The rest of the page will be populated by a graph of your traces over time, and the traces that meet your search criteria. You will notice that some of your traces have much longer durations than others - don\u2019t worry about these. This is due to hard-coded http timeouts in the productpage python application code. The graph displayed at the top of the page has circles representing traces, with time on the x-axis and duration of the trace on the y-axis. The size of the circle represents how many spans make up the trace. A span is the logical unit of work associated with one microservice in the mesh. Click one of the traces in the list below the graph. Select a trace that has 8 spans, as described in the left side of the box . You will be taken to a page that looks like the following: This graph shows how long each microservice took, when it started, when it ended, and includes detailed information about each span. Expand the span for your productpage, and then expand the Tags row . Here you will find more information about the productpage span that may be helpful with debugging an application issue or latency. Click the back arrow in the top left of the page to navigate back to your project traces . Jaeger includes a feature to compare two traces to one another. In your list of traces, find one trace that has 8 spans, and another that has 6 spans . Click the checkboxes next to the names of the traces, and then Compare Traces to the top right of the traces list . The resulting page shows the microservices that exist in trace A, trace B, and both traces you selected. Your comparison will likely look different. Any gray nodes are microservices that exist in both traces at the same version. Any red nodes exist in Trace A, but not Trace B. Any green nodes exist in Trace B, but not Trace A. If you\u2019re comparing the Traces in the screenshot above, you can discern that the ratings microservice is not being called in Trace B, and that the versions of the reviews microservices are different in the two traces. Note The distributed tracing sampling rate is set to sample 100% of traces in your Service Mesh by default. A high sampling rate consumes cluster resources and performance but is useful when debugging issues. Before you deploy Red Hat OpenShift Service Mesh in production, you would want to set the value to a smaller proportion of traces.","title":"Distributed Tracing with Jaeger"},{"location":"lab010/lab010-9/#distributed-tracing-with-jaeger","text":"Distributed Tracing is the process of tracking the performance of individual microservices in an application by tracing the path of the service calls in the application. Each time a user takes action in an application, a request is executed that might require many microservices to interact to produce a response. Jaeger is an open source distributed tracing system used by OpenShift Service Mesh. With Jaeger, you can perform a trace that follows the path of a request through various microservices which make up an application. For our Bookinfo application, traces are generated when HTTP requests are made to the productpage microservice. This starts a cascade of requests to the other microservices in the Bookinfo mesh. Before we start looking at traces, check that your watch command is still running in your terminal session . Example Output Every 1.0s: curl -o /dev/null -s -w %{http_code} user01-project.istio.apps.atsocppa.dmz/productpage 200 If you see the 200 status code return and the time in the top right is current, you are still sending requests to your productpage and will be able to generate traces. Note If your watch command has been stopped for whatever reason, start it again with the command: watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage Navigate to the Jaeger console located at: https://jaeger-istio-system.apps.atsocppa.dmz Login with your OpenShift credentials . In the Service dropdown, select productpage.userNN-project . If others are doing this same lab, it might be easier to search for your userNN-project. Scroll to the bottom-left of the page and click the Find Traces button . The rest of the page will be populated by a graph of your traces over time, and the traces that meet your search criteria. You will notice that some of your traces have much longer durations than others - don\u2019t worry about these. This is due to hard-coded http timeouts in the productpage python application code. The graph displayed at the top of the page has circles representing traces, with time on the x-axis and duration of the trace on the y-axis. The size of the circle represents how many spans make up the trace. A span is the logical unit of work associated with one microservice in the mesh. Click one of the traces in the list below the graph. Select a trace that has 8 spans, as described in the left side of the box . You will be taken to a page that looks like the following: This graph shows how long each microservice took, when it started, when it ended, and includes detailed information about each span. Expand the span for your productpage, and then expand the Tags row . Here you will find more information about the productpage span that may be helpful with debugging an application issue or latency. Click the back arrow in the top left of the page to navigate back to your project traces . Jaeger includes a feature to compare two traces to one another. In your list of traces, find one trace that has 8 spans, and another that has 6 spans . Click the checkboxes next to the names of the traces, and then Compare Traces to the top right of the traces list . The resulting page shows the microservices that exist in trace A, trace B, and both traces you selected. Your comparison will likely look different. Any gray nodes are microservices that exist in both traces at the same version. Any red nodes exist in Trace A, but not Trace B. Any green nodes exist in Trace B, but not Trace A. If you\u2019re comparing the Traces in the screenshot above, you can discern that the ratings microservice is not being called in Trace B, and that the versions of the reviews microservices are different in the two traces. Note The distributed tracing sampling rate is set to sample 100% of traces in your Service Mesh by default. A high sampling rate consumes cluster resources and performance but is useful when debugging issues. Before you deploy Red Hat OpenShift Service Mesh in production, you would want to set the value to a smaller proportion of traces.","title":"Distributed Tracing with Jaeger"}]}