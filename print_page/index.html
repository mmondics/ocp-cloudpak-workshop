
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://mmondics.github.io/ocp-cloudpak-workshop/print_page/">
      
      <link rel="icon" href="../images/openshift-logo.png">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.3.0">
    
    
      
        <title>Print Site - Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    
      


    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
  <script>
      (function(s,t,a,n){s[t]||(s[t]=a,n=s[a]=function(){n.q.push(arguments)},
      n.q=[],n.v=2,n.l=1*new Date)})(window,"InstanaEumObject","ineum");
    
      ineum('reportingUrl', 'https://eum-orange-saas.instana.io');
      ineum('key', '5TTGOP2NTWKuA4a5VujUxA');
      ineum('page', pageName);
      ineum('user', userId);
    </script>
    <script defer crossorigin="anonymous" src="https://eum.instana.io/eum.min.js"></script>

    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop" class="md-nav__button md-logo" aria-label="Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop" data-md-component="logo">
      
  <img src="../images/openshift-logo.png" alt="logo">

    </a>
    Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/mmondics/ocp-cloudpak-workshop/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../prerequisites/" class="md-nav__link">
        Prerequisites
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../lab-assignments/" class="md-nav__link">
        Lab Assignments
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../workshop-architecture/" class="md-nav__link">
        Lab Architecture
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Presentations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Presentations" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Presentations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../presentations/presentation1.pdf" class="md-nav__link">
        Presentation 1 - Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../presentations/presentation2.pdf" class="md-nav__link">
        Presentation 2 - Technical Deep Dive
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Labs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Labs" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Labs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_1" type="checkbox" id="__nav_6_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_1">
          Exploring the OpenShift Console
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Exploring the OpenShift Console" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          Exploring the OpenShift Console
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab001/lab001-1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab001/lab001-2/" class="md-nav__link">
        Connect to OCP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab001/lab001-3/" class="md-nav__link">
        The Administrator Persepctive
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab001/lab001-4/" class="md-nav__link">
        The Developer Perspective
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab001/lab001-5/" class="md-nav__link">
        Deploy from the Developer Catalog
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab001/lab001-6/" class="md-nav__link">
        View Workload from the Administrator Perspective
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab001/lab001-7/" class="md-nav__link">
        Cleaning Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_2" type="checkbox" id="__nav_6_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_2">
          Using the OpenShift Command Line (oc)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Using the OpenShift Command Line (oc)" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          Using the OpenShift Command Line (oc)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab002/lab002-1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab002/lab002-2/" class="md-nav__link">
        Log into OpenShift Using the CLI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab002/lab002-3/" class="md-nav__link">
        Overview of the OpenShift CLI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab002/lab002-4/" class="md-nav__link">
        Deploy Container Image from the CLI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab002/lab002-5/" class="md-nav__link">
        Open a Remote Shell Session into the MongoDB Pod
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab002/lab002-6/" class="md-nav__link">
        Working with Pods
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab002/lab002-7/" class="md-nav__link">
        Administrative CLI Commands
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab002/lab002-8/" class="md-nav__link">
        Cleaning Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_3" type="checkbox" id="__nav_6_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_3">
          Deploying an Application from Source Code
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Deploying an Application from Source Code" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_3">
          <span class="md-nav__icon md-icon"></span>
          Deploying an Application from Source Code
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab004/lab004-1/" class="md-nav__link">
        Source-to-Image (S2I) Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab004/lab004-2/" class="md-nav__link">
        Exploring GitHub and the Example Health Source Code
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab004/lab004-3/" class="md-nav__link">
        Connect to OCP and Authenticate
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab004/lab004-4/" class="md-nav__link">
        Edit the Source Code and Push an Update
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab004/lab004-5/" class="md-nav__link">
        Cleaning Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_4" type="checkbox" id="__nav_6_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_4">
          OpenShift Pipelines
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="OpenShift Pipelines" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_4">
          <span class="md-nav__icon md-icon"></span>
          OpenShift Pipelines
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab009/lab009-0/" class="md-nav__link">
        Introduction to OpenShift Pipelines
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab009/lab009-1/" class="md-nav__link">
        Log into OpenShift Using the CLI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab009/lab009-2/" class="md-nav__link">
        Cloning the GitHub Repository and Viewing its Contents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab009/lab009-3/" class="md-nav__link">
        Understanding and Deploying Tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab009/lab009-4/" class="md-nav__link">
        Understanding and Deploying Pipelines
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab009/lab009-5/" class="md-nav__link">
        Running the Pipeline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab009/lab009-6/" class="md-nav__link">
        Access the Application in a Browser
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab009/lab009-7/" class="md-nav__link">
        Cleaning Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_5" type="checkbox" id="__nav_6_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_5">
          OpenShift Service Mesh
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="OpenShift Service Mesh" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_5">
          <span class="md-nav__icon md-icon"></span>
          OpenShift Service Mesh
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-1/" class="md-nav__link">
        Overview of the OpenShift Service Mesh
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-2/" class="md-nav__link">
        OpenShift Service Mesh Architecture
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-3/" class="md-nav__link">
        Log into OpenShift Using the CLI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-4/" class="md-nav__link">
        Cloning the GitHub Repository and Reviewing its Contents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-5/" class="md-nav__link">
        Deploying an Application on the Service Mesh
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-6/" class="md-nav__link">
        Understanding and Deploying a Service Mesh Gateway and Virtual Service
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-7/" class="md-nav__link">
        Traffic Management
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-8/" class="md-nav__link">
        Application Observability with Kiali
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-9/" class="md-nav__link">
        Distributed Tracing with Jaeger
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab010/lab010-10/" class="md-nav__link">
        Wrap Up & Clean Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_6" type="checkbox" id="__nav_6_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_6">
          Monitoring, Metering, and Metrics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Monitoring, Metering, and Metrics" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_6">
          <span class="md-nav__icon md-icon"></span>
          Monitoring, Metering, and Metrics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab005/lab005-1/" class="md-nav__link">
        Overview of OpenShift Monitoring
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab005/lab005-2/" class="md-nav__link">
        Connect to OCP and Authenticate
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab005/lab005-3/" class="md-nav__link">
        Using OpenShift Metrics (Prometheus)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab005/lab005-4/" class="md-nav__link">
        Using the In-Browser Grafana Dashboards
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab005/lab005-5/" class="md-nav__link">
        Connect to Grafana
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab005/lab005-6/" class="md-nav__link">
        Using Grafana Dashboards
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab005/lab005-7/" class="md-nav__link">
        Using OpenShift Alerts (Alertmanager)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_7" type="checkbox" id="__nav_6_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_7">
          Using Persistent Storage with MongoDB and NodeJS
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Using Persistent Storage with MongoDB and NodeJS" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_7">
          <span class="md-nav__icon md-icon"></span>
          Using Persistent Storage with MongoDB and NodeJS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab006/lab006-1/" class="md-nav__link">
        Overview of Persistent Storage and Application Architecture
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab006/lab006-2/" class="md-nav__link">
        Connect to OCP and Authenticate
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab006/lab006-3/" class="md-nav__link">
        Create a PersistentVolumeClaim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab006/lab006-4/" class="md-nav__link">
        Deploy MongoDB from Container Image
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab006/lab006-5/" class="md-nav__link">
        Deploy Node.js Application from Dockerfile
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab006/lab006-6/" class="md-nav__link">
        Interacting with MongoDB from Node.js Application
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab006/lab006-7/" class="md-nav__link">
        Cleaning Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_8" type="checkbox" id="__nav_6_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_8">
          Using the z/OS Cloud Broker
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Using the z/OS Cloud Broker" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_8">
          <span class="md-nav__icon md-icon"></span>
          Using the z/OS Cloud Broker
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab003/lab003-1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab003/lab003-2/" class="md-nav__link">
        Connect to OCP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab003/lab003-3/" class="md-nav__link">
        Deploy Liberty for z/OS Using the z/OS Cloud Broker
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab003/lab003-4/" class="md-nav__link">
        Stop and Restart Liberty for z/OS from OCP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab003/lab003-5/" class="md-nav__link">
        Cleaning Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_9" type="checkbox" id="__nav_6_9" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_9">
          Deploying an Application with the Open Liberty Operator
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Deploying an Application with the Open Liberty Operator" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_9">
          <span class="md-nav__icon md-icon"></span>
          Deploying an Application with the Open Liberty Operator
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab007/lab007-1/" class="md-nav__link">
        Overview of the Open Liberty Operator and Sample Application
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab007/lab007-2/" class="md-nav__link">
        Log into OpenShift Using the CLI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab007/lab007-3/" class="md-nav__link">
        Cloning the GitHub Repository and Reviewing its Contents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab007/lab007-4/" class="md-nav__link">
        Using the Open Liberty Operator to Install an Application
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab007/lab007-5/" class="md-nav__link">
        Access the Application in a Browser
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab007/lab007-6/" class="md-nav__link">
        Cleaning Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_10" type="checkbox" id="__nav_6_10" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_10">
          Deploying an Application with Quarkus Red Hat Runtime
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Deploying an Application with Quarkus Red Hat Runtime" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_10">
          <span class="md-nav__icon md-icon"></span>
          Deploying an Application with Quarkus Red Hat Runtime
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab008/lab008-1/" class="md-nav__link">
        Overview of the Quarkus Red Hat Runtime
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab008/lab008-2/" class="md-nav__link">
        Log into OpenShift Using the CLI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab008/lab008-3/" class="md-nav__link">
        Creating and Reviewing the Quarkus Project
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab008/lab008-4/" class="md-nav__link">
        Configure the Application for OpenShift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab008/lab008-5/" class="md-nav__link">
        Deploy the Application onto OpenShift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../lab008/lab008-6/" class="md-nav__link">
        Cleaning Up
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Print Site
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Print Site
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    Home
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    Prerequisites
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-assignments" class="md-nav__link">
    Lab Assignments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#workshop-architecture" class="md-nav__link">
    Lab Architecture
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-presentations" class="md-nav__link">
    Presentations
  </a>
  
    <nav class="md-nav" aria-label="Presentations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#presentations-presentation1.pdf" class="md-nav__link">
    Presentation 1 - Overview
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#presentations-presentation2.pdf" class="md-nav__link">
    Presentation 2 - Technical Deep Dive
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-labs" class="md-nav__link">
    Labs
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    Home
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    Prerequisites
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-assignments" class="md-nav__link">
    Lab Assignments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#workshop-architecture" class="md-nav__link">
    Lab Architecture
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-presentations" class="md-nav__link">
    Presentations
  </a>
  
    <nav class="md-nav" aria-label="Presentations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#presentations-presentation1.pdf" class="md-nav__link">
    Presentation 1 - Overview
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#presentations-presentation2.pdf" class="md-nav__link">
    Presentation 2 - Technical Deep Dive
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-labs" class="md-nav__link">
    Labs
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index"><h1 id="index-red-hat-openshift-and-ibm-cloud-paks-on-ibm-z-and-linuxone-workshop">Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE Workshop<a class="headerlink" href="#index-red-hat-openshift-and-ibm-cloud-paks-on-ibm-z-and-linuxone-workshop" title="Permanent link">&para;</a></h1>
<p>Welcome to the Red Hat OpenShift and IBM Cloud Paks on IBM Z and LinuxONE workshop. Below you can find the workshop agenda, presentations, and lab documentation.</p>
<h2 id="index-agenda">Agenda<a class="headerlink" href="#index-agenda" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Activity</th>
<th>Duration</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../presentations/presentation1.pdf"><strong>Presentation 1 - High level overview of OpenShift, Cloud Paks, and running them on IBM Z</strong></a></td>
<td>30-45 minutes</td>
</tr>
<tr>
<td><a href="../presentations/presentation2.pdf"><strong>Presentation 2 - Technical Deep Dive</strong></a></td>
<td>~ 1 hour</td>
</tr>
<tr>
<td>Connect to environment as a group</td>
<td>5-10 minutes</td>
</tr>
<tr>
<td><a href="#index-labs"><strong>Hands-on, self-paced labs</strong></a></td>
<td>Remainder of day</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The lab environments will be available the day following the workshop.</p>
<p>For example, If the workshop is on a Thursday, the environments will be available until 5PM EST Friday.</p>
</div>
<h2 id="index-presentations">Presentations<a class="headerlink" href="#index-presentations" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="../presentations/presentation1.pdf">Presentation 1 - High Level Overview of Red Hat OpenShift &amp; IBM Cloud Paks on IBM Z</a></li>
<li><a href="../presentations/presentation2.pdf">Presentation 2 - Technical Deep Dive, Installation &amp; Configuration, Lessons Learned</a></li>
</ul>
<h2 id="index-labs">Labs<a class="headerlink" href="#index-labs" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The labs are designed so that you can pick and choose which you would like to complete. The labs <em>are not</em> designed for you to get through them all in one day.</p>
<p>Labs are non-sequential and have no dependencies on one another.</p>
</div>
<h3 id="index-introductory-labs">Introductory Labs<a class="headerlink" href="#index-introductory-labs" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="#lab001-lab001-1">Exploring the OpenShift Console</a></li>
<li><a href="#lab002-lab002-1">Using the OpenShift Command Line (oc)</a></li>
</ul>
<h3 id="index-openshift-capability-labs">OpenShift Capability Labs<a class="headerlink" href="#index-openshift-capability-labs" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="#lab004-lab004-1">Deploying an Application from Source Code</a></li>
<li><a href="#lab009-lab009-0">OpenShift Pipelines</a></li>
<li><a href="#lab010-lab010-1">OpenShift Service Mesh</a></li>
<li><a href="#lab005-lab005-1">Monitoring, Metering, and Metrics</a></li>
<li><a href="#lab006-lab006-1">Using Persistent Storage - MongoDB and NodeJS</a></li>
</ul>
<h3 id="index-extended-capability-labs">Extended Capability Labs<a class="headerlink" href="#index-extended-capability-labs" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="#lab003-lab003-1">Using the z/OS Cloud Broker</a></li>
<li><a href="#lab007-lab007-1">Deploying an Application with the Open Liberty Operator</a></li>
<li><a href="#lab008-lab008-1">Deploying an Application with Quarkus Red Hat Runtime</a></li>
</ul>
<h2 id="index-workshop-environment-architecture">Workshop Environment Architecture<a class="headerlink" href="#index-workshop-environment-architecture" title="Permanent link">&para;</a></h2>
<p>Please visit <a href="#workshop-architecture">this page</a> to see the architecture of the workshop's lab environment.</p>
<h2 id="index-workshop-owners">Workshop Owners<a class="headerlink" href="#index-workshop-owners" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="mailto:matt.mondics@ibm.com">Matt Mondics</a></li>
<li><a href="mailto:pwnovak@us.ibm.com">Paul Novak</a></li>
</ul></section><section class="print-page" id="prerequisites"><h1 id="prerequisites-prerequisites">Prerequisites<a class="headerlink" href="#prerequisites-prerequisites" title="Permanent link">&para;</a></h1>
<h2 id="prerequisites-github-account">GitHub Account<a class="headerlink" href="#prerequisites-github-account" title="Permanent link">&para;</a></h2>
<p>If you wish to complete <a href="#lab004-lab004-1">Deploying an Application from Source Code</a>, you must have your own GitHub account. You can create one create one by clicking the <em>Sign Up</em> button on the <a href="https://github.com/">GitHub homepage</a>.</p></section><section class="print-page" id="lab-assignments"><h1 id="lab-assignments-lab-assignments">Lab Assignments<a class="headerlink" href="#lab-assignments-lab-assignments" title="Permanent link">&para;</a></h1>
<p><strong>There are connection instructions below the table on this page.</strong></p>
<h2 id="lab-assignments-virtual-machine-openshift-logins">Virtual Machine &amp; OpenShift Logins<a class="headerlink" href="#lab-assignments-virtual-machine-openshift-logins" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Name</th>
<th>Virtual Machine Password</th>
<th>User Number</th>
<th>OpenShift Username</th>
<th>OpenShift Password</th>
</tr>
</thead>
<tbody>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/b851e61d0ac433a9e57b0cfb102f1013/desktops">Alfredo Chang Mariselli</a></td>
<td><strong>yr5dud4p</strong></td>
<td>01</td>
<td>user01</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://cloud.skytap.com/vms/dbd48391b4e1e87b701a360a72beb0c1/desktops">Avill</a></td>
<td><strong>lwwn2j89</strong></td>
<td>02</td>
<td>user02</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/b1ff9dddfbeed1d09a23e466bf551aaa/desktops">Barnett</a></td>
<td><strong>4a20d4m6</strong></td>
<td>03</td>
<td>user03</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://cloud.skytap.com/vms/1878f521e922803edd64071ea36157e5/desktops">Batta</a></td>
<td><strong>eh4k27iz</strong></td>
<td>04</td>
<td>user04</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/7f611a21ecd1eb7d5fb40dd1277d6030/desktops">Culotta</a></td>
<td><strong>ldj6ae68</strong></td>
<td>05</td>
<td>user05</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://cloud.skytap.com/vms/2226642a77175c85ddd20b8449b00e55/desktops">Gandepalli</a></td>
<td><strong>xmd3eqdp</strong></td>
<td>06</td>
<td>user06</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/2ec27fe4ed773a8284e01e68677385e3/desktops">Gonzalez</a></td>
<td><strong>u5zb0d5b</strong></td>
<td>07</td>
<td>user07</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://cloud.skytap.com/vms/4cec7f0a0cc2b52a3e507529f726a970/desktops">Kovalenko</a></td>
<td><strong>sg8dhtyf</strong></td>
<td>08</td>
<td>user08</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/b653ef68304a47e91ae7b99ad5ada9f4/desktops">Kussmau</a></td>
<td><strong>jge0vkff</strong></td>
<td>09</td>
<td>user09</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://cloud.skytap.com/vms/d80f45bfea5bec3def4780acae020828/desktops">Mcdowell</a></td>
<td><strong>spj6lzdb</strong></td>
<td>10</td>
<td>user10</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/e9feed84bffade462acd664e1014c104/desktops">Medo</a></td>
<td><strong>cmryi4fw</strong></td>
<td>11</td>
<td>user11</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://cloud.skytap.com/vms/846d1f6c321a8d998c599f7d26d7c170/desktops">Olin</a></td>
<td><strong>yb3rsk58</strong></td>
<td>12</td>
<td>user12</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/513cc733d62561bc18adf88453b65ed6/desktops">Paul</a></td>
<td><strong>jguv4xhj</strong></td>
<td>13</td>
<td>user13</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://cloud.skytap.com/vms/866934a09b0e854d284bcb6325485c0d/desktops">Pfann</a></td>
<td><strong>o1ei8vlo</strong></td>
<td>14</td>
<td>user14</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/d26d8d91ed398e32c1c77576d41600e5/desktops">Srikakolapu</a></td>
<td><strong>upngpwvy</strong></td>
<td>15</td>
<td>user15</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://cloud.skytap.com/vms/d5757c47914c5f0b6270bbc5d1e7c397/desktops">Stadelman</a></td>
<td><strong>xpnqpw8r</strong></td>
<td>16</td>
<td>user16</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://cloud.skytap.com/vms/b7d15623f03ace57cfd91e9bbd4dd5a9/desktops">Zhang-Cohen</a></td>
<td><strong>x40e6p2l</strong></td>
<td>17</td>
<td>user17</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://google.com">TBD</a></td>
<td><strong>TBD</strong></td>
<td>18</td>
<td>user18</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button" href="https://google.com">TBD</a></td>
<td><strong>TBD</strong></td>
<td>19</td>
<td>user19</td>
<td>p@ssw0rd</td>
</tr>
<tr>
<td><a class="md-button md-button--primary" href="https://google.com">TBD</a></td>
<td><strong>TBD</strong></td>
<td>20</td>
<td>user20</td>
<td>p@ssw0rd</td>
</tr>
</tbody>
</table>
<!---
| [TBD](https://google.com){ .md-button } | **TBD** |  21 | user21 | p@ssw0rd |
| [TBD](https://google.com){ .md-button .md-button--primary } | **TBD** |  22 | user22 | p@ssw0rd |
| [TBD](https://google.com){ .md-button } | **TBD** |  23 | user23 | p@ssw0rd |
| [TBD](https://google.com){ .md-button .md-button--primary } | **TBD** |  24 | user24 | p@ssw0rd |
| [TBD](https://google.com){ .md-button } | **TBD** |  25 | user25 | p@ssw0rd |
| [TBD](https://google.com){ .md-button .md-button--primary } | **TBD** |  26 | user26 | p@ssw0rd |
| [TBD](https://google.com){ .md-button } | **TBD** |  27 | user27 | p@ssw0rd |
| [TBD](https://google.com){ .md-button .md-button--primary } | **TBD** |  28 | user28 | p@ssw0rd |
| [TBD](https://google.com){ .md-button } | **TBD** |  29 | user29 | p@ssw0rd |
| [TBD](https://google.com){ .md-button .md-button--primary } | **TBD** |  30 | user30 | p@ssw0rd |
--->

<h2 id="lab-assignments-connecting-to-your-rhel-virtual-desktop">Connecting to your RHEL Virtual Desktop<a class="headerlink" href="#lab-assignments-connecting-to-your-rhel-virtual-desktop" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p><strong>Click the link to your personal virtual machine</strong> and <strong>enter the Virtual Machine Password provided on the table</strong>.</p>
</li>
<li>
<p><strong>Click the box for the RHEL desktop</strong> that should be green and running.</p>
<p><img alt="rhel-running" src="../images/rhel-running.png" /></p>
</li>
<li>
<p><strong>Log into the RHEL desktop</strong> with the password: <code>p@ssw0rd</code>.</p>
<p><img alt="rhel-login" src="../images/rhel-login.png" /></p>
<div class="admonition note">
<p class="admonition-title"><img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /> Important <img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /></p>
<p>Each virtual machine has a 3 hour inactivity timer. If you exceed this timeout, you can restart the virtual machine, but this will log you off of the VPN. If this happens, reach out to an instructor and they will log back into the VPN for you.</p>
</div>
</li>
</ol></section><section class="print-page" id="workshop-architecture"><h1 id="workshop-architecture-workshop-architecture-diagram">Workshop Architecture Diagram<a class="headerlink" href="#workshop-architecture-workshop-architecture-diagram" title="Permanent link">&para;</a></h1>
<p><img alt="workshop-arch" src="../ocpz-workshop-arch.drawio.svg" /></p>
<p>The OpenShift (OCP) on IBM Z environment used in this workshop is detailed in the diagram above.</p>
<p>Note that this <em>is not</em> the recommended OpenShift architecture for high availablity or production. For OCP on Z reference architectures <a href="https://www.ibm.com/docs/en/linux-on-systems?topic=openshift-reference-architecture">navigate to this link</a>.</p>
<p>The entire lab environment is behing the Washington Systems Center VPN. You are given a RHEL virtual machine with the Cisco AnyConnect VPN client installed and running which provides access to the WSC environment.</p>
<p>There are 3 OpenShift clusters you access during the labs. Because of the wide variety of lab material that requires different <a href="https://cloud.redhat.com/learn/topics/operators">operators</a>, each with their own resource and version requirements, it is simpler to divide labs on to multiple clusters.</p>
<p>Each OCP cluster is made up of 3 Control Planes and 3 Compute Nodes as shown in OCP Cluster 1 in the diagram. The Control Planes and Compute Nodes have a minimum of the resources shown for OCP cluster 1, although some clusters have more than the resources listed because of some more intensive applications running on them (IBM Cloud Paks &amp; Foundational Services).</p>
<p>All clusters are running on a single z/VM 7.1 instance on a single LPAR of an IBM z14 (again, not recommended outside of POC/demo).</p>
<p>There are various other support servers running as Linux guests that you use during these labs. These are outside of the OCP cluster itself, but take care of tasks such as LDAP, NFS storage, and a server with the <code>oc</code> command line <a href="https://docs.openshift.com/container-platform/4.9/cli_reference/openshift_cli/getting-started-cli.html">installed</a> that will let you connect to the three OpenShift clusters.</p></section>
                        <h1 class='nav-section-title' id='section-presentations'>
                            Presentations <a class='headerlink' href='#section-presentations' title='Permanent link'>↵</a>
                        </h1>
                        <h1 class='nav-section-title-end'>Ended: Presentations</h1>
                        <h1 class='nav-section-title' id='section-labs'>
                            Labs <a class='headerlink' href='#section-labs' title='Permanent link'>↵</a>
                        </h1>
                        
                        <h2 class='nav-section-title' id='section-exploring-the-openshift-console'>
                            Exploring the OpenShift Console <a class='headerlink' href='#section-exploring-the-openshift-console' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab001-lab001-1"><h1 id="lab001-lab001-1-exploring-the-openshift-console">Exploring the OpenShift Console<a class="headerlink" href="#lab001-lab001-1-exploring-the-openshift-console" title="Permanent link">&para;</a></h1>
<p>The OpenShift Container Platform web console is a user interface accessible from a web browser.</p>
<p>Developers can use the web console to <a href="https://docs.openshift.com/container-platform/4.8/web_console/odc-about-developer-perspective.html">visualize, browse, and manage the contents of projects</a>.</p>
<p><img alt="openshift-console" src="../images/openshift-console.png" /></p>
<p>Administrators can use the web console to monitor the status of the applications running on the cluster, <a href="https://docs.openshift.com/container-platform/4.8/web_console/using-dashboard-to-get-cluster-information.html">along with the cluster itself</a>.</p>
<p><img alt="openshift-console-admin" src="../images/openshift-console-admin.png" /></p>
<p><a href="https://docs.openshift.com/container-platform/4.8/web_console/configuring-web-console.html">The web console can be customized</a> to suit an organization's needs, and when you log into the web console, you will only see the cluster resources that are available to you as allowed by the OpenShift Role Based Access Control (RBAC).</p>
<p>The web console runs as a group of pods on the control plane nodes in the <code>openshift-console</code> project, along with a service exposed as a route.</p></section><section class="print-page" id="lab001-lab001-2"><h1 id="lab001-lab001-2-connect-to-ocp-and-authenticate">Connect to OCP and Authenticate<a class="headerlink" href="#lab001-lab001-2-connect-to-ocp-and-authenticate" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>In your virtual machine desktop, <strong>open a Firefox web browser</strong>.</p>
</li>
<li>
<p>In the browser, <strong>navigate to the OpenShift on IBM Z console</strong> at the following address: <a href="https://console-openshift-console.apps.atsocppa.dmz/">https://console-openshift-console.apps.atsocppa.dmz/</a></p>
<details class="note" open="open"><summary>Note</summary><p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being “self-signed” and not yet recognized.</p>
<p>Accept the challenge to continue by <strong>clicking Advanced</strong> and then <strong>clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe)</strong>.</p>
<p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p>
</details>
<details><summary>Expand for screenshot</summary><p><img alt="security-accept.png" src="../images/security-accept.png" />
<img alt="security-accept-2.png" src="../images/security-accept-2.png" /></p>
</details>
<p>You will now see the OpenShift console login page.</p>
<p><img alt="openshift-console-login" src="../images/openshift-console-login.png" /></p>
</li>
<li>
<p><strong>Log in with the OpenShift credentials</strong> provided to you on the <a href="#lab-assignments">Lab Assignments</a> page.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Your OpenShift credentials will be something like the following:</p>
<ul>
<li>
<p>Username: userNN (where NN is your user number)</p>
</li>
<li>
<p>Password: p@ssw0rd</p>
</li>
</ul>
</div>
</li>
</ol></section><section class="print-page" id="lab001-lab001-3"><h1 id="lab001-lab001-3-the-administrator-perspective">The Administrator Perspective<a class="headerlink" href="#lab001-lab001-3-the-administrator-perspective" title="Permanent link">&para;</a></h1>
<p>Take a moment to notice the following elements in the navigation bar:</p>
<p><img alt="navigation-bar.png" src="../images/navigation-bar.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These buttons display on each page of the OpenShift console. Note that the Applications button might be missing from your screen, depending on your credentials.</p>
</div>
<p>By default, the menu on the left side of the page should be activated and displaying the cluster menu.</p>
<ol>
<li>
<p>In the left-side menu, <strong>select the Administrator perspective</strong> if it isn't already showing.</p>
<p><img alt="administrator-perspective.png" src="../images/administrator-perspective.png" /></p>
<p>With the administrator menu showing, you are provided with a broad range of options to manage the OpenShift cluster and the applications running on it.</p>
<p><img alt="administrator-menu.png" src="../images/administrator-menu.png" /></p>
<details class="information"><summary>Expand to Learn More About the Different Views</summary><ul>
<li>
<p><em>Developer / Administrator toggle</em>.  This lets you flip between which of the two perspectives you want to use.</p>
</li>
<li>
<p><em>Home</em>: Provides overview of projects, resources, and events in the scope of your credentials.</p>
</li>
<li>
<p><em>Operators</em>: Provides access to the OperatorHub to install new operators and also lets you view operators that are already installed.</p>
</li>
<li>
<p><em>Workloads</em>: Expands to provide access to many Kubernetes and OpenShift objects, such as pods, deployments, secrets, jobs and more.</p>
</li>
<li>
<p><em>Networking</em>: Provides access to services, routes, and ingresses required for external access to the cluster.</p>
</li>
<li>
<p><em>Storage</em>: Provides access to storage objects in the OpenShift cluster, such as PersistentVolumeClaims.</p>
</li>
<li>
<p><em>Builds</em>: View and create Build objects – use to transform input parameters into resulting objects.</p>
</li>
<li>
<p><em>Pipelines</em>: View and create Pipelines – Tekton-based CI/CD processes and objects. This will be missing if not installed in your OpenShift cluster.</p>
</li>
<li>
<p><em>Monitoring</em>: Access cluster resource Monitoring, Metrics, and Alerting.</p>
</li>
<li>
<p><em>Compute</em>: Access cluster infrastructure – Control &amp; Compute Nodes, Machines, and more.</p>
</li>
<li>
<p><em>User Management</em>: Access and manage Users, Groups, Roles, RoleBindings, Service Accounts, and more.</p>
</li>
<li>
<p><em>Administration</em>: View and edit cluster settings.</p>
</li>
</ul>
</details>
<p>The <em>Administrator</em> perspective is the default view for the OpenShift console for users who have an administrative access level. This perspective provides visibility into options related to cluster administration, as well as a broader view of the projects associated with the currently logged-in user.</p>
</li>
<li>
<p>In the Menu, <strong>click Home -&gt; Projects</strong>.</p>
<p><img alt="home-projects.png" src="../images/home-projects.png" /></p>
<p>The rest of the page is populated by projects. A project has been created for you to work in named userNN-project (where NN is your user number).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Any project starting with <em>openshift-</em> or <em>kube-</em> contain the workloads running the OpenShift platform itself.</p>
</div>
</li>
<li>
<p><strong>Click the userNN-project hyperlink</strong> (where NN is your user number).</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>With so many Projects displayed, you can use the search bar to find yours more easily.</p>
</div>
<p>You will now see the Dashboard for your project.</p>
<p><img alt="empty-openshift-project.png" src="../images/empty-openshift-project.png" /></p>
</li>
<li>
<p><strong>Scroll down the Overview tab of your project</strong>.</p>
<p>This displays information about what’s going on in your project, such as CPU and memory usage, any alerts or crashlooping pods, an inventory of all the Kubernetes resources deployed in the project, and more. You won’t see much information yet, as no workloads should be running in this project.</p>
</li>
<li>
<p><strong>Click the Workloads tab</strong> to the right of YAML.</p>
<p>This page displays all of the workloads in your project, so it’s empty for now.</p>
<details class="note" open="open"><summary>Note</summary><p>All objects in OpenShift are generated using YAML files. YAML (standing for Yet Another Markup Language) is meant to be a human-readable language for configuration files. Any OpenShift object such as Deployments, Services, Routes, and nearly everything else can be modified by directly editing their YAML file in either the console or command line.</p>
</details>
<p>Workloads are typically created by developers, so in the next section, you will swap to the developer perspective to deploy a an application. You will return to the administrator perspective later in this lab.</p>
</li>
</ol></section><section class="print-page" id="lab001-lab001-4"><h1 id="lab001-lab001-4-the-developer-perspective">The Developer Perspective<a class="headerlink" href="#lab001-lab001-4-the-developer-perspective" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>In the left-side Menu, <strong>click the Administrator dropdown, and select Developer</strong>.</p>
<p><img alt="developer-perspective.png" src="../images/developer-perspective.png" /></p>
<p>The <em>Developer</em> perspective provides views and workflows specific to developer use cases, while hiding many of the cluster management options typically used by administrators. This perspective provides developers with a streamlined view of the options they typically use.</p>
<p><img alt="developer-menu.png" src="../images/developer-menu.png" /></p>
<details class="information"><summary>Expand to Learn More About the Different Views</summary><ul>
<li>
<p><em>+Add</em>: Clicking on this will open a prompt letting you add a workload to your current project.</p>
</li>
<li>
<p><em>Topology</em>: Displays all of the deployed workloads in the currently selected project.</p>
</li>
<li>
<p><em>Monitoring</em>: Lets you view the monitoring dashboard for just this project.</p>
</li>
<li>
<p><em>Search</em>: Used to search for any type of API resource present in this project, provided you have access to that resource type.</p>
</li>
<li>
<p><em>Builds</em>: This will let you view or create Build Configurations in the currently selected project.</p>
</li>
<li>
<p><em>Pipelines</em>: View and create Pipelines – Tekton-based CI/CD processes and objects.</p>
</li>
<li>
<p><em>Helm</em>: Displays the Helm releases in this project, or prompts you to install one from the catalog if none are present.</p>
</li>
<li>
<p><em>Project</em>: Takes you to your project overview page, the project inventory, events, utilization, and more.</p>
</li>
<li>
<p><em>Config Maps</em>: Displays Config Maps for your project, which store non-confidential data in key-value pairs.</p>
</li>
<li>
<p><em>Secrets</em>: Displays Secrets for your project. Used to store sensitive, confidential data in key-value pairs, tokens, or passwords.</p>
</li>
</ul>
</details>
</li>
</ol>
<p>Switching to the Developer perspective takes you to the <em>Topology</em> view. If no workloads are deployed in the selected project, options to start building an application or visit the +Add page or are displayed.</p>
<p>If you ended up on a page other than Topology, continue with step 1 below anyways.</p>
<ol>
<li>
<p><strong>Click the +Add button in the menu</strong>.</p>
<p><img alt="add-workload-notes.png" src="../images/add-workload-notes.png" /></p>
<details class="information"><summary>Expand to learn about Deployment Methods</summary><p>There are multiple methods of deploying workloads from the OpenShift web browser.</p>
<ul>
<li><em>Samples</em>: Red Hat provides sample applications in various languages. Use these to see what a pre-made application running in OpenShift can look like.</li>
<li><em>From Git</em>: Use this option to import an existing codebase in a Git repository to create, build, and deploy an application.</li>
<li><em>From Devfile</em>: Similar to From Git, use this option to import a Devfile from your Git repository to build and deploy an application.</li>
<li><em>Container Image</em>: Use existing images from an image stream or registry to deploy it.</li>
<li><em>From Catalog</em>: Explore the Developer Catalog to select the required applications, services, or source to image builders and add it to your project.</li>
<li><em>From Dockerfile</em>: Import a dockerfile from your Git repository to build and deploy an application.</li>
<li><em>YAML</em>: Use the editor to add YAML or JSON definitions to create and modify resources.</li>
<li><em>Database</em>: Filters the Developer Catalog to display only the databases it contains.</li>
<li><em>Operator Backed</em>: Deploy applications that are managed by Operators. Many of these will come from the OperatorHub.</li>
<li><em>Helm Chart</em>: Deploy applications defined by Helm Charts, which provide simple installations, upgrades, rollbacks, and generally reduced complexity.</li>
<li><em>Pipeline</em>: Create a Tekton-based Pipeline to automate application creation and delivery using OpenShift’s built-in CI/CD capabilities.</li>
</ul>
</details>
<p>In the next section, you will deploy an application from the OpenShift Developer Catalog.</p>
</li>
</ol></section><section class="print-page" id="lab001-lab001-5"><h1 id="lab001-lab001-5-deploy-from-the-developer-catalog">Deploy from the Developer Catalog<a class="headerlink" href="#lab001-lab001-5-deploy-from-the-developer-catalog" title="Permanent link">&para;</a></h1>
<p>In this section, you will be building a sample application from a template. The template will create two pods:</p>
<ul>
<li>
<p>A Ruby on Rails blogging application from source code in GitHub</p>
</li>
<li>
<p>A PostgreSQL database from a container image</p>
</li>
</ul>
<details class="info" open="open"><summary>Info</summary><p>A <em>container image</em> holds a set of software that is ready to run, while a <em>container</em> is a running instance of a container image. Images can be hosted in registries, such as the OpenShift internal registry, the Red Hat registry, Docker Hub, or a private registry of your own.</p>
</details>
<ol>
<li>
<p><strong>Click the All Services option in the Developer Catalog section</strong> on the +Add page.</p>
<p>This brings up the OpenShift Developer catalog containing all types of applications you can deploy including Operators, Helm Charts, Templates, and more.</p>
</li>
<li>
<p><strong>Find and click the Rails + PostgreSQL (Ephemeral) tile</strong>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can search for <em>Rails + PostgreSQL (Ephemeral</em>) in the search bar.</p>
</div>
<p><img alt="rails-tile.png" src="../images/rails-tile.png" /></p>
</li>
<li>
<p><strong>Click Instantiate Template</strong> on the next screen that appears.</p>
<p>You are brought to a page full of configurable parameters that you can edit if so desired. Notice that all of the required fields on this page automatically populate. You can read through all of the options, but there is no need to edit any of them.</p>
</li>
<li>
<p><strong>Click the Create button</strong> at the bottom of the page.</p>
<p>You will now be taken to the topology view, where you will see two icons – one for each of the two workload pods that the template will create. If you don’t see the icons right away, you may need to refresh your browser window.</p>
<details class="info" open="open"><summary>Info</summary><p>The Ruby on Rails application will take a few minutes to fully deploy, while the PostgreSQL application will deploy in just a few seconds. The reason for this difference is that the Ruby application is being built (containerized) from Ruby source code located in the GitHub repository located here: <a href="https://github.com/sclorg/rails-ex.git">https://github.com/sclorg/rails-ex.git</a> into a container image, and then deployed. If you would like to watch the steps that OpenShift is taking to build the containerized application, <strong>click the circle labeled rails-postgresql-example, click the Resources tab, and click View Logs in the Builds section</strong>.</p>
<p>The PostgreSQL application, on the other hand, is deployed from a pre-built container image hosted in quay.io, so it takes much less time to start up.</p>
</details>
<p>You will know that both applications are successfully deployed and running when each icon has a solid blue circle.</p>
<p><img alt="running-pods.png" src="../images/running-pods.png" /></p>
</li>
<li>
<p><strong>Click the icon for the rails-postgresql-example application</strong>. This will bring up a window on the right side of the screen with information about your DeploymentConfig.</p>
</li>
<li>
<p><strong>Click the Details tab</strong> if it is not already selected.</p>
<p><img alt="rails-details.png" src="../images/rails-details.png" /></p>
<p>Here you’ll see information about your DeploymentConfig. Notice that many of the fields such as Labels, Update Strategy, and more have been populated with default values. These can be modified.</p>
</li>
<li>
<p><strong>Click the Actions dropdown</strong>.</p>
<p><img alt="actions-dropdown.png" src="../images/actions-dropdown.png" /></p>
<p>Many application configurations can be modified from this menu, along with other tasks such as starting or pausing a rollout, or deleting the deployment configuration.</p>
</li>
<li>
<p><strong>Click the up arrow</strong> next to the blue circle.</p>
<p><img alt="scaling-up.png" src="../images/scaling-up.png" /></p>
<p>This scales your application from one pod to two pods.</p>
<p><img alt="scaled-up.png" src="../images/scaled-up.png" /></p>
<details class="note" open="open"><summary>Note</summary><p>This is a simple demonstration of horizontal scaling with Kubernetes. You now have two instances of your pod running in the OpenShift cluster. Traffic to the Rails application will now be distributed to each pod, and if for some reason a pod is lost, that traffic will be redistributed to the remaining pods until a Kubernetes starts another. If a whole compute node is lost, Kubernetes will move the pods to different compute nodes.</p>
<p>OpenShift and Kubernetes also support autoscaling of pods based on CPU or memory consumption, but that is outside the scope of this lab.  </p>
</details>
</li>
<li>
<p><strong>Click the Resources tab</strong>.</p>
<p><img alt="resources-tab.png" src="../images/resources-tab.png" /></p>
<p>Notice the two pods associated with your Rails application. On this page, you’ll see more information about your pods, any build configurations currently running or completed, and the services/ports associated with the pod.</p>
</li>
<li>
<p><strong>Click the route address at the bottom of the resources tab</strong>.</p>
<details><summary>Expand for a Tip</summary><p>You could also access this route by clicking on the external link icon associated with your Rails pod on the Topology view.</p>
<p><img alt="external-link.png" src="../images/external-link.png" /></p>
</details>
<p><img alt="rails-dashboard.png" src="../images/rails-dashboard.png" /></p>
<p>If you see the page above, your Rails application is up and running. You just deployed a Ruby on Rails application from source code residing in GitHub, and connected it to a PostgreSQL container deployed from a container image pulled from quay.io into OpenShift running on an IBM Z server.</p>
<p>Feel free to read through the Rails application homepage to learn more about what this application can do.</p>
</li>
<li>
<p><strong>Add</strong> <code>/articles</code> <strong>to the end of the Rails homepage URL</strong>.</p>
<p>This will result in a URL like the following:</p>
<p><a href="http://rails-postgresql-example-userNN-project.apps.atsocppa.dmz/articles">http://rails-postgresql-example-userNN-project.apps.atsocppa.dmz/articles</a></p>
<p>Where <code>NN</code> is your user number.</p>
<p><img alt="listing-articles.png" src="../images/listing-articles.png" /></p>
<p>You are now interacting with the blogging application that’s shipped with the Rails source code. If you create a new article, the contents for the Title and Body are stored in the PostgreSQL database in the other pod that makes up this application.</p>
<p>In the next section you will navigate back to the Administrator perspective to see the overview of your project with a workload running.</p>
</li>
</ol></section><section class="print-page" id="lab001-lab001-6"><h1 id="lab001-lab001-6-view-workload-from-the-administrator-perspective">View Workload from the Administrator Perspective<a class="headerlink" href="#lab001-lab001-6-view-workload-from-the-administrator-perspective" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>In the left-side menu, <strong>select the Administrator perspective</strong>.</p>
<p><img alt="administrator-perspective.png" src="../images/administrator-perspective.png" /></p>
</li>
<li>
<p><strong>Navigate back to your project by clicking Menu -&gt; Home -&gt; Projects -&gt; userNN-project</strong>.</p>
<p>The overview page now displays data about the CPU and Memory Usage, new objects in your project inventory, and new activity in the events panel.</p>
<p><img alt="populated-project.png" src="../images/populated-project.png" /></p>
</li>
<li>
<p><strong>Click View Events</strong> under the right-side panel.</p>
<p><img alt="project-events.png" src="../images/project-events.png" /></p>
<p>This page is populated with all of the events associated with your project, including errors, container creation messages, pod scaling and deletion, and much more. You can filter by type, category, or by searching for keywords.</p>
<details class="note" open="open"><summary>Note</summary><p>Feel free to click through a few more pages from the left-side main menu. You’ll notice a few of them have objects created as a part of the Rails-PostgreSQL application, such as Workloads  Pods, Networking  Services and Routes, Builds  Image Streams. These were all created as part of the template package.</p>
</details>
</li>
<li>
<p><strong>Navigate back your project</strong> as in the previous step (or by clicking your browser’s back button).</p>
</li>
<li>
<p><strong>Find the Inventory on the project page</strong> which lists all of the objects created as part of your application</p>
</li>
</ol></section><section class="print-page" id="lab001-lab001-7"><h1 id="lab001-lab001-7-cleaning-up">Cleaning Up<a class="headerlink" href="#lab001-lab001-7-cleaning-up" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p><strong>Navigate back your project</strong> as in the previous section (or by clicking your browser’s back button).</p>
</li>
<li>
<p><strong>Find the Inventory on the project page</strong> which lists all of the objects created as part of your application</p>
</li>
<li>
<p><strong>Click the Deployment Configs hyperlink</strong>.</p>
<p><img alt="project-inventory.png" src="../images/project-inventory.png" /></p>
</li>
<li>
<p>For both of the 2 Deployment Configs that appear <strong>click the three dots on the right side of the screen, and then click Delete Deployment Config.</strong></p>
<p><img alt="delete-dc.png" src="../images/delete-dc.png" /></p>
<p>This will delete some, but not all of the resources created by the application template. The running pods will be stopped and deleted, but some other components will remain. This is not a problem in the case of these labs.</p>
</li>
</ol></section><h1 class='nav-section-title-end'>Ended: Exploring the OpenShift Console</h1>
                        <h2 class='nav-section-title' id='section-using-the-openshift-command-line-oc-'>
                            Using the OpenShift Command Line (oc) <a class='headerlink' href='#section-using-the-openshift-command-line-oc-' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab002-lab002-1"><h1 id="lab002-lab002-1-the-openshift-command-line-oc">The OpenShift Command Line (oc)<a class="headerlink" href="#lab002-lab002-1-the-openshift-command-line-oc" title="Permanent link">&para;</a></h1>
<p>The OpenShift command line <code>oc</code> is a command line tool that can be used to create applications and manage OpenShift projects. <code>oc</code> is ideal in situations where you:</p>
<ul>
<li>Work directly with project source code.</li>
<li>Script OpenShift Container Platform operations.</li>
<li>Are restricted by bandwidth resources and cannot use the web console.</li>
</ul>
<p>Furthermore, many people familiar with Linux and/or Kubernetes tend to find the <code>oc</code> command line an easier and more efficient method of performing tasks, rather than the web-based console.</p>
<p>Like with the OpenShift web console, the OpenShift command line includes functions both <a href="https://docs.openshift.com/container-platform/4.9/cli_reference/openshift_cli/developer-cli-commands.html">for developers</a> and for <a href="https://docs.openshift.com/container-platform/4.9/cli_reference/openshift_cli/administrator-cli-commands.html">administrators</a>.</p></section><section class="print-page" id="lab002-lab002-2"><h1 id="lab002-lab002-2-log-into-openshift-using-the-cli">Log into OpenShift Using the CLI<a class="headerlink" href="#lab002-lab002-2-log-into-openshift-using-the-cli" title="Permanent link">&para;</a></h1>
<p>In this section, you will be connecting to a “Linux Guest” server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don’t have to install it on your RHEL VM terminal.</p>
<ol>
<li>
<p><strong>Open a Terminal session</strong></p>
</li>
<li>
<p><strong>ssh into the Linux Guest server</strong>: <code>ssh userNN@192.168.176.61</code> (where NN is your user number).</p>
</li>
<li>
<p>When prompted, <strong>enter your password</strong>: <code>p@ssw0rd</code> and <strong>hit enter</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><p><img alt="ascii-wsc.png" src="../images/ascii-wsc.png" /></p>
</details>
</li>
<li>
<p>In Firefox, <strong>navigate to the following URL</strong> to request an API token:</p>
<p><a href="https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request">https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</a></p>
</li>
<li>
<p><strong>Enter your OpenShift credentials when prompted</strong>.</p>
<ul>
<li>
<p>Username: userNN</p>
</li>
<li>
<p>Password: p@ssw0rd</p>
</li>
</ul>
</li>
<li>
<p><strong>Click the “Display Token” hyperlink</strong>.</p>
<p><img alt="display-token.png" src="../images/display-token.png" /></p>
</li>
<li>
<p><strong>Copy the contents of the first text box</strong> beginning with “oc login” and ending with “6443”.</p>
<p><img alt="oc-login-token.png" src="../images/oc-login-token.png" /></p>
</li>
<li>
<p><strong>Paste this command back in your terminal session and press enter</strong>.</p>
<div class="highlight"><pre><span></span><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you’re prompted to use an insecure connection, type Y and hit enter.</p>
</div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443
Logged into &quot;https://api.atsocppa.dmz:6443&quot; as &quot;user01&quot; using the token provided.

You have access to 161 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;user01-project&quot;.
</code></pre></div>
</details>
<p>You are now logged into the cluster via the command line, and you are told which project you are using.</p>
<p>If you’re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p>
</li>
</ol></section><section class="print-page" id="lab002-lab002-3"><h1 id="lab002-lab002-3-overview-of-the-openshift-cli">Overview of the OpenShift CLI<a class="headerlink" href="#lab002-lab002-3-overview-of-the-openshift-cli" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>In your terminal, <strong>enter the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc --help
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc --help
OpenShift Client

This client helps you develop, build, deploy, and run your applications on any
OpenShift or Kubernetes cluster. It also includes the administrative
commands for managing a cluster under the &#39;adm&#39; subcommand.

Usage:
oc [flags]

Basic Commands:
login           Log in to a server
new-project     Request a new project
new-app         Create a new application
status          Show an overview of the current project
project         Switch to another project
projects        Display existing projects
explain         Documentation of resources

Build and Deploy Commands:
rollout         Manage a Kubernetes deployment or OpenShift deployment config
rollback        Revert part of an application back to a previous deployment
new-build       Create a new build configuration
start-build     Start a new build
</code></pre></div>
</details>
<p>The <code>--help</code> flag will display all of the available options the oc CLI.</p>
</li>
<li>
<p><strong>Enter the following command</strong></p>
<div class="highlight"><pre><span></span><code>oc new-app --help
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc new-app --help
Create a new application by specifying source code, templates, and/or images

This command will try to build up the components of an application using images, templates, or code
that has a public repository. It will lookup the images on the local Docker installation (if
available), a container image registry, an integrated image stream, or stored templates.

If you specify a source code URL, it will set up a build that takes your source code and converts
it into an image that can run inside of a pod. Local source must be in a git repository that has a
remote repository that the server can see. The images will be deployed via a deployment
configuration, and a service will be connected to the first public port of the app. You may either
specify components using the various existing flags or let new-app autodetect what kind of
components you have provided.

If you provide source code, a new build will be automatically triggered. You can use &#39;oc status&#39; to
check the progress.

Usage:
oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [flags]

Examples:
# List all local templates and image streams that can be used to create an app
oc new-app --list

# Create an application based on the source code in the current git repository (with a public
remote) and a Docker image
oc new-app . --docker-image=repo/langimage
</code></pre></div>
</details>
<p>The <code>--help</code> flag now displays all of the available options for the oc new-app command. If you get confused about any of the commands we use in this workshop, or just want more information, using this flag is a good first step.</p>
</li>
</ol></section><section class="print-page" id="lab002-lab002-4"><h1 id="lab002-lab002-4-deploy-container-image-from-the-cli">Deploy Container Image from the CLI<a class="headerlink" href="#lab002-lab002-4-deploy-container-image-from-the-cli" title="Permanent link">&para;</a></h1>
<p><code>oc new-app</code> is a powerful and commonly used command in the OpenShift CLI. It has the ability to deploy applications from components that include:</p>
<ul>
<li>Source or binary code</li>
<li>Container images</li>
<li>Templates</li>
</ul>
<p>The set of objects created by <code>oc new-app</code> depends on the artifacts passed as an input.</p>
<ol>
<li>
<p><strong>Run the following command to start a MongoDB deployment from a template</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="go">oc new-app --template=mongodb-ephemeral</span>
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc new-app --template=mongodb-ephemeral
--&gt; Deploying template &quot;openshift/mongodb-ephemeral&quot; to project user01-project

MongoDB (Ephemeral)
---------
MongoDB database service, without persistent storage. For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container.

WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing

The following service(s) have been created in your project: mongodb.

        Username: userFUX
        Password: AXGgm5dnKY44Byuk
Database Name: sampledb
Connection URL: mongodb://userFUX:AXGgm5dnKY44Byuk@mongodb/sampledb

For more information about using this template, including OpenShift considerations, see documentation in the upstream repository: https://github.com/sclorg/mongodb-container.

* With parameters:
    * Memory Limit=512Mi
    * Namespace=openshift
    * Database Service Name=mongodb
    * MongoDB Connection Username=userFUX # generated
    * MongoDB Connection Password=AXGgm5dnKY44Byuk # generated
    * MongoDB Database Name=sampledb
    * MongoDB Admin Password=JibwnlSwiow18owJ # generated
    * Version of MongoDB Image=3.6

--&gt; Creating resources ...
    secret &quot;mongodb&quot; created
    service &quot;mongodb&quot; created
    deploymentconfig.apps.openshift.io &quot;mongodb&quot; created
--&gt; Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
    &#39;oc expose svc/mongodb&#39; 
    Run &#39;oc status&#39; to view your app.
</code></pre></div>
</details>
<details class="note" open="open"><summary>Note</summary><p>Notice a few things:</p>
<ul>
<li>
<p>OpenShift went out and found a template that matches your desired deployment – MongoDB-ephemeral.</p>
</li>
<li>
<p>You’re told what exactly is going to be created and what it will be named.</p>
</li>
<li>
<p>Those objects are then created within your project space.</p>
</li>
<li>
<p>You’re told that the application was successfully deployed, but it is not yet exposed. This means that it’s running, but it’s not accessible from outside the cluster.</p>
</li>
</ul>
</details>
</li>
<li>
<p><strong>Run the following command</strong> to view the app in your project space:</p>
<div class="highlight"><pre><span></span><code>oc status
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc status
In project user01-project on server https://api.atsocppa.dmz:6443

svc/mongodb - 172.30.94.118:27017
dc/mongodb deploys istag/mongodb:latest 
    deployment #1 deployed 3 minutes ago - 1 pod

View details with &#39;oc describe &lt;resource&gt;/&lt;name&gt;&#39; or list everything with &#39;oc get all&#39;.
</code></pre></div>
</details>
<ol>
<li>Now <strong>run the following command</strong> to see all of the objects that were built:</li>
</ol>
<div class="highlight"><pre><span></span><code>oc get all
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc get all
NAME                 READY   STATUS      RESTARTS   AGE
pod/mongodb-1-deploy   0/1     Completed   0          5m30s
pod/mongodb-1-sj6mk    1/1     Running     0          5m22s

NAME                            DESIRED   CURRENT   READY   AGE
replicationcontroller/mongodb-1   1         1         1       5m30s

NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
service/mongodb   ClusterIP   172.30.94.118   &lt;none&gt;        27017/TCP   5m32s

NAME                                       REVISION   DESIRED   CURRENT   TRIGGERED BY
deploymentconfig.apps.openshift.io/mongodb   1          1         1       config,image(mongodb:3.6)
</code></pre></div>
</details>
<p>These are the objects that OpenShift told us would be created, and they all work together to run the application. While they’re all important pieces of this puzzle, <em>pods</em> are where the application code is actually running. Let’s narrow down on our pods.</p>
<details class="note" open="open"><summary>Note</summary><p>You might also have objects left over from other labs if they were not completely cleaned out. This is okay and the objects for different applications will not interfere with one another due to their use of <em>labels</em>.</p>
</details>
</li>
<li>
<p><strong>Run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get pods
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc get pods
NAME             READY   STATUS      RESTARTS   AGE
mongodb-1-deploy   0/1     Completed   0          28s
mongodb-1-r8dpw    1/1     Running     0          19s
</code></pre></div>
</details>
<p>The <code>oc new-app</code> command created two pods. One ending with “deploy”, and the other ending with a randomly-generated string of 5 characters (r8dpw in the screenshot above). They are both associated with your mongo deployment, but one is in a <em>Completed</em> status, and one is <em>Running</em>. The <em>Completed</em> pod had one simple job – scale the other pod to its desired count of 1.</p>
</li>
<li>
<p><strong>Run the following command</strong> to see the logs for the deploy pod</p>
<div class="highlight"><pre><span></span><code>oc logs pod/mongodb-1-deploy
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc logs pod/mongodb-1-deploy
--&gt; Scaling mongodb-1 to 1
--&gt; Success
</code></pre></div>
</details>
<p>That’s a pretty simple responsibility. The second pod, ending in the randomly generated string of characters, has a much more complicated job. This is the pod where the MongoDB application code is actually running.</p>
</li>
<li>
<p><strong>Run the following command</strong> to see the logs for the MongoDB deployment:</p>
<div class="highlight"><pre><span></span><code>oc logs pod/mongodb-1-XXXXX
</code></pre></div>
<p>Where <code>XXXXX</code> is your unique string of characters that you saw in the <code>oc get pods</code> output.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc logs pod/mongodb-1-r8dpw
2020-04-15T16:56:12.344+0000 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols &#39;none&#39;
2020-04-15T16:56:12.346+0000 W  ASIO     [main] No TransportLayer configured during NetworkInterface startup
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=mongo-1-r8dpw
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten] db version v4.2.5
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten] git version: 2261279b51ea13df08ae708ff278f0679c59dc32
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten] OpenSSL version: OpenSSL 1.1.1  11 Sep 2018
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten] allocator: tcmalloc
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten] modules: none
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten] build environment:
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten]     distmod: ubuntu1804
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten]     distarch: s390x
2020-04-15T16:56:12.351+0000 I  CONTROL  [initandlisten]     target_arch: s390x
</code></pre></div>
</details>
<p>This is obviously a much busier pod. One of the first lines in its log tells you which version of MongoDB is running.</p>
</li>
</ol>
<p>In the next section, you will connect to the pod and see that it is actually running MongoDB.</p></section><section class="print-page" id="lab002-lab002-5"><h1 id="lab002-lab002-5-open-a-remote-shell-session-into-the-mongodb-pod">Open a Remote Shell Session into the MongoDB Pod<a class="headerlink" href="#lab002-lab002-5-open-a-remote-shell-session-into-the-mongodb-pod" title="Permanent link">&para;</a></h1>
<p>OpenShift provides <em>Remote Shell</em> capabilities from both the command line and from the web console.</p>
<p>With the <code>oc rsh</code> command, you can issue commands as if you are inside the container and perform local operations like monitoring, debugging, and using CLI commands specific to what is running in the container.</p>
<details class="information" open="open"><summary>Information</summary><p>For example, if you open a remote shell session into a MySQL container, you can count the number of records in the database by invoking the mysql command, then using the prompt to type in the SELECT command. You can also use commands like ps(1) and ls(1) for validation.</p>
</details>
<p>With the MongoDB application you deployed, you can <code>rsh</code> into the MongoDB pod to run <code>mongo</code> CLI commands.</p>
<ol>
<li>
<p><strong>Enter the following command</strong> to rsh into the container:</p>
<div class="highlight"><pre><span></span><code>oc rsh mongo-XXXXX
</code></pre></div>
<p>Where <code>XXXXX</code> is your unique string of 5 characters</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc rsh mongodb-1-r8dpw
$ 
</code></pre></div>
</details>
<p>This new line that <em>does not</em> start with <code>userNN@lab061</code> indicates that you are now in the remote shell session for the pod</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you wait too long to interact with the remote shell (about a minute), it will automatically time-out and you will have to re-connect. You can tell that this happened if the prompt reappears.</p>
</div>
</li>
<li>
<p>In the remote session, <strong>issue the command</strong>:</p>
<div class="highlight"><pre><span></span><code>mongo
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>$ mongo
MongoDB shell version v4.2.5
connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodb
Implicit session: session { &quot;id&quot; : UUID(&quot;2320e01b-168e-41d0-a132-af0c9243d29c&quot;) }
MongoDB server version: 4.2.5
Welcome to the MongoDB shell.
For interactive help, type &quot;help&quot;.
For more comprehensive documentation, see
    http://docs.mongodb.org/
Questions? Try the support group
    http://groups.google.com/group/mongodb-user
</code></pre></div>
</details>
<p><code>mongo</code> is the shell command for MongoDB. Issuing the <code>mongo</code> command without any options or flags connects you to a MongoDB instance running on your localhost with port 27017. If you see this message, MongoDB is up and running in the container.</p>
</li>
<li>
<p><strong>Exit the MongoDB shell by entering the command</strong>:</p>
<div class="highlight"><pre><span></span><code>exit
</code></pre></div>
</li>
<li>
<p><strong>Exit the remote shell session by entering, once again</strong>:</p>
<div class="highlight"><pre><span></span><code>exit
</code></pre></div>
<p>You should be back in the <code>userNN@lab061</code> command line.</p>
</li>
</ol></section><section class="print-page" id="lab002-lab002-6"><h1 id="lab002-lab002-6-working-with-pods">Working with Pods<a class="headerlink" href="#lab002-lab002-6-working-with-pods" title="Permanent link">&para;</a></h1>
<p>One of the main benefits of using containers and Kubernetes-based cloud platforms like OpenShift is the ability to <em>scale horizontally</em> – rapidly duplicating or deleting pods to meet a desired state.</p>
<details class="information" open="open"><summary>Information</summary><p>One of the core concepts of Kubernetes is the <em>Declarative State</em>. Users <em>declare</em> what resources they want, and Kubernetes does whatever it can to make that happen. Scaling is one example of this.</p>
</details>
<p>Scaling essentially creates copies of the application in order to distribute traffic to multiple instances and/or compute nodes for high availability and load balancing.</p>
<ol>
<li>
<p><strong>Enter the following command</strong> to get the name of your MongoDB deploymentconfig (dc)</p>
<div class="highlight"><pre><span></span><code>oc get dc
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc get dc
NAME    REVISION   DESIRED   CURRENT   TRIGGERED BY
mongodb   1          1         1         config,image(mongodb:3.6)
</code></pre></div>
</details>
<p>Your deploymentconfig named <code>mongo</code> has a count desired = current = 1.</p>
</li>
<li>
<p><strong>Scale the mongo deployment to 3 replicas</strong>:</p>
<div class="highlight"><pre><span></span><code>oc scale dc/mongodb --replicas=3
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc scale dc/mongodb --replicas=3
deploymentconfig.apps.openshift.io/mongodb scaled
</code></pre></div>
</details>
</li>
<li>
<p><strong>Enter the following command again</strong> to see the scaled application.</p>
<div class="highlight"><pre><span></span><code>oc get dc
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc get dc
NAME    REVISION   DESIRED   CURRENT   TRIGGERED BY
mongodb   1          3         3         config,image(mongodb:3.6)
</code></pre></div>
</details>
<p>This output is telling you that OpenShift knows that you want three copies (pods) of MongoDB, and it is successfully meeting that declared state.</p>
</li>
<li>
<p><strong>Enter the following command again to see your three pods</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get pods
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc get pods
NAME             READY   STATUS      RESTARTS   AGE
mongodb-1-5nmjn    1/1     Running     0          2m6s
mongodb-1-deploy   0/1     Completed   0          20m
mongodb-1-dh49x    1/1     Running     0          2m6s
mongodb-1-r8dpw    1/1     Running     0          19m
</code></pre></div>
</details>
<p>Two of the pods will have a shorter age than the original one – these are the two new pods that were just created when you scaled the application.</p>
</li>
<li>
<p><strong>Dig into the pods a little bit further by entering the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc describe pod/mongodb-1-XXXXX
</code></pre></div>
<p>Where <code>XXXXX</code> is one of your unique strings of characters.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc describe pod/mongodb-1-5nmjn
Name:               mongodb-1-5nmjn
Namespace:          user01-project
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               worker-0.atsocppa.dmz/192.168.176.175
Start Time:         Wed, 15 Apr 2020 13:13:53 -0400
Labels:             app=mongodb
                    deployment=mongodb-1
                    deploymentconfig=mongodb
</code></pre></div>
</details>
<p>This command gives you all kinds of information about your pod. Notice the <code>Node:</code> field that begins with <code>worker-#</code>.</p>
</li>
<li>
<p><strong>Run the same command again, but on a different pod this time</strong>:</p>
<div class="highlight"><pre><span></span><code>oc describe pod/mongodb-1-YYYYY
</code></pre></div>
<p>Where <code>YYYYY</code> is one of your other unique strings of characters. Pick one different than the previous step.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc describe pod/mongodb-1-r8dpw
Name:               mongodb-1-r8dpw
Namespace:          user01-project
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               worker-2.atsocppa.dmz/192.168.176.177
Start Time:         Wed, 15 Apr 2020 12:56:03 -0400
Labels:             app=mongodb
                    deployment=mongodb-1
                    deploymentconfig=mongodb
</code></pre></div>
</details>
<p>It is likely (but not guaranteed) that this pod has been placed on a <em>different</em> compute node than the first pod you described. The reason for this is that you have three compute nodes in this OpenShift cluster, and Kubernetes balances the load for this application across multiple nodes.</p>
</li>
</ol></section><section class="print-page" id="lab002-lab002-7"><h1 id="lab002-lab002-7-administrative-cli-commands">Administrative CLI Commands<a class="headerlink" href="#lab002-lab002-7-administrative-cli-commands" title="Permanent link">&para;</a></h1>
<p>If you’ve already completed <a href="#lab001-lab001-1">Exploring the OpenShift Console</a>, you’ll remember that there are both developer and administrator perspectives. The same is true in the OpenShift CLI. </p>
<p>The <code>oc adm</code> command gives cluster administrators the ability to check logs, manage users, groups, policies, certificates, and many other tasks usually associated with administrative roles.</p>
<ol>
<li>
<p><strong>Issue the following command to see all of the OpenShift administrator commands</strong>.</p>
<div class="highlight"><pre><span></span><code>oc adm --help
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc adm --help
Administrative Commands

Actions for administering an OpenShift cluster are exposed here.

Usage:
oc adm [flags]

Cluster Management:
upgrade                            Upgrade a cluster
top                                Show usage statistics of resources on the server
must-gather                        Launch a new instance of a pod for gathering debug information

Node Management:
drain                              Drain node in preparation for maintenance
cordon                             Mark node as unschedulable
uncordon                           Mark node as schedulable
taint                              Update the taints on one or more nodes
node-logs                          Display and filter node logs

Security and Policy:
new-project                        Create a new project
policy                             Manage cluster authorization and security policy
groups                             Manage groups
certificate                        Approve or reject certificate requests
pod-network                        Manage pod network

Maintenance:
prune                              Remove older versions of resources from the server
migrate                            Migrate data in the cluster

Configuration:
create-kubeconfig                  Create a basic .kubeconfig file from client certs
create-bootstrap-project-template  Create a bootstrap project template
create-login-template              Create a login template
create-provider-selection-template Create a provider selection template
create-error-template              Create an error page template

Other Commands:
build-chain                        Output the inputs and dependencies of your builds
completion                         Output shell completion code for the specified shell (text or zsh)
config                             Change configuration files for the client
verify-image-signature             Verify the image identity contained in the image signature
</code></pre></div>
</details>
<details class="note" open="open"><summary>Note</summary><p>Your userNN credential has the privileges required to run some, but not all of these commands.</p>
</details>
</li>
<li>
<p><strong>Run the following administrative command to show see usage statistics for pods in your project</strong>.</p>
<div class="highlight"><pre><span></span><code>oc adm top pods
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc adm top pods
NAME            CPU(cores)   MEMORY(bytes)   
mongodb-1-5nmjn   3m           83Mi            
mongodb-1-dh49x   3m           83Mi            
mongodb-1-r8dpw   3m           85Mi
</code></pre></div>
</details>
<p>As OpenShift clusters grow in production, administrative commands like this one become more and more essential to keep everything running smoothly.</p>
</li>
</ol></section><section class="print-page" id="lab002-lab002-8"><h1 id="lab002-lab002-8-cleaning-up">Cleaning Up<a class="headerlink" href="#lab002-lab002-8-cleaning-up" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p><strong>Double check that you are in your own userNN-project by issuing the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc project
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc project
Using project &quot;user01-project&quot; on server &quot;https://api.atsocppa.dmz:6443&quot;.
</code></pre></div>
</details>
</li>
<li>
<p><strong>Once you’re sure you’re in your own project, issue the following command to delete all objects associated with your application labeled mongodb-ephemeral</strong>.  </p>
<div class="highlight"><pre><span></span><code>oc delete all --selector app=mongodb-ephemeral -o name
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc delete all --selector app=mongodb-ephemeral -o name
replicationcontroller/mongodb-1
service/mongodb
deploymentconfig.apps.openshift.io/mongodb
</code></pre></div>
</details>
</li>
<li>
<p><strong>Run the following command to check that all of your mongo application resources were deleted</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get all
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc get all
No resources found.
user00@lab061:~$
</code></pre></div>
</details>
</li>
<li>
<p>(<strong><em>Optional</em></strong>) <strong>If there are leftover resources from other labs that you would like to delete, run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc delete all --all
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc delete all --all
pod &quot;rails-postgresql-example-1-build&quot; deleted
service &quot;postgresql&quot; deleted
service &quot;rails-postgresql-example&quot; deleted
buildconfig.build.openshift.io &quot;rails-postgresql-example&quot; deleted
build.build.openshift.io &quot;rails-postgresql-example-1&quot; deleted
imagestream.image.openshift.io &quot;rails-postgresql-example&quot; deleted
route.route.openshift.io &quot;rails-postgresql-example&quot; deleted
</code></pre></div>
</details>
</li>
</ol></section><h1 class='nav-section-title-end'>Ended: Using the OpenShift Command Line (oc)</h1>
                        <h2 class='nav-section-title' id='section-deploying-an-application-from-source-code'>
                            Deploying an Application from Source Code <a class='headerlink' href='#section-deploying-an-application-from-source-code' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab004-lab004-1"><h1 id="lab004-lab004-1-deploying-an-application-from-source-code">Deploying an Application from Source Code<a class="headerlink" href="#lab004-lab004-1-deploying-an-application-from-source-code" title="Permanent link">&para;</a></h1>
<p>OpenShift is designed for users with various responsibilities, backgrounds and skillsets. Most broadly, OpenShift is designed for two main groups – <em>administrators</em> and <em>developers</em>. Furthermore, there are different types of administrators, and different types of developers.</p>
<p>As much of the Information Technology world moves toward cloud technology as the consumption model for enterprise computing, developers are required to make a shift in the tools they use to perform their work. At the heart of almost every cloud platform there are two of these new, core technologies – <em>containers</em> and <em>container orchestrators</em>.</p>
<details class="note" open="open"><summary>Note</summary><p>We won’t be specifically covering these technologies in this lab, but you’ve probably heard of them. Docker is the most popular container runtime, and Kubernetes is the most popular container orchestrator. For the curious, OpenShift replaced Docker containers with CRI-O containers when moving from v3.11 to v4.1 (although Docker containers will still work in OpenShift 4.x).</p>
</details>
<p>However, not every developer wants (or needs) to learn these new technologies in order to take advantage of them. In fact, OpenShift enables developers with no container experience at all to simply provide their source code (written in Javascript, Python, Go, etc.) and let OpenShift build the container for them using its <strong><em>Source-to-Image (S2I) capability</em></strong>.</p>
<p><img alt="s2i-diagram" src="../images/s2i-diagram.png" /></p>
<p>OpenShift's S2I capability allows developers to focus on developing their application and leaves the containerization process to OpenShift. Using the S2I tooling and Builder Images loaded into the OpenShift image registry, the developer does not need to create a Dockerfile, use any podman or docker commands, or do anything else that is usually required to make a container image out of application source code.</p></section><section class="print-page" id="lab004-lab004-2"><h1 id="lab004-lab004-2-exploring-github-and-the-example-health-source-code">Exploring GitHub and the Example Health Source Code<a class="headerlink" href="#lab004-lab004-2-exploring-github-and-the-example-health-source-code" title="Permanent link">&para;</a></h1>
<p>In this lab, you will be deploying a sample healthcare application called <em>Example Health</em>. The application is written in JavaScript, and it’s loaded into IBM’s GitHub repository.</p>
<ol>
<li>
<p>In Firefox, <strong>navigate to <a href="https://github.com/IBM/node-s2i-openshift">https://github.com/IBM/node-s2i-openshift</a></strong></p>
<p><img alt="ibm-repo" src="../images/ibm-repo.png" /></p>
<p>This is an IBM repository that contains everything you need in order to deploy the application – including a <code>README.md</code> file with information and instructions, additional files required for the source code to work, and the source code itself. Let’s look at the source code now.</p>
</li>
<li>
<p><strong>Open the site folder</strong>.</p>
<p><img alt="site-dir" src="../images/site-dir.png" /></p>
</li>
<li>
<p><strong>Open the app.js file</strong>.</p>
<p><img alt="app-js" src="../images/app-js.png" /></p>
<p>This is the source code for the frontend application that OpenShift will build into a container. Notice that it is NOT any sort of container image, Dockerfile, or YAML file itself – rather, it is <em>written in Javascript</em>. Feel free to look through the code.</p>
</li>
<li>
<p><strong>Click on the node-s2i-openshift hyperlink</strong> to get back to the main repository page.</p>
<p>Your URL should again be <a href="https://github.com/IBM/node-s2i-openshift">https://github.com/IBM/node-s2i-openshift</a></p>
<p>You’ll need to make a fork of this repository so you have your own copy to work with. To do so, you’ll first need to sign into GitHub.</p>
</li>
<li>
<p><strong>Click the Sign In button in the top right</strong>.</p>
<p><img alt="gh-sign-in" src="../images/gh-sign-in.png" /></p>
</li>
<li>
<p><strong>Log in with YOUR OWN GitHub credentials</strong>.</p>
<details class="note" open="open"><summary>Note</summary><p>If you don’t have a GitHub account already, please create one and then sign in with it.</p>
</details>
<p>After a successful login, you will be taken back to the main repository page. Now you can create your own fork of the repository.</p>
</li>
<li>
<p><strong>Click the Fork button on the left side of the page</strong>.</p>
<p><img alt="gh-fork" src="../images/gh-fork.png" /></p>
<p>When complete, you will be taken to your forked repository page</p>
<p>Notice that while everything else looks basically  the same, the URL has changed from <a href="https://github.com/IBM/node-s2i-openshift">https://github.com/IBM/node-s2i-openshift</a></p>
<p>to:</p>
<p><a href="https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift">https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift</a></p>
</li>
<li>
<p><strong>Leave this browser tab open, and open another</strong>.</p>
</li>
</ol></section><section class="print-page" id="lab004-lab004-3"><h1 id="lab004-lab004-3-connect-to-ocp-and-authenticate">Connect to OCP and Authenticate<a class="headerlink" href="#lab004-lab004-3-connect-to-ocp-and-authenticate" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>In your virtual machine desktop, <strong>open a Firefox web browser</strong>.</p>
</li>
<li>
<p>In the browser, <strong>navigate to the OpenShift on IBM Z console</strong> at the following address: <a href="https://console-openshift-console.apps.atsocppa.dmz/">https://console-openshift-console.apps.atsocppa.dmz/</a>.</p>
<details class="note" open="open"><summary>Note</summary><p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being “self-signed” and not yet recognized.</p>
<p>Accept the challenge to continue by <strong>clicking Advanced</strong> and then <strong>clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe)</strong>.</p>
<p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p>
</details>
<details><summary>Expand for screenshot</summary><p><img alt="security-accept.png" src="../images/security-accept.png" />
<img alt="security-accept-2.png" src="../images/security-accept-2.png" /></p>
</details>
<p>You will now see the OpenShift console login page.</p>
<p><img alt="openshift-console-login" src="../images/openshift-console-login.png" /></p>
</li>
<li>
<p>Log in with the OpenShift credentials provided to you on the <a href="#lab-assignments">Lab Assignments</a> page.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Your OpenShift credentials will be something like the following:</p>
<ul>
<li>
<p>Username: userNN (where NN is your user number)</p>
</li>
<li>
<p>Password: p@ssw0rd</p>
</li>
</ul>
</div>
</li>
</ol></section><section class="print-page" id="lab004-lab004-4"><h1 id="lab004-lab004-4-edit-the-source-code-and-push-an-update">Edit the Source Code and Push an Update<a class="headerlink" href="#lab004-lab004-4-edit-the-source-code-and-push-an-update" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p><strong>Switch to the Developer Perspective</strong>, if not already on it.</p>
<p><img alt="developer-perspective" src="../images/developer-perspective.png" /></p>
</li>
<li>
<p><strong>Change into your userNN-project</strong> if not already in it.</p>
<p><img alt="userNN-project" src="../images/userNN-project.png" /></p>
</li>
<li>
<p><strong>Click the +Add button from the left-side menu</strong>.</p>
<p><img alt="add-workload" src="../images/add-workload.png" /></p>
</li>
<li>
<p><strong>Click the From Git option in the Git Repository section</strong> of the +Add page.</p>
<p><img alt="from-git" src="../images/from-git.png" /></p>
</li>
<li>
<p><strong>In the Git Repo URL field, enter the URL of your forked repository</strong>.</p>
<p>It will look something like: <a href="https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift">https://github.com/YOUR_GITHUB_USERNAME/node-s2i-openshift</a></p>
<p><img alt="import-from-git" src="../images/import-from-git.png" /></p>
</li>
<li>
<p><strong>Click the Show Advanced Git Options hyperlink</strong>.</p>
</li>
<li>
<p><strong>In the Context Dir field, enter <code>/site</code></strong>.</p>
<p><img alt="show-advanced" src="../images/show-advanced.png" /></p>
<details class="note" open="open"><summary>Recall</summary><p>This is the folder in the GitHub repository that you dug into to view the source code.</p>
</details>
</li>
<li>
<p><strong>For Builder Image, select the Node.js tile</strong>. It is likely that OpenShift will detect the programming language in the GitHub repository and automatically select the Node.js builder image.</p>
<p><img alt="node-tile" src="../images/node-tile.png" /></p>
</li>
<li>
<p><strong>Scroll to the bottom of this page and click the Create button</strong>.</p>
<p>You will be taken to the Topology page, which will show your new application along with three smaller circular buttons that can be used to perform different actions against the application.</p>
</li>
<li>
<p><strong>Click the circular Node.js application icon</strong>.</p>
<p><img alt="node-icon" src="../images/node-icon.png" /></p>
<p>At first, the icon will be all white and you will see “Build #1 is running” in the righthand panel. If you wish, you can watch the logs for the running Build to see everything it’s doing. After a minute or two, the icon will have a green check mark next to it, indicating the Build is complete. Once the Build is complete, your Pod will be created. You can also watch the logs for this, if you wish. About 10 seconds later, a solid blue ring will appear around the edge of the circular icon, indicating that the pod is up and running.</p>
<p><img alt="node-panel" src="../images/node-panel.png" /></p>
<details class="note" open="open"><summary>Note</summary><p>Feel free to click on the <em>View Logs</em> hyperlink to watch everything that the build is creating. When complete, the log will display <code>Push Successful</code>, and you can return to the Topology page by clicking the link on the left side of the page.</p>
</details>
<p>If you clicked off of the Node.js application Resources tab, click on the circular icon again, then click on the Resources tab.</p>
</li>
<li>
<p><strong>Click the Route URL</strong> – beginning with http://</p>
<p><img alt="route-url" src="../images/route-url.png" /></p>
<p>You will be taken to the Example Health application.</p>
<p><img alt="example-health-login" src="../images/example-health-login.png" /></p>
</li>
<li>
<p><strong>Log into the Example Health application using the following credentials</strong>:</p>
<ul>
<li>
<p>Username: <code>test</code></p>
</li>
<li>
<p>Password: <code>test</code></p>
</li>
</ul>
<p><img alt="example-health-patient" src="../images/example-health-patient.png" /></p>
<p>All of the data in this application is simulated to look similar to the health records of an insurance company. Feel free to explore the application and notice the multiple tabs it contains.</p>
<p>The JavaScript code you looked at in your forked GitHub repository was containerized by OpenShift’s S2I function and deployed onto the OpenShift cluster.</p>
<p>Now that your application is running in a container in OpenShift, let’s see how an application developer can make a change to the source code, and then push the update to the running application.</p>
<p>We’ll make a simple change in a few lines of text to demonstrate. As we see in the Example Health application, there is a section with Personal Information.</p>
<p><img alt="personal-info" src="../images/personal-info.png" /></p>
</li>
<li>
<p><strong>In your forked GitHub repository, navigate to the source code again</strong>:</p>
</li>
<li>
<p><strong>Make sure you are in your own fork of the repository</strong>.</p>
</li>
<li>
<p>From <https://github.com/\<YOUR_GITHUB_USERNAME>/node-s2i-openshift>, <strong>click on the site folder</strong>.</p>
</li>
<li>
<p><strong>Click on the app.js file</strong>.</p>
</li>
<li>
<p>With the app.js file open, <strong>click on the edit button</strong> pointed out in the picture below.</p>
<p><img alt="gh-edit-button" src="../images/gh-edit-button.png" /></p>
</li>
<li>
<p><strong>Scroll down to line 55</strong>, which displays the patient name.</p>
</li>
<li>
<p><strong>Edit lines 55-60 as you wish, modifying the text strings for Name, Age, Gender, etc</strong>.</p>
<p><img alt="edited-gh" src="../images/edited-gh.png" /></p>
</li>
<li>
<p><strong>Click Commit Changes</strong> at the bottom of the page.</p>
</li>
</ol>
<p>You just edited the source code, but you still need to push the update to the running application.</p>
<ol>
<li>
<p><strong>Back in the OpenShift console, navigate to the Topology page -&gt; Click the Node.js application icon -&gt; Click the Resources tab -&gt; Click the Start Build button</strong>.</p>
<p><img alt="start-build" src="../images/start-build.png" /></p>
<p>A new Build #2 will be created. As with the first build, you can view the build logs to watch everything it’s doing, or you can simply wait for the console to display <code>Build #2 is complete</code> and your Pod <code>Running</code>.</p>
</li>
<li>
<p>When the Pod is <code>running</code>, <strong>refresh the Example Health browser tab</strong>.</p>
<p><img alt="patient-info-edited" src="../images/patient-info-edited.png" /></p>
</li>
</ol>
<p>Your code changes have been pushed to the running Example Health application.</p></section><section class="print-page" id="lab004-lab004-5"><h1 id="lab004-lab004-5-cleaning-up">Cleaning Up<a class="headerlink" href="#lab004-lab004-5-cleaning-up" title="Permanent link">&para;</a></h1>
<p>There is no easy way to delete all of these objects from the OpenShift console. This is a much easier task in the OpenShift command line.</p>
<ol>
<li>
<p>In the OpenShift CLI, <strong>make sure you are in your own project</strong> (i.e. userNN-project) <strong>and run the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc delete all --selector app=node-s-2-i-openshift -o name
</code></pre></div>
<details class="note" open="open"><summary>Note</summary><p>If you are not connected to the OpenShift command line, refer to <a href="#lab002-lab002-1">Using the OpenShift Command Line</a>.</p>
</details>
</li>
</ol>
<p>In this lab, you have exposed JavaScript source code in a GitHub repository to an OpenShift cluster, which containerized that JavaScript code into a container image, and then deployed it as a container running in a pod. You then made a code change to the JavaScript code and pushed an update to the application while it was running</p></section><h1 class='nav-section-title-end'>Ended: Deploying an Application from Source Code</h1>
                        <h2 class='nav-section-title' id='section-openshift-pipelines'>
                            OpenShift Pipelines <a class='headerlink' href='#section-openshift-pipelines' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab009-lab009-0"><h1 id="lab009-lab009-0-using-openshift-pipelines">Using OpenShift Pipelines<a class="headerlink" href="#lab009-lab009-0-using-openshift-pipelines" title="Permanent link">&para;</a></h1>
<p><img alt="openshift-pipelines-graphic.svg" src="../images/openshift-pipelines-graphic.svg" /></p>
<p><strong>Red Hat OpenShift Pipelines</strong> is a cloud-native, continuous integration and continuous delivery (CI/CD) solution based on Kubernetes resources. It uses Tekton building blocks to automate deployments across multiple platforms by abstracting away the underlying implementation details. Tekton introduces a number of standard custom resource definitions (CRDs) for defining CI/CD pipelines that are portable across Kubernetes distributions.</p>
<h2 id="lab009-lab009-0-key-features">Key features<a class="headerlink" href="#lab009-lab009-0-key-features" title="Permanent link">&para;</a></h2>
<ul>
<li>Red Hat OpenShift Pipelines is a <em>serverless</em> CI/CD system that runs pipelines with all the required dependencies in <em>isolated containers</em>.</li>
<li>Red Hat OpenShift Pipelines are designed for <em>decentralized</em> teams that work on microservice-based architecture.</li>
<li>Red Hat OpenShift Pipelines use <em>standard CI/CD pipeline definitions</em> that are easy to extend and integrate with the existing Kubernetes tools, enabling you to scale on-demand.</li>
<li>You can use Red Hat OpenShift Pipelines to build images with Kubernetes tools such as Source-to-Image (S2I), Buildah, Buildpacks, and Kaniko that are portable across any Kubernetes platform.</li>
<li>You can use the OpenShift Container Platform Developer console to create Tekton resources, view logs of pipeline runs, and manage pipelines in your OpenShift Container Platform namespaces.</li>
</ul>
<h2 id="lab009-lab009-0-what-is-tekton">What is Tekton?<a class="headerlink" href="#lab009-lab009-0-what-is-tekton" title="Permanent link">&para;</a></h2>
<p><img alt="Logo-tekton.svg" src="../images/Logo-tekton.svg" /></p>
<p><strong>Tekton</strong> is an <em>open source</em> project that provides a framework to create <em>cloud-native CI/CD pipelines</em> quickly. As a <em>Kubernetes-native</em> framework, Tekton makes it easier to deploy across multiple cloud providers or hybrid environments. By leveraging the Custom Resource Definitions (CRDs) in Kubernetes, Tekton uses the Kubernetes control plane to run pipeline tasks. By using standard industry specifications, Tekton will work well with existing CI/CD tools such as Jenkins, Jenkins X, Skaffold, Knative, and now OpenShift.</p>
<p>Source of images and information on this page: <a href="https://cloud.redhat.com/learn/topics/ci-cd">https://cloud.redhat.com/learn/topics/ci-cd</a></p></section><section class="print-page" id="lab009-lab009-1"><h1 id="lab009-lab009-1-using-openshift-pipelines">Using OpenShift Pipelines<a class="headerlink" href="#lab009-lab009-1-using-openshift-pipelines" title="Permanent link">&para;</a></h1>
<p>In this section, you will be connecting to a “Linux Guest” server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don’t have to install it on your RHEL VM terminal.</p>
<ol>
<li>
<p><strong>Open a Terminal session</strong>.</p>
</li>
<li>
<p><strong>ssh into the Linux Guest server</strong>:</p>
<div class="highlight"><pre><span></span><code>ssh userNN@192.168.176.61
</code></pre></div>
<p>Where <code>NN</code> is your user number.</p>
</li>
<li>
<p>When prompted, <strong>enter your password:</strong> <code>p@ssw0rd</code> <strong>and hit enter</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><p><img alt="ascii-wsc.png" src="../images/ascii-wsc.png" /></p>
</details>
</li>
<li>
<p>In Firefox, <strong>navigate to the following URL</strong> to request an API token:</p>
<p><a href="https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request">https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</a></p>
</li>
<li>
<p><strong>Enter your OpenShift credentials when prompted</strong>.</p>
<ul>
<li>
<p>Username: <code>userNN</code></p>
</li>
<li>
<p>Password: <code>p@ssw0rd</code></p>
</li>
</ul>
</li>
<li>
<p><strong>Click the “Display Token” hyperlink</strong>.</p>
<p><img alt="display-token.png" src="../images/display-token.png" /></p>
</li>
<li>
<p><strong>Copy the contents of the first text box</strong> beginning with “oc login” and ending with “6443”.</p>
<p><img alt="oc-login-token.png" src="../images/oc-login-token.png" /></p>
</li>
<li>
<p><strong>Paste this command back in your terminal session and press enter</strong>.</p>
<div class="highlight"><pre><span></span><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you’re prompted to use an insecure connection, type Y and hit enter.</p>
</div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443
Logged into &quot;https://api.atsocppa.dmz:6443&quot; as &quot;user01&quot; using the token provided.

You have access to 161 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;user01-project&quot;.
</code></pre></div>
</details>
<p>You are now logged into the cluster via the command line, and you are told which project you are using.</p>
<p>If you’re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p>
</li>
</ol></section><section class="print-page" id="lab009-lab009-2"><h1 id="lab009-lab009-2-cloning-the-github-repository-and-viewing-its-contents">Cloning the GitHub Repository and Viewing its Contents<a class="headerlink" href="#lab009-lab009-2-cloning-the-github-repository-and-viewing-its-contents" title="Permanent link">&para;</a></h1>
<p>In the terminal session, you should have been automatically placed in your home directory <code>/home/userNN</code> (where NN is your user number).</p>
<ol>
<li>
<p><strong>Run the command</strong> <code>pwd</code> <strong>to check your current working directory</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ pwd
/home/user01
</code></pre></div>
</details>
</li>
<li>
<p>If you are in any other directory, <strong>change into the correct home directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd /home/userNN
</code></pre></div>
<p>Where <code>NN</code> is your user number).</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ cd /home/user01
user01@lab061:~$ pwd
/home/user01
</code></pre></div>
</details>
<ol>
<li>In your home directory, <strong>clone the OpenShift Pipelines repository using the command</strong>:</li>
</ol>
<div class="highlight"><pre><span></span><code>git clone https://github.com/mmondics/openshift-pipelines-s390x 
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ git clone https://github.com/mmondics/openshift-pipelines-s390x
Cloning into &#39;openshift-pipelines-s390x&#39;...
remote: Enumerating objects: 25, done.
remote: Counting objects: 100% (25/25), done.
remote: Compressing objects: 100% (21/21), done.
remote: Total 25 (delta 5), reused 0 (delta 0), pack-reused 0
Unpacking objects: 100% (25/25), done.
Checking connectivity... done
</code></pre></div>
</details>
</li>
<li>
<p>This will create a new directory called <code>openliberty-pipelines-s390x</code>. <strong>Change into this directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd openshift-pipelines-s390x
</code></pre></div>
</li>
<li>
<p><strong>List its contents using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>ls -l
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ cd openliberty-operator-ocpz
user01@lab061:~/openliberty-operator-ocpz$ ls -l
total 16
-rw-r--r-- 1 user00 users   48 Mar 16 14:20 README.md
drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 pipeline
-rw-r--r-- 1 user00 users  251 Mar 22 13:23 pipeline-cleanup.sh
drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 resources
drwxr-xr-x 2 user00 users 4096 Mar 16 14:20 tasks
</code></pre></div>
</details>
<p>If you navigate to the GitHub repository in a web browser <a href="https://github.com/mmondics/openshift-pipelines-s390x">https://github.com/mmondics/openshift-pipelines-s390x</a>, you will notice that the sub-directories in your Linux session reflect the folders contained in the repository.</p>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>README.MD</strong></td>
<td>Contains   the content displayed on the GitHub page for this repository. You can read through   this README file if you want to get more information about this lab.</td>
</tr>
<tr>
<td><strong>pipeline</strong></td>
<td>Directory   containing the YAML file that will be used to create a Pipeline.</td>
</tr>
<tr>
<td><strong>pipeline-cleanup.sh</strong></td>
<td>Shell   script that will delete all objects created in this lab.</td>
</tr>
<tr>
<td><strong>resources</strong></td>
<td>Directory   containing the YAML file that will create a PersistentVolumeClaim in   the cluster.</td>
</tr>
<tr>
<td><strong>tasks</strong></td>
<td>Directory   containing YAML files to create various Tasks that make up a pipeline.</td>
</tr>
</tbody>
</table>
</li>
</ol></section><section class="print-page" id="lab009-lab009-3"><h1 id="lab009-lab009-3-understanding-and-deploying-tasks">Understanding and Deploying Tasks<a class="headerlink" href="#lab009-lab009-3-understanding-and-deploying-tasks" title="Permanent link">&para;</a></h1>
<p>A <em>Task</em> defines a series of steps that run in a desired order and complete a set amount of build work. Every Task runs as a Pod on your OpenShift cluster with each step as its own container. Tasks have one single responsibility so they can be reused across different Pipelines or in multiple places within a single Pipeline.</p>
<p>The repository you pulled includes the YAML files needed to create three Tasks. Let’s take a look at one of them.</p>
<ol>
<li>
<p><strong>From the openshift-pipelines-s390x directory, run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat tasks/hello.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ cat tasks/hello.yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
name: hello
spec:
steps:
    - name: say-hello
    image: registry.access.redhat.com/ubi8/ubi
    command:
        - /bin/text
    args: [&#39;-c&#39;, &#39;echo Hello World&#39;]
</code></pre></div>
</details>
<p>This file will create a Kubernetes Task object called hello that is made up of one step. That step has its own name, image, command, and args associated with it. As explained above, once created, this Task will create one Pod that includes one Container.</p>
</li>
<li>
<p><strong>Create the Task using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc create -f tasks/hello.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/hello.yaml
task.tekton.dev/hello created
</code></pre></div>
</details>
<p>The Task is now created in your project and can be run using Tekton, the CI/CD tool that OpenShift Pipelines are based on.</p>
</li>
<li>
<p><strong>Run the</strong> <code>hello</code> <strong>task using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>tkn task start --showlog hello
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ tkn task start --showlog hello
TaskRun started: hello-run-xvr92
Waiting for logs to be available...
[say-hello] Hello World
</code></pre></div>
</details>
<p>Running the <code>tkn task start</code> command created a new Kubernetes resource called a TaskRun. TaskRuns are automatically created for each Task that is run in a Pipeline, but as you can see, they can also be manually created by running a Task. This can be useful for debugging a single Task in a Pipeline.</p>
</li>
<li>
<p>Your Pipeline will consist of three Tasks total. <strong>Create the remaining Tasks using the commands</strong>:</p>
<div class="highlight"><pre><span></span><code>oc create -f tasks/apply_manifest_task.yaml
</code></pre></div>
<p>and</p>
<div class="highlight"><pre><span></span><code>oc create -f tasks/update_deployment_task.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/apply_manifest_task.yaml
task.tekton.dev/apply-manifests created
user01@lab061:~/openshift-pipelines-s390x$ oc create -f tasks/update_deployment_task.yaml
task.tekton.dev/update-deployment created
</code></pre></div>
</details>
</li>
<li>
<p><strong>You have now created three Tasks that will be plumbed together to create a Pipeline. To see them, run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>tkn task ls
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ tkn task ls
NAME                DESCRIPTION   AGE
apply-manifests                   7 minutes ago
hello                             8 minutes ago
update-deployment                 7 minutes ago
</code></pre></div>
</details>
<p>You will also need a <em>Workspace</em> in which your will run all of the Tasks associated with your Pipeline. This will be a shared space across each Task, TaskRun, Pipeline, and PipelineRun that you associate with the Workspace. With a Workspace, you can store Task inputs and outputs, share data among Tasks, use it as a mount point for credentials held in Secrets, create a cache of build artifacts that speed up jobs, and more.</p>
</li>
<li>
<p><strong>In our case, we will be using a PersistentVolumeClaim as our Workspace. Create the PVC using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc create -f resources/persistent_volume_claim.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ oc create -f resources/persistent_volume_claim.yaml
persistentvolumeclaim/source-pvc created
</code></pre></div>
</details>
</li>
</ol>
<p>In the next section, you will create a Pipeline that uses the Tasks and Workspace you just created to pull the source code of an application from GitHub and then builds and deploys it in a container on OpenShift.</p></section><section class="print-page" id="lab009-lab009-4"><h1 id="lab009-lab009-4-understanding-and-deploying-pipelines">Understanding and Deploying Pipelines<a class="headerlink" href="#lab009-lab009-4-understanding-and-deploying-pipelines" title="Permanent link">&para;</a></h1>
<p>A <em>Pipeline</em> consists of a series of Tasks that are executed to construct complex workflows that automate the build, deployment, and delivery of applications. It is a collection of PipelineResources, parameters, and one or more Tasks.</p>
<p>Below is a diagram of the Pipeline you will be creating.</p>
<p><img alt="pipeline-diagram" src="../images/pipeline-diagram.png" /></p>
<p>The repository you pulled provides the YAML file necessary to generate this Pipeline.</p>
<ol>
<li>
<p><strong>Take a look at the YAML by using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat pipeline/pipeline.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ cat tasks/hello.yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
name: build-and-deploy
spec:
workspaces:
- name: shared-workspace
params:
- name: deployment-name
    type: string
    description: name of the deployment to be patched
- name: git-url
    type: string
    description: url of the git repo for the code of deployment
- name: git-revision
    type: string
    description: revision to be used from repo of the code for deployment
    default: &quot;master&quot;
- name: IMAGE
    type: string
    description: image to be build from the code
tasks:
- name: fetch-repository
    taskRef:
    name: git-clone
    kind: ClusterTask
    workspaces:
    - name: output
    workspace: shared-workspace
    params:
    - name: url
    value: $(params.git-url)
    - name: subdirectory
    value: &quot;&quot;
    - name: deleteExisting
    value: &quot;true&quot;
    - name: revision
    value: $(params.git-revision)
- name: build-image
    taskRef:
    name: buildah
    kind: ClusterTask
    params:
    - name: TLSVERIFY
    value: &quot;false&quot;
    - name: IMAGE
    value: $(params.IMAGE)
    workspaces:
    - name: source
    workspace: shared-workspace
    runAfter:
    - fetch-repository
- name: apply-manifests
    taskRef:
    name: apply-manifests
    workspaces:
    - name: source
    workspace: shared-workspace
    runAfter:
    - build-image
- name: update-deployment
    taskRef:
    name: update-deployment
    params:
    - name: deployment
    value: $(params.deployment-name)
    - name: IMAGE
    value: $(params.IMAGE)
    runAfter:
    - apply-manifests
</code></pre></div>
</details>
<p>The Tasks included in this pipeline and their responsibilities are as follows:</p>
<ul>
<li>
<p><em>fetch-repository</em> clones the source code of the application from a GitHub repository based on the git-url and git-revision parameters.</p>
</li>
<li>
<p><em>build-image</em> builds the container image of the application using Buildah.</p>
</li>
<li>
<p><em>apply-manifests</em> deploys the application to OpenShift by running the oc apply command on the new container image with the provided parameters.</p>
</li>
<li>
<p><em>update-deployment</em> will update the application in OpenShift with the oc patch command when changes are needed.</p>
</li>
</ul>
<p>You will notice that there are no references to the GitHub repository or the image registry that will be pushed to in the pipeline. This is because Pipelines are designed to be generic and re-used in different situations or to deploy different applications. Pipelines abstract away the specific parameters that can be passed into the Pipeline. When triggering the Pipeline, you will provide different GitHub repositories and images to be used when executed.</p>
<p>Also notice that the execution order of Tasks can be determined by dependencies defined between Tasks via inputs and outputs, or explicitly ordered via runAfter.</p>
</li>
<li>
<p><strong>Create the Pipeline with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc create -f pipeline/pipeline.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ oc create -f pipeline/pipeline.yaml
pipeline.tekton.dev/build-and-deploy created
</code></pre></div>
</details>
<p>Although we are using pre-built YAML files to simplify the creation of these resources, everything in this lab could also be done in the OpenShift console in a browser.</p>
</li>
<li>
<p><strong>Take a look at the graphical representation of your Pipeline by accessing the cluster at the URL: <a href="https://console-openshift-console.apps.atsocppa.dmz/">https://console-openshift-console.apps.atsocppa.dmz/</a></strong></p>
<p>Username: userNN (where NN is your user number)
Password: p@ssw0rd</p>
</li>
<li>
<p><strong>Navigate to the Developer Perspective -&gt; Pipelines -&gt; select your userNN Project</strong>.</p>
<p><img alt="dev-pipeline" src="../images/dev-pipeline.png" /></p>
</li>
<li>
<p><strong>Click your new Pipeline</strong> called <code>build-and-deploy</code>.</p>
<p><img alt="build-and-deploy" src="../images/build-and-deploy.png" /></p>
<p>The framework of your Pipeline has been created, and you can see the four Tasks that make up your Pipeline.</p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>If you remember making the apply-manifests and update-deployment Tasks, but not the “fetch-repository” and “build-image” Tasks -- you aren’t wrong. These are ClusterTasks that come pre-built into OpenShift.</p>
</div>
</li>
</ol>
<p>In the next section you will trigger a PipelineRun to execute your Pipeline and the Tasks it contains.</p></section><section class="print-page" id="lab009-lab009-5"><h1 id="lab009-lab009-5-running-the-pipeline">Running the Pipeline<a class="headerlink" href="#lab009-lab009-5-running-the-pipeline" title="Permanent link">&para;</a></h1>
<p>Let’s use this Pipeline to create an application. To demonstrate the re-usability of OpenShift Pipelines, we will be creating both a frontend and a backend with the same Pipeline you created in the previous step.</p>
<p>We’ll also demonstrate the flexibility provided by OpenShift Pipelines that lets you use them from either the web console or the command line.</p>
<p>Let’s create the backend application with the Tekton CLI in your terminal.</p>
<p>Now that you have all of the building blocks in place, you can start the Pipeline with the following command. The command will run the Pipeline and pass in parameters to:</p>
<ul>
<li>
<p>Use the shared workspace and the PersistentVolumeClaim you created</p>
</li>
<li>
<p>Create the deployment named vote-api</p>
</li>
<li>
<p>Build a container image from the source code at the given GitHub repository</p>
</li>
<li>
<p>Push that container image into the OpenShift internal registry and tag it for your project</p>
</li>
<li>
<p>Show the log so you can follow its progress
Note that the forward slash  simply breaks the command into multiple lines for readability.</p>
</li>
</ul>
<h2 id="lab009-lab009-5-creating-the-backend-application-through-the-cli">Creating the Backend Application through the CLI<a class="headerlink" href="#lab009-lab009-5-creating-the-backend-application-through-the-cli" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p><strong>Run the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>tkn pipeline start build-and-deploy \
 -w name=shared-workspace,claimName=source-pvc \
 -p deployment-name=vote-api \
 -p git-url=https://github.com/mmondics/pipelines-vote-api.git \
 -p IMAGE=image-registry.openshift-image-registry.svc:5000/userNN-project/pipelines-vote-api --showlog 
</code></pre></div>
<div class="admonition important">
<p class="admonition-title"><img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /> Important <img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /></p>
<p>Make sure you change the one instance of <code>NN</code> to your team number in the command above.</p>
</div>
<details class="example"><summary>Expand for Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ tkn pipeline start build-and-deploy \
&gt; -w name=shared-workspace,claimName=source-pvc \
&gt; -p deployment-name=vote-api \
&gt; -p git-url=https://github.com/mmondics/vote-api.git \
&gt; -p IMAGE=image-registry.openshift-image-registry.svc:5000/userNN-project/vote-api \
&gt; --showlog
PipelineRun started: build-and-deploy-run-75zqv
Waiting for logs to be available...
[fetch-respository : clone] + CHECKOUT_DIR=/workspace/output/
[fetch-respository : clone] + [[ true == \t\r\u\e ]]
[fetch-respository : clone] + cleandir
[fetch-respository : clone] + [[ -d /workspace/output/ ]]
[fetch-respository : clone] + rm -rf /workspace/output//Dockerfile /workspace/output//LICENSE /workspace/output//README.md /workspace/output//go.mod /workspace/output//go.sum /workspace/output//k8s /workspace/output//main.go /workspace/output//vendor
[fetch-respository : clone] + rm -rf /workspace/output//.git /workspace/output//.gitignore
[fetch-respository : clone] + rm -rf &#39;/workspace/output//..?*&#39;
[fetch-respository : clone] + test -z &#39;&#39;
[fetch-respository : clone] + test -z &#39;&#39;
[fetch-respository : clone] + test -z &#39;&#39;
[fetch-respository : clone] + /ko-app/git-init -url https://github.com/mmondics/vote-api.git -revision master -refspec &#39;&#39; -path /workspace/output/ -sslVerify=true -submodules=true -depth 1
[fetch-respository : clone] {&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1616101272.5251348,&quot;caller&quot;:&quot;git/git.go:165&quot;,&quot;msg&quot;:&quot;Successfully cloned https://github.com/mmondics/vote-api.git @ a08f579f6135293358b9423a3370e725ae1380cc (grafted, HEAD, origin/master) in path /workspace/output/&quot;}
[fetch-respository : clone] {&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1616101272.6701891,&quot;caller&quot;:&quot;git/git.go:203&quot;,&quot;msg&quot;:&quot;Successfully initialized and updated submodules in path /workspace/output/&quot;}
[fetch-respository : clone] + cd /workspace/output/
[fetch-respository : clone] ++ git rev-parse HEAD
[fetch-respository : clone] + RESULT_SHA=a08f579f6135293358b9423a3370e725ae1380cc
[fetch-respository : clone] + EXIT_CODE=0
[fetch-respository : clone] + &#39;[&#39; 0 &#39;!=&#39; 0 &#39;]&#39;
[fetch-respository : clone] + echo -n a08f579f6135293358b9423a3370e725ae1380cc
[fetch-respository : clone] + echo -n https://github.com/mmondics/vote-api.git

[build-image : build] + buildah --storage-driver=vfs bud --format=oci --tls-verify=false --no-cache -f ./Dockerfile -t image-registry.openshift-image-registry.svc:5000/user00-project/vote-api .
[build-image : build] STEP 1: FROM image-registry.openshift-image-registry.svc:5000/openshift/golang:latest AS builder
[build-image : build] Getting image source signatures
[build-image : build] Copying blob sha256:ff637d5a66cba4903fc7d9343b0f9dbb4e1bf8ada19bd3934ea0edfb85dc4
[build-image : build] Copying blob sha256:f7fb0662b957bcb1b5007f9b5502af4da4c13e17b7bc2eff4f02c3e5ec08e
[build-image : build] Copying blob sha256:35aab756d1a095511ab75eeca5aa77a37fa62a258f3fa5bcfb37ad604e369
[build-image : build] Copying blob sha256:7cc70ce0e0ee7fe5f8ea22894ad8c2f962f1dfdd00d05de91a32181c89179
[build-image : build] Copying blob sha256:73986f838dc404255946f6aa282b0aeabc420faa4f21b572e1de735498edf
[build-image : build] Copying config sha256:9e8f033b036bdb224dc931cfcaaf532da6a6ae7d779e8a09c52eed12305
[build-image : build] Writing manifest to image destination
[build-image : build] Storing signatures
[build-image : build] STEP 2: WORKDIR /build
[build-image : build] STEP 3: ADD . /build/
[build-image : build] STEP 4: RUN export GARCH=&quot;$(uname -m)&quot; &amp;&amp; if [[ ${GARCH} == &quot;s390x&quot; ]]; then export GARCH=&quot;s390x&quot;; fi &amp;&amp; GOOS=linux GOARCH=${GARCH} CGO_ENABLED=0 go build -mod=vendor -o api-server 
[build-image : build] STEP 5: FROM scratch
[build-image : build] STEP 6: WORKDIR /app
[build-image : build] STEP 7: COPY --from=builder /build/api-server /app/api-server
[build-image : build] STEP 8: CMD [ &quot;/app/api-server&quot; ]
[build-image : build] STEP 9: COMMIT image-registry.openshift-image-registry.svc:5000/user00-project/vote-api
[build-image : build] --&gt; 36faca61f94
[build-image : build] 36faca61f941af886128abd8792753095eaac7c1041084e222f426243ed50ecc

[build-image : push] + buildah --storage-driver=vfs push --tls-verify=false --digestfile /workspace/source/image-digest image-registry.openshift-image-registry.svc:5000/user00-project/vote-api docker://image-registry.openshift-image-registry.svc:5000/user00-project/vote-api
[build-image : push] + buildah --storage-driver=vfs push --tls-verify=false --digestfile /workspace/source/image-digest image-registry.openshift-image-registry.svc:5000/user00-project/vote-api docker://image-registry.openshift-image-registry.svc:5000/user00-project/vote-api
[build-image : push] Getting image source signatures
[build-image : push] Copying blob sha256:9eda1116f7414b98e397f94cc650fd50890c2d97fa47925e02b83df7726119
[build-image : push] Copying config sha256:36faca61f941af886128abd8792753095eaac7c1041084e222f426243ed5
[build-image : push] Writing manifest to image destination
[build-image : push] Storing signatures

[build-image : digest-to-results] + cat /workspace/source/image-digest
[build-image : digest-to-results] + tee /tekton/results/IMAGE_DIGEST
[build-image : digest-to-results] sha256:a7d730f92530c2f10891c55ba86a44e4fcc907436831c99733779ffb0d0fe8

[apply-manifests : apply] Applying manifests in k8s directory
[apply-manifests : apply] deployment.apps &quot;vote-api&quot; created
[apply-manifests : apply] service &quot;vote-api&quot; created
[apply-manifests : apply] -----------------------------------

[update-deployment : patch] deployment.apps &quot;vote-api&quot; patched
</code></pre></div>
</details>
<p>If you see the final <code>deployment.apps “vote-api” patched</code> line, your PipelineRun was successful and your backend application is now deployed in OpenShift.</p>
</li>
<li>
<p><strong>Look at your running application Pod by issuing the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get pod
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ oc get pod
NAME                                                           READY   STATUS      RESTARTS   AGE
build-and-deploy-run-sgtc7-apply-manifests-9p6mv-pod-95jhw     0/1     Completed   0          9m52s
build-and-deploy-run-sgtc7-build-image-6kh6n-pod-pkvgx         0/3     Completed   0          12m
build-and-deploy-run-sgtc7-fetch-repository-flxfx-pod-p6nzh    0/1     Completed   0          13m
build-and-deploy-run-sgtc7-update-deployment-hgxrz-pod-htqpf   0/1     Completed   0          9m33s
vote-api-6765569bfb-v4jlh                                      1/1     Running     0          9m20s
</code></pre></div>
</details>
<p>You should see one <em>running</em> Pod and four <em>completed</em> Pods. The running Pod is your application that the Pipeline pulled from GitHub, containerized, pushed into the internal OpenShift repository, and started. The completed Pods were created to complete the Tasks defined in the Pipeline, and each is made up of one container per step in the Task.</p>
<p>Looking at the READY column, you can see that most of the Pods have one container, with the exception of the build-image Pod that has three.</p>
</li>
<li>
<p><strong>The Tekton CLI also provides a way to check your Pipelines and PipelineRuns by running</strong>:</p>
<div class="highlight"><pre><span></span><code>tkn pipeline ls
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ tkn pipeline ls
NAME               AGE             LAST RUN                     STARTED         DURATION    STATUS
build-and-deploy   4 minutes ago   build-and-deploy-run-2q5fp   4 minutes ago   3 minutes   Succeeded
</code></pre></div>
</details>
</li>
</ol>
<p>Since we have successfully run the Pipeline in the CLI, let’s trigger a run from the console in the next section.</p>
<h2 id="lab009-lab009-5-creating-the-frontend-application-through-the-console">Creating the Frontend Application through the Console<a class="headerlink" href="#lab009-lab009-5-creating-the-frontend-application-through-the-console" title="Permanent link">&para;</a></h2>
<p>Let’s create the frontend portion of our application by running the Pipeline from the OpenShift console.</p>
<ol>
<li>
<p><strong>If you’ve closed out of the OpenShift console in your web browser, go back to <a href="https://console-openshift-console.apps.atsocppa.dmz/">https://console-openshift-console.apps.atsocppa.dmz/</a></strong></p>
</li>
<li>
<p><strong>Navigate to the Developer Perspective -&gt; Pipelines -&gt; and select your userNN Project</strong>.</p>
<p><img alt="build-and-deploy-2" src="../images/build-and-deploy-2.png" /></p>
<p>The main Pipelines page displays the same information returned from the <code>tkn pipeline ls</code> command.</p>
</li>
<li>
<p><strong>Click your build-and-deploy Pipeline and then click the Actions -&gt; Start button</strong>.</p>
<p><img alt="start-pipeline" src="../images/start-pipeline.png" /></p>
<p>This will open a new window that prompts you for the parameters with which to start your second PipelineRun. This window is the GUI equivalent to the multi-line <code>tkn pipeline start</code> command that we entered in the CLI PipelineRun.</p>
</li>
<li>
<p><strong>Enter the following parameters</strong>:</p>
<ul>
<li>
<p>deployment name: <code>vote-ui</code></p>
</li>
<li>
<p>git-url: <code>https://github.com/mmondics/pipelines-vote-ui.git</code></p>
</li>
<li>
<p>git-revision: <code>master</code></p>
</li>
<li>
<p>IMAGE: <code>image-registry.openshift-image-registry.svc:5000/userNN-project/vote-ui</code></p>
</li>
<li>
<p>shared-workspace: <code>PVC</code> -&gt; <code>source-pvc</code></p>
</li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Make sure you change the one instance of <code>NN</code> in the IMAGE field to your user number.</p>
</div>
</li>
<li>
<p><strong>Then click start</strong>.</p>
<p><img alt="start-pipeline-ui" src="../images/start-pipeline-ui.png" /></p>
<p>You will be taken to the page for your PipelineRun and shown the graphical representation of the running Pipeline.</p>
<p><img alt="pipeline-running" src="../images/pipeline-running.png" /></p>
</li>
<li>
<p><strong>Click the logs tab</strong> to follow what’s happening in more detail like you saw in the CLI.</p>
<p><img alt="pipeline-logs" src="../images/pipeline-running.png" /></p>
</li>
</ol>
<p>When you see the PipelineRun has Succeeded and the <code>deployment.apps “vote-ui” has been patched</code>, your frontend application is also up and running.</p>
<p>With both your backend and frontend applications are running, in the next section we’ll access it in a browser.</p></section><section class="print-page" id="lab009-lab009-6"><h1 id="lab009-lab009-6-accessing-the-pipeline-in-a-browser">Accessing the Pipeline in a Browser<a class="headerlink" href="#lab009-lab009-6-accessing-the-pipeline-in-a-browser" title="Permanent link">&para;</a></h1>
<p>Your application is accessible via its route.</p>
<ol>
<li>
<p>In the OpenShift console, <strong>navigate to the Topology page in the Developer Perspective and make sure you’re in your userNN-project</strong>.</p>
<p><img alt="pipeline-app-topology" src="../images/pipeline-app-topology.png" /></p>
<p>You should see two Icons with solid blue bars indicating your application pods are running without error.</p>
</li>
<li>
<p>On the vote-ui icon, <strong>click the button in the top right corner to navigate to the application’s exposed route</strong>.</p>
<p><img alt="pipeline-app-topology-2" src="../images/pipeline-app-topology-2.png" /></p>
<p>This will open a new browser tab for your frontend application UI.</p>
<p><img alt="cat-dog" src="../images/cat-dog.png" /></p>
</li>
<li>
<p><strong>Click the box for your desired option</strong>.</p>
<p>By casting your vote with the vote-ui frontend, you are invoking a REST API call and sending a POST request that is stored in the vote-api backend application.</p>
<p>You can see this POST request reflected in the vote-api Pod logs.</p>
</li>
<li>
<p><strong>In your terminal session find the name of your vote-api Pod using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get pods | grep Running
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ oc get pods | grep Running
vote-api-6765569bfb-p2bhh                                      1/1     Running     0          65m
vote-ui-6846f88f6f-rzzgt                                       1/1     Running     0          18m
</code></pre></div>
</details>
</li>
<li>
<p><strong>Copy the full name for your vote-api Pod and view its logs with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc logs pod/vote-api-XXXXXXXXXX-YYYYY 
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Your randomly-generated Pod names will differ.</p>
</div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ oc logs pod/vote-api-6765569bfb-p2bhh
[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.

[GIN-debug] [WARNING] Running in &quot;debug&quot; mode. Switch to &quot;release&quot; mode in production.
- using env:    export GIN_MODE=release
- using code:   gin.SetMode(gin.ReleaseMode)

[GIN-debug] GET    /vote                     --&gt; main.setupRouter.func1 (3 handlers)
[GIN-debug] POST   /vote                     --&gt; main.setupRouter.func2 (3 handlers)
[GIN-debug] Listening and serving HTTP on :9000
[GIN] 2021/03/22 - 16:18:18 | 200 |     179.658µs |    10.131.1.157 | POST     /vote
[GIN] 2021/03/22 - 16:18:48 | 200 |     107.379µs |    10.131.1.157 | POST     /vote
</code></pre></div>
</details>
<p>You can see your POST requests at the <code>/vote</code> endpoint at the bottom, and more detail is stored in NFS by the PersistentVolumeClaim you created earlier.</p>
</li>
</ol>
<p>In this lab, you have:</p>
<ul>
<li>Created Tasks that have specific responsibilities in the building and deploying of a containerized application onto an OpenShift on IBM Z cluster</li>
<li>Created a Pipeline that combines these Tasks to one end-to-end process</li>
<li>Ran the Pipeline twice -- once from the command line, and once from the OpenShift console -- to create a backend and a frontend application.</li>
<li>Used the created applications to invoke a REST API call that is stored persistently in NFS storage.</li>
</ul></section><section class="print-page" id="lab009-lab009-7"><h1 id="lab009-lab009-7-cleaning-up">Cleaning Up<a class="headerlink" href="#lab009-lab009-7-cleaning-up" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>When you’re ready to finish the lab and delete the objects you created, <strong>return to your terminal and double check that you’re in your own userNN-project with</strong>:</p>
<div class="highlight"><pre><span></span><code>oc project
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ oc project
Using project &quot;user00-project&quot; on server &quot;https://api.atsocpd2.dmz:6443&quot;.
</code></pre></div>
</details>
</li>
<li>
<p><strong>Run the uninstall script</strong>:</p>
<div class="highlight"><pre><span></span><code>./pipeline-cleanup.sh
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-pipelines-s390x$ ./pipeline-cleanup.sh
Running oc delete all --all
pod &quot;build-and-deploy-run-6hgg7-apply-manifests-ksns5-pod-zvnq8&quot; deleted
pod &quot;build-and-deploy-run-6hgg7-build-image-4bstq-pod-sd2xv&quot; deleted
pod &quot;build-and-deploy-run-6hgg7-fetch-repository-jgqml-pod-ld4nj&quot; deleted
pod &quot;build-and-deploy-run-6hgg7-update-deployment-qhmxh-pod-c56gj&quot; deleted
pod &quot;build-and-deploy-run-fnx5s-apply-manifests-k8v7f-pod-88fpm&quot; deleted
pod &quot;build-and-deploy-run-fnx5s-build-image-4xknh-pod-kq4zk&quot; deleted
pod &quot;build-and-deploy-run-fnx5s-fetch-repository-m5pmr-pod-8m5gt&quot; deleted
pod &quot;build-and-deploy-run-fnx5s-update-deployment-fgq9s-pod-2s4vw&quot; deleted
pod &quot;hello-run-l2skb-pod-vzvrj&quot; deleted
pod &quot;vote-api-6765569bfb-p59vj&quot; deleted
pod &quot;vote-ui-6846f88f6f-z7zp9&quot; deleted
service &quot;vote-api&quot; deleted
service &quot;vote-ui&quot; deleted
deployment.apps &quot;vote-api&quot; deleted
deployment.apps &quot;vote-ui&quot; deleted
replicaset.apps &quot;vote-ui-566848fff4&quot; deleted
replicaset.apps &quot;vote-ui-6846f88f6f&quot; deleted
imagestream.image.openshift.io &quot;vote-api&quot; deleted
imagestream.image.openshift.io &quot;vote-ui&quot; deleted
route.route.openshift.io &quot;vote-ui&quot; deleted
Deleting Pipeline &amp; resources
pipeline.tekton.dev &quot;build-and-deploy&quot; deleted
pipelinerun.tekton.dev &quot;build-and-deploy-run-6hgg7&quot; deleted
pipelinerun.tekton.dev &quot;build-and-deploy-run-fnx5s&quot; deleted
task.tekton.dev &quot;apply-manifests&quot; deleted
task.tekton.dev &quot;hello&quot; deleted
task.tekton.dev &quot;update-deployment&quot; deleted
taskrun.tekton.dev &quot;hello-run-l2skb&quot; deleted
Deleting PVC
persistentvolumeclaim &quot;source-pvc&quot; deleted
Removing Images
</code></pre></div>
</details>
</li>
</ol></section><h1 class='nav-section-title-end'>Ended: OpenShift Pipelines</h1>
                        <h2 class='nav-section-title' id='section-openshift-service-mesh'>
                            OpenShift Service Mesh <a class='headerlink' href='#section-openshift-service-mesh' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab010-lab010-1"><h1 id="lab010-lab010-1-openshift-service-mesh">OpenShift Service Mesh<a class="headerlink" href="#lab010-lab010-1-openshift-service-mesh" title="Permanent link">&para;</a></h1>
<p><em>Red Hat OpenShift Service Mesh</em> (OSSM) provides a platform for behavioral insight and operational control over your networked microservices in a service mesh. With OSSM, you can connect, secure, and monitor microservices in your OpenShift Container Platform environment.</p>
<p>A <em>Service Mesh</em> is the network of microservices that make up applications and the interactions between those microservices. When a Service Mesh grows in size and complexity, it can become harder to understand and manage. Take, for example, an application made up of 5 microservices that is managed from the OpenShift Console and/or Command Line.</p>
<p><img alt="microservice-no-ossm" src="../images/microservice-no-ossm.png" /></p>
<p>Based on the open source <a href="https://istio.io/">Istio</a> project, OSSM adds a transparent layer on existing microservice applications without requiring any changes to the application code. You add OSSM support to services by deploying a sidecar proxy to relevant microservices in the mesh that intercepts all network communication between microservices. See the figure below.</p>
<p><img alt="microservice-ossm" src="../images/microservice-ossm.png" /></p>
<p>OpenShift Service Mesh gives you an easy way to create a network of deployed services that provide:</p>
<ul>
<li>Discovery</li>
<li>Load balancing</li>
<li>Service-to-service authentication</li>
<li>Failure recovery</li>
<li>Metrics</li>
<li>Monitoring</li>
</ul>
<p>OpenShift Service Mesh also provides more complex operational functions including:</p>
<ul>
<li>A/B testing</li>
<li>Canary releases</li>
<li>Rate limiting</li>
<li>Access control</li>
<li>End-to-end authentication</li>
</ul>
<p>In this lab, we will be exploring many of the OSSM features above.</p></section><section class="print-page" id="lab010-lab010-2"><h1 id="lab010-lab010-2-openshift-service-mesh-architecture">OpenShift Service Mesh Architecture<a class="headerlink" href="#lab010-lab010-2-openshift-service-mesh-architecture" title="Permanent link">&para;</a></h1>
<p><img alt="ossm-arch" src="../images/ossm-arch.png" /></p>
<p>OpenShift Service Mesh is logically split into a <em>data plane</em> and a <em>control plane</em>:</p>
<p>The <em>data plane</em> is a set of intelligent proxies deployed as sidecars. These proxies intercept and control all inbound and outbound network communication between microservices in the service mesh.</p>
<ul>
<li>Envoy proxy intercepts all inbound and outbound traffic for all services in the service mesh. Envoy is deployed as a sidecar to the relevant service in the same pod.</li>
</ul>
<p>The <em>control plane</em> manages and configures <em>Istiod</em> to enforce proxies to route traffic.</p>
<p><em>Istiod</em> provides service discovery, configuration and certificate management. It converts high-level routing rules to Envoy configurations and propagates them to the sidecars at runtime.</p></section><section class="print-page" id="lab010-lab010-3"><h1 id="lab010-lab010-3-log-into-openshift-using-the-cli">Log into OpenShift Using the CLI<a class="headerlink" href="#lab010-lab010-3-log-into-openshift-using-the-cli" title="Permanent link">&para;</a></h1>
<p>In this section, you will be connecting to a “Linux Guest” server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don’t have to install it on your RHEL VM terminal.</p>
<ol>
<li>
<p><strong>Open a Terminal session</strong></p>
</li>
<li>
<p><strong>ssh into the Linux Guest server</strong>:</p>
<div class="highlight"><pre><span></span><code>ssh userNN@192.168.176.61
</code></pre></div>
<p>Where <code>NN</code> is your user number.</p>
</li>
<li>
<p>When prompted, <strong>enter your password:</strong> <code>p@ssw0rd</code> <strong>and hit enter</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><p><img alt="ascii-wsc.png" src="../images/ascii-wsc.png" /></p>
</details>
</li>
<li>
<p>In Firefox, <strong>navigate to the following URL</strong> to request an API token:</p>
<p><a href="https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request">https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</a></p>
</li>
<li>
<p><strong>Enter your OpenShift credentials when prompted</strong>.</p>
<ul>
<li>
<p>Username: <code>userNN</code></p>
</li>
<li>
<p>Password: <code>p@ssw0rd</code></p>
</li>
</ul>
</li>
<li>
<p><strong>Click the “Display Token” hyperlink</strong>.</p>
<p><img alt="display-token.png" src="../images/display-token.png" /></p>
</li>
<li>
<p><strong>Copy the contents of the first text box</strong> beginning with “oc login” and ending with “6443”.</p>
<p><img alt="oc-login-token.png" src="../images/oc-login-token.png" /></p>
</li>
<li>
<p><strong>Paste this command back in your terminal session and press enter</strong>.</p>
<div class="highlight"><pre><span></span><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you’re prompted to use an insecure connection, type Y and hit enter.</p>
</div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443
Logged into &quot;https://api.atsocppa.dmz:6443&quot; as &quot;user01&quot; using the token provided.

You have access to 161 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;user01-project&quot;.
</code></pre></div>
</details>
<p>You are now logged into the cluster via the command line, and you are told which project you are using.</p>
<p>If you’re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p>
</li>
</ol></section><section class="print-page" id="lab010-lab010-4"><h1 id="lab010-lab010-4-cloning-the-github-repository-and-reviewing-its-contents">Cloning the GitHub Repository and Reviewing its Contents<a class="headerlink" href="#lab010-lab010-4-cloning-the-github-repository-and-reviewing-its-contents" title="Permanent link">&para;</a></h1>
<p>In the terminal session, you should have been automatically placed in your home directory <code>/home/userNN</code> (where NN is your user number).</p>
<ol>
<li>
<p><strong>Run the command</strong> <code>pwd</code> <strong>to check your current working directory</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ pwd
/home/user01
</code></pre></div>
</details>
</li>
<li>
<p>If you are in any other directory, <strong>change into the correct home directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd /home/userNN
</code></pre></div>
<p>Where NN is your user number.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ cd /home/user01
user01@lab061:~$ pwd
/home/user01
</code></pre></div>
</details>
</li>
<li>
<p><strong>In your home directory, clone the OpenShift Service Mesh repository using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>git clone https://github.com/mmondics/istio-s390x -b ocp-wildfire
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ git clone https://github.com/mmondics/istio-s390x -b ocp-wildfire
Cloning into &#39;istio-s390x&#39;...
remote: Enumerating objects: 171, done.
remote: Counting objects: 100% (171/171), done.
remote: Compressing objects: 100% (98/98), done.
remote: Total 171 (delta 61), reused 159 (delta 52), pack-reused 0
Receiving objects: 100% (171/171), 332.76 KiB | 0 bytes/s, done.
Resolving deltas: 100% (61/61), done.
Checking connectivity... done.
</code></pre></div>
</details>
</li>
<li>
<p>This will create a new directory called istio-s390x. <strong>Change into this directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd istio-s390x 
</code></pre></div>
</li>
<li>
<p><strong>Then list its contents using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>ls -l
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ cd Istio-s390x
user01@lab061:~/istio-s390x$ total 32
-rw-r--r-- 1 user01 users 1306 Jun 24 12:54 README.md
-rwxr-xr-x 1 user01 users 4029 Jun 24 12:54 build_push_update_images.sh
drwxr-xr-x 2 user01 users 4096 Jun 24 12:54 networking
drwxr-xr-x 3 user01 users 4096 Jun 24 12:54 platform
drwxr-xr-x 2 user01 users 4096 Jun 24 12:54 policy
drwxr-xr-x 8 user01 users 4096 Jun 24 12:54 src
-rw-r--r-- 1 user01 users 6329 Jun 24 12:54 swagger.yaml
</code></pre></div>
</details>
<p>If you navigate to the GitHub in a web browser <a href="https://github.com/mmondics/istio-s390x/tree/ocp-wildfire">https://github.com/mmondics/istio-s390x/tree/ocp-wildfire</a>, you will notice that the sub-directories in your Linux session reflect the folders contained in the repository.</p>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>README.md</strong></td>
<td>Contains   the content displayed on the GitHub page for this repository. You can read   through this README file if you want to get more information about this lab.</td>
</tr>
<tr>
<td><strong>build_push_update_images.sh</strong></td>
<td>Directory   containing a shell script that was used to create the container images used   in this lab. You will not be using this script, but it is here for anyone who   wishes to update images to newer versions in the future.</td>
</tr>
<tr>
<td><strong>networking</strong></td>
<td>Directory   container various YAML files for networking components such as gateways,   virtualservices, destinationrules, and more.</td>
</tr>
<tr>
<td><strong>platform</strong></td>
<td>Directory   containing various YAML files that will create the application deployments,   services, serviceaccounts, and more.</td>
</tr>
<tr>
<td><strong>policy</strong></td>
<td>Directory   containing a YAML file that will create envoyfilters in order to dynamically   rate-limit the traffic to the service mesh application.</td>
</tr>
<tr>
<td><strong>src</strong></td>
<td>Directory   containing the source files used to build each container image used in this   lab. These source files will not be used in this lab.</td>
</tr>
<tr>
<td><strong>swagger.yaml</strong></td>
<td>A   YAML file that defines and documents the structure of the APIs used in this   lab. You will not be interacting with this file directly.</td>
</tr>
</tbody>
</table>
</li>
</ol></section><section class="print-page" id="lab010-lab010-5"><h1 id="lab010-lab010-5-deploying-an-application-on-the-service-mesh">Deploying an Application on the Service Mesh<a class="headerlink" href="#lab010-lab010-5-deploying-an-application-on-the-service-mesh" title="Permanent link">&para;</a></h1>
<p>The first thing we will do is deploy our application - <em>Bookinfo</em>. <a href="https://istio.io/latest/docs/examples/bookinfo/">Bookinfo</a> is a sample application provided by Istio, the upstream project from which OpenShift Service Mesh is built.</p>
<p>The application displays information about a book, similar to a single catalog entry of an online bookstore. Displayed on the page is a description of the book, book details (ISBN, number of pages, and so on), and a few book reviews.</p>
<p>The Bookinfo application is broken into four separate microservices:</p>
<ul>
<li><em>productpage</em>. The productpage microservice calls the details and reviews microservices to populate the page.</li>
<li><em>details</em>. The details microservice contains book information.</li>
<li><em>reviews</em>. The reviews microservice contains book reviews. It also calls the ratings microservice.</li>
<li><em>ratings</em>. The ratings microservice contains book ranking information that accompanies a book review.</li>
</ul>
<p>There are 3 versions of the reviews microservice:</p>
<ul>
<li>Version <em>v1</em> doesn’t call the ratings service.</li>
<li>Version <em>v2</em> calls the ratings service and displays each rating as 1 to 5 black stars.</li>
<li>Version <em>v3</em> calls the ratings service and displays each rating as 1 to 5 red stars.</li>
</ul>
<p>The end-to-end architecture of the application is shown below.</p>
<p><img alt="bookinfo-arch" src="../images/bookinfo-arch.png" /></p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>This application is <em>polyglot</em>, i.e., the microservices are written in different languages. It’s worth noting that these microservices have no dependencies on the Istio or OSSM, but make an interesting Service Mesh example, particularly because of the multitude of microservices, languages and versions for the reviews microservice.</p>
</div>
<p>To run Bookinfo on the Service Mesh requires <em>no changes to the application itself</em>. You simply need to run the application in a Service Mesh-enabled environment, with Envoy sidecars injected alongside each microservice.</p>
<p>Our OpenShift environment is Service Mesh enabled. OSSM is already installed in our OpenShift cluster and is configured to watch for Service Mesh-enabled deployments to appear in your project, userNN-project. The Service Mesh knows which project to watch through an instance of the Istio Service Mesh Member Roll custom resource. Your userNN ID is not able to access this custom resource, but it is displayed below for your reference.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ oc describe ServiceMeshMemberRoll/default
Name:         default
Namespace:    istio-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  maistra.io/v1
Kind:         ServiceMeshMemberRoll
Spec:
Members:
    user01-project
    user02-project
    user03-project
    user04-project
    user05-project
Status:
Annotations:
    Configured Member Count:  5/5
Conditions:
    Last Transition Time:  2021-06-28T19:04:47Z
    Message:               All namespaces have been configured successfully
    Reason:                Configured
    Status:                True
    Type:                  Ready
Configured Members:
    user01-project
    user02-project
    user03-project
    user04-project
    user05-project
Mesh Generation:          1
Mesh Reconciled Version:  2.0.6-2.el8-1
Observed Generation:      6
Pending Members:
Events:  &lt;none&gt;
</code></pre></div>
</details>
<details class="note" open="open"><summary>Note</summary><p>The screenshot above has been trimmed down for brevity. User projects up to user30-project are configured.</p>
</details>
<p>This custom resource will watch each member project and automatically inject Envoy sidecars when a deployment is created with the annotation <code>sidecar.istio.io/inject: "true"</code>.</p>
<p>The deployments contained in the GitHub repository you pulled already have this annotation configured.</p>
<ol>
<li>
<p><strong>From the Istio-s390x directory, view the application deployments with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat platform/kube/bookinfo.yaml
</code></pre></div>
<p>You will find that this YAML file contains a Service, a ServiceAccount, and a Deployment for each of the microservices described previously. Find a section of the YAML file that has kind: Deployment, and you will see the <code>sidecar.istio.io/inject: "true"</code> annotation in its spec section.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>    apiVersion: apps/v1
    kind: Deployment
    metadata:
    name: details-v1
    labels:
        app: details
        version: v1
    spec:
    replicas: 1
    selector:
        matchLabels:
        app: details
        version: v1
    template:
        metadata:
        annotations:
            sidecar.istio.io/inject: &quot;true&quot;
        labels:
            app: details
            version: v1
        spec:
        serviceAccountName: bookinfo-details
        containers:
        - name: details
            image: quay.io/mmondics/examples-bookinfo-details-v1:1.16.2
            imagePullPolicy: IfNotPresent
            ports:
            - containerPort: 9080
</code></pre></div>
</details>
<p>Once created, this YAML file will set up the entire Bookinfo application for you and the Envoy proxy sidecars will be automatically injected for the Istio control plane to interact with.</p>
</li>
<li>
<p><strong>Create the application by running the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc create -f platform/kube/bookinfo.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>    user01@lab061:~/istio-s390x$ oc create -f platform/kube/bookinfo.yaml
    oc create -f platform/kube/bookinfo.yaml 
    service/details created
    serviceaccount/bookinfo-details created
    deployment.apps/details-v1 created
    service/ratings created
    serviceaccount/bookinfo-ratings created
    deployment.apps/ratings-v1 created
    service/reviews created
    serviceaccount/bookinfo-reviews created
    deployment.apps/reviews-v1 created
    deployment.apps/reviews-v2 created
    deployment.apps/reviews-v3 created
    service/productpage created
    serviceaccount/bookinfo-productpage created
    deployment.apps/productpage-v1 created
</code></pre></div>
<p>Although not yet accessible, all of the application components should be up and running within a few seconds.</p>
</details>
</li>
<li>
<p><strong>Check that your four services were created with</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get services
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>    user01@lab061:~/istio-s390x$ oc get services
    NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
    details       ClusterIP   172.30.237.245   &lt;none&gt;        9080/TCP   10s
    productpage   ClusterIP   172.30.73.57     &lt;none&gt;        9080/TCP   10s
    ratings       ClusterIP   172.30.241.30    &lt;none&gt;        9080/TCP   10s
    reviews       ClusterIP   172.30.34.195    &lt;none&gt;        9080/TCP   10s
</code></pre></div>
</details>
</li>
<li>
<p><strong>And check that your six pods have been created with</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get pods
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>    user01@lab061:~/istio-s390x$ oc get services
    NAME                              READY   STATUS    RESTARTS   AGE
    details-v1-78cb8b797f-ndqdz       2/2     Running   0          45s
    productpage-v1-568ddd75bf-rmqtf   2/2     Running   0          45s
    ratings-v1-768dc65999-q5dft       2/2     Running   0          45s
    reviews-v1-6cf69f46c9-wbtqm       2/2     Running   0          45s
    reviews-v2-64fd74bbd7-hx4rd       2/2     Running   0          46s
    reviews-v3-8cb65c475-wr22r        2/2     Running   0          46s
</code></pre></div>
</details>
<p>Each pod should have a <em>STATUS</em>: Running and should show <em>Ready</em>: 2/2. The 2/2 indicates that the pod includes two containers, and both are ready. In our case, each pod has one container for its application, and one container for the Envoy proxy, or sidecar.</p>
<details class="note"><summary>Optional</summary><p>You can list the containers in your productpage pod with the command:</p>
<div class="highlight"><pre><span></span><code>oc get pod productpage-v1-xxxxx -o jsonpath={.spec.containers[*].name}
</code></pre></div>
<p>(where xxxxx is your randomly generated string of characters returned by oc get pods)</p>
<div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ oc get pod productpage-v1-78797f -o jsonpath={.spec.containers[*].name}
productpage istio-proxyuser01@lab061:~/istio-s390x$
</code></pre></div>
<p>Your two container names will be printed at the beginning of the next line - productpage, and Istio-proxy.</p>
</details>
<p>productpage is the end user’s primary ingress into the Bookinfo application. This is the microservice that will pull the data from the ratings, reviews, and details microservices and display them when requested by a user.</p>
</li>
<li>
<p><strong>Check that the productpage microservice is running correctly by issuing the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc exec &quot;$(oc get pod -l app=ratings -o jsonpath=&#39;{.items[0].metadata.name}&#39;)&quot; -c ratings -- curl -sS productpage:9080/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>    user01@lab061:~/istio-s390x$ oc exec &quot;$(oc get pod -l app=ratings -o jsonpath=&#39;{.items[0].metadata.name}&#39;)&quot; -c ratings -- curl -sS productpage:9080/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;
    &lt;title&gt;Simple Bookstore App&lt;/title&gt;
</code></pre></div>
</details>
<p>If you see <code>&lt;title&gt;Simple Bookstore App&lt;/title&gt;</code> returned, your Bookinfo application is working correctly, yet still inaccessible.</p>
</li>
</ol>
<p>In the next section, we will create a Gateway to provide ingress into the Service Mesh application.</p></section><section class="print-page" id="lab010-lab010-6"><h1 id="lab010-lab010-6-understanding-and-deploying-a-service-mesh-gateway-and-virtualservice">Understanding and Deploying a Service Mesh Gateway and VirtualService<a class="headerlink" href="#lab010-lab010-6-understanding-and-deploying-a-service-mesh-gateway-and-virtualservice" title="Permanent link">&para;</a></h1>
<p>Gateways are used to manage inbound and outbound traffic for your mesh, letting you specify which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads.</p>
<p><img alt="gateway-arch" src="../images/gateway-arch.png" /></p>
<p>Unlike other mechanisms for controlling traffic entering your systems, such as the Kubernetes Ingress APIs, Istio gateways let you use the full power and flexibility of Istio’s traffic routing. You can do this because Istio’s Gateway resource just lets you configure layer 4-6 load balancing properties such as ports to expose, TLS settings, and so on. Then instead of adding application-layer traffic routing (L7) to the same API resource, you bind a regular Istio virtual service to the gateway. This lets you basically manage gateway traffic like any other data plane traffic in an Istio mesh.</p>
<p>Gateways are primarily used to manage ingress traffic, but you can also configure egress gateways. An egress gateway lets you configure a dedicated exit node for the traffic leaving the mesh, letting you limit which services can or should access external networks, or to enable secure control of egress traffic to add security to your mesh, for example. We will only be deploying an ingress Gateway in this lab.</p>
<p>Along with a Gateway, we will need a <em>VirtualService</em>. A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry.</p>
<ol>
<li>
<p>You will find a YAML file for a Gateway and VirtualService in your terminal session. <strong>From the istio-s390x directory, view the YAML file with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat networking/bookinfo-gateway.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>    <span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">networking.istio.io/v1alpha3</span>
    <span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Gateway</span>
    <span class="nt">metadata</span><span class="p">:</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">bookinfo-gateway</span>
    <span class="nt">spec</span><span class="p">:</span>
    <span class="nt">selector</span><span class="p">:</span>
        <span class="nt">istio</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">ingressgateway</span> <span class="c1"># use default controller</span>
    <span class="nt">servers</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="nt">port</span><span class="p">:</span>
        <span class="nt">number</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">80</span>
        <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">http</span>
        <span class="nt">protocol</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">HTTP</span>
        <span class="nt">hosts</span><span class="p">:</span>
        <span class="p p-Indicator">-</span> <span class="s">&quot;userNN-project.istio.apps.atsocppa.dmz&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">---</span>
    <span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">networking.istio.io/v1alpha3</span>
    <span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">VirtualService</span>
    <span class="nt">metadata</span><span class="p">:</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">bookinfo</span>
    <span class="nt">spec</span><span class="p">:</span>
    <span class="nt">hosts</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="s">&quot;userNN-project.istio.apps.atsocppa.dmz&quot;</span>
    <span class="nt">gateways</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">bookinfo-gateway</span>
    <span class="nt">http</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="nt">match</span><span class="p">:</span>
        <span class="p p-Indicator">-</span> <span class="nt">uri</span><span class="p">:</span>
            <span class="nt">exact</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">/productpage</span>
        <span class="p p-Indicator">-</span> <span class="nt">uri</span><span class="p">:</span>
            <span class="nt">prefix</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">/static</span>
        <span class="p p-Indicator">-</span> <span class="nt">uri</span><span class="p">:</span>
            <span class="nt">exact</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">/login</span>
        <span class="p p-Indicator">-</span> <span class="nt">uri</span><span class="p">:</span>
            <span class="nt">exact</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">/logout</span>
        <span class="p p-Indicator">-</span> <span class="nt">uri</span><span class="p">:</span>
            <span class="nt">prefix</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">/api/v1/products</span>
        <span class="nt">route</span><span class="p">:</span>
        <span class="p p-Indicator">-</span> <span class="nt">destination</span><span class="p">:</span>
            <span class="nt">host</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">productpage</span>
            <span class="nt">port</span><span class="p">:</span>
            <span class="nt">number</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">9080</span>
</code></pre></div>
</details>
<p>You will notice that there are a few instances of userNN in the YAML file that must be edited to match your user number. You can quickly change these by entering the following command.</p>
<details class="information"><summary>Expand for more information</summary><p>Extra information for those interested…</p>
<p>The hosts field lists the VirtualService’s hosts - in other words, the user-addressable destination or destinations that these routing rules apply to. This is the address or addresses used when sending requests to the service.</p>
<p>The virtual service hostname can be an IP address, a DNS name, or, depending on the platform, a short name (such as a Kubernetes service short name) that resolves to a fully qualified domain name (FQDN). You can also use wildcard (”*”) prefixes, letting you create a single set of routing rules for all matching services.</p>
<p>The http section contains the virtual service’s routing rules, describing match conditions and actions for routing HTTP/1.1, HTTP2, and gRPC traffic sent to the destination(s) specified in the hosts field (you can also use tcp and tls sections to configure routing rules for TCP and unterminated TLS traffic). A routing rule consists of the destination where you want the traffic to go and zero or more match conditions, depending on your use case.</p>
<p>The route section’s destination field specifies the actual destination for traffic that matches this condition. Unlike the VirtuallService’s host(s), the destination’s host must be a real destination that exists in Istio’s service registry or Envoy won’t know where to send traffic to it. This can be a mesh service with proxies or a non-mesh service added using a service entry. In this case we’re running on Kubernetes and the host name is a Kubernetes service name.</p>
</details>
</li>
<li>
<p><strong>Make sure that you change <YOUR_USER_NUMBER> to the correct number</strong>, i.e. 01 for user01</p>
<div class="highlight"><pre><span></span><code>sed -i &#39;s/NN/&lt;YOUR_USER_NUMBER&gt;/g&#39; networking/bookinfo-gateway.yaml
</code></pre></div>
</li>
<li>
<p><strong>Create the Gateway and VirtualService with the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc create -f networking/bookinfo-gateway.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ oc create -f networking/bookinfo-gateway.yaml
gateway.networking.istio.io/bookinfo-gateway created
virtualservice.networking.istio.io/bookinfo created
</code></pre></div>
</details>
</li>
<li>
<p><strong>And view the new objects with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get gateway,virtualservice
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
</div>
<p>This command will <em>not</em> work if there is a space after the comma.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ oc get gateway,virtualservice
NAME                       AGE
gateway/bookinfo-gateway   42s

NAME                      GATEWAYS               HOST               
virtualservice/bookinfo   [&quot;bookinfo-gateway&quot;]   [&quot;user15-project.istio.apps.atsocppa.dmz&quot;]
</code></pre></div>
</details>
<p>With this Gateway and VirtualService, you are now able to access the application.</p>
</li>
<li>
<p><strong>First, identify your Gateway URL by entering the following command in your terminal</strong>:</p>
<div class="highlight"><pre><span></span><code>export GATEWAY_URL=$(oc get virtualservice bookinfo -o jsonpath=&#39;{.spec.hosts[0]}&#39;)
</code></pre></div>
</li>
<li>
<p><strong>Next, enter the following command to print your productpage URL</strong>:</p>
<div class="highlight"><pre><span></span><code>echo &quot;http://$GATEWAY_URL/productpage&quot;
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ export GATEWAY_URL=$(oc get virtualservice bookinfo -o jsonpath=&#39;{.spec.hosts[0]}&#39;)
user01@lab061:~/istio-s390x$ echo &quot;http://$GATEWAY_URL/productpage&quot;
user01@lab061:~/istio-s390x$ http://user15-project.istio.apps.atsocppa.dmz/productpage
</code></pre></div>
</details>
</li>
<li>
<p><strong>Copy the URL that is returned, and paste it into a web browser</strong></p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The URL will be similar to <a href="http://userNN-project.istio.apps.atsocppa.dmz/productpage">http://userNN-project.istio.apps.atsocppa.dmz/productpage</a>, (where NN is your user number).</p>
</div>
<p><img alt="bookinfo-1" src="../images/bookinfo-1.png" /></p>
<p>If all is working correctly, the overall productpage is shown, which displays information about William Shakespeare’s play, The Comedy of Errors. The details on the left are returned by the details microservice, the text for the two reviews is provided by the ratings microservice, and the star ratings (or lack thereof) are returned by one of the three reviews microservices, depending on which was called by the ratings microservice.</p>
<p>You now have an application made up of six microservices written in four different languages deployed on the OpenShift Service Mesh and can now take full advantage of its features. We will look at a subset of them in the following sections.</p>
</li>
</ol></section><section class="print-page" id="lab010-lab010-7"><h1 id="lab010-lab010-7-traffic-management">Traffic Management<a class="headerlink" href="#lab010-lab010-7-traffic-management" title="Permanent link">&para;</a></h1>
<p>Istio’s traffic routing rules let you easily control the flow of traffic and API calls between services. Istio simplifies configuration of service-level properties like circuit breakers, timeouts, and retries, and makes it easy to set up important tasks like A/B testing, canary rollouts, and staged rollouts with percentage-based traffic splits. It also provides out-of-box failure recovery features that help make your application more robust against failures of dependent services or the network.</p>
<ol>
<li>
<p><strong>Refresh the productpage a few times</strong> and you will see either black stars, red stars, or no stars.</p>
<p>This is because, by default, Istio uses a <em>round-robin load balancing</em> policy, where each service instance in the instance pool gets a request in turn. Istio also supports the following models, which you can specify in destination rules for requests to a particular service or service subset.</p>
<ul>
<li>Random: Requests are forwarded at random to instances in the pool.</li>
<li>Weighted: Requests are forwarded to instances in the pool according to a specific percentage.</li>
<li>Least requests: Requests are forwarded to instances with the least number of requests.</li>
</ul>
<p>Along with VirtualServices, <em>DestinationRules</em> are a key part of Istio’s traffic routing functionality. You can think of virtual services as how you route your traffic to a given destination, and then you use destination rules to configure what happens to traffic for that destination. Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic’s “real” destination.</p>
</li>
<li>
<p>There is a DestinationRule YAML file provided in the networking directory. <strong>Take a look at it with</strong>:</p>
<div class="highlight"><pre><span></span><code>cat networking/destination-rule-all.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">networking.istio.io/v1alpha3</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">DestinationRule</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">productpage</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="nt">host</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">productpage</span>
<span class="nt">subsets</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
    <span class="l l-Scalar l-Scalar-Plain">labels</span><span class="p p-Indicator">:</span>
    <span class="nt">version</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">networking.istio.io/v1alpha3</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">DestinationRule</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">reviews</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="nt">host</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">reviews</span>
<span class="nt">subsets</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
    <span class="l l-Scalar l-Scalar-Plain">labels</span><span class="p p-Indicator">:</span>
    <span class="nt">version</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v2</span>
    <span class="l l-Scalar l-Scalar-Plain">labels</span><span class="p p-Indicator">:</span>
    <span class="nt">version</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v2</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v3</span>
    <span class="l l-Scalar l-Scalar-Plain">labels</span><span class="p p-Indicator">:</span>
    <span class="nt">version</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v3</span>
<span class="nn">---</span>
    <span class="l l-Scalar l-Scalar-Plain">More cut from screenshot</span>     
</code></pre></div>
</details>
<p>This will be our baseline DestinationRules with no special routing or load balancing included. These DesintationRules simply describe the various versions of each microservce (v1, v2, v3).</p>
</li>
<li>
<p><strong>Create the DestinationRules with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc create -f networking/destination-rule-all.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ oc create -f networking/destination-rule-all.yaml
destinationrule.networking.istio.io/productpage created
destinationrule.networking.istio.io/reviews created
destinationrule.networking.istio.io/ratings created
destinationrule.networking.istio.io/details created
</code></pre></div>
</details>
<p>Now that OpenShift knows which versions of each microservice are available, we can use Istio to control the version routing.</p>
<p>For example, each of the microservices in Bookinfo application include a version v1. We can deploy a new VirtualService that routes all traffic to the v1 microservices.</p>
</li>
<li>
<p><strong>Look at the v1-specific VirtualService with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat networking/virtual-service-all-v1.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">networking.istio.io/v1alpha3</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">VirtualService</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">productpage</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="nt">hosts</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">productpage</span>
<span class="nt">http</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">route</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="nt">destination</span><span class="p">:</span>
        <span class="nt">host</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">productpage</span>
        <span class="nt">subset</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">networking.istio.io/v1alpha3</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">VirtualService</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">reviews</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="nt">hosts</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">reviews</span>
<span class="nt">http</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">route</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="nt">destination</span><span class="p">:</span>
        <span class="nt">host</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">reviews</span>
        <span class="nt">subset</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nn">---</span>
            <span class="l l-Scalar l-Scalar-Plain">More cut from screenshot</span>
</code></pre></div>
</details>
<p>And notice that v1 is the only version specified for each microservice.</p>
</li>
<li>
<p><strong>Create the new VirtualService with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc apply -f networking/virtual-service-all-v1.yaml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ oc apply -f networking/virtual-service-all-v1.yaml 
virtualservice.networking.istio.io/productpage created
virtualservice.networking.istio.io/reviews created
virtualservice.networking.istio.io/ratings created
virtualservice.networking.istio.io/details created
</code></pre></div>
</details>
<p>You have just configured Istio to route all traffic to the v1 version of each microservice, most importantly the reviews microservice that decides which stars are displayed on the productpage.</p>
</li>
<li>
<p><strong>Back in your web browser, refresh the productpage a few times</strong>.</p>
<p><img alt="bookinfo-2" src="../images/bookinfo-2.png" /></p>
<p>You should notice now that no matter how many times you refresh, no stars will be displayed, because the new VirtualService only allows you to reach v1 of reviews.</p>
<p><img alt="bookinfo-arch-v1" src="../images/bookinfo-arch-v1.png" /></p>
<p>You can also control the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service reviews:v2.</p>
</li>
<li>
<p><strong>Run the following command to enable user-based routing</strong>:</p>
<div class="highlight"><pre><span></span><code>oc apply -f networking/virtual-service-reviews-test-v2.yaml
</code></pre></div>
</li>
<li>
<p><strong>In your web browser, log into the productpage as user:</strong> <code>Jason</code> <strong>with password:</strong> <code>Jason</code>.</p>
</li>
<li>
<p><strong>Refresh the productpage a few times again</strong> and notice that Jason is only able to reach v2 of the reviews microservice, which displays black stars.</p>
<p><img alt="bookinfo-3" src="../images/bookinfo-3.png" /></p>
</li>
<li>
<p><strong>On the productpage, sign out, then sign in with a different user:</strong> <code>Richard</code> <strong>with password:</strong> <code>Richard</code>.</p>
</li>
<li>
<p><strong>Refresh the productpage a few times again and notice that Richard can only reach v1 of the reviews microservice, which does not display stars</strong>.</p>
</li>
</ol>
<p>In this task, you used Istio to send 100% of the traffic to the v1 version of each of the Bookinfo services. You then set a rule to selectively send traffic to version v2 of the reviews service based on a custom end-user header (for Jason) added to the request by the productpage service.</p></section><section class="print-page" id="lab010-lab010-8"><h1 id="lab010-lab010-8-application-observability-with-kiali">Application Observability with Kiali<a class="headerlink" href="#lab010-lab010-8-application-observability-with-kiali" title="Permanent link">&para;</a></h1>
<p><em>Kiali</em> provides visibility into your service mesh by showing you the microservices in your service mesh, and how they are connected.</p>
<p>Kiali provides an interactive graph view of your namespace in real time that provides visibility into features like circuit breakers, request rates, latency, and even graphs of traffic flows. Kiali offers insights about components at different levels, from Applications to Services and Workloads, and can display the interactions with contextual information and charts on the selected graph node or edge. Kiali also provides the ability to validate your Istio configurations, such as gateways, destination rules, virtual services, mesh policies, and more.</p>
<ol>
<li>
<p><strong>Navigate to the Kiali console located at: <a href="https://kiali-istio-system.apps.atsocppa.dmz">https://kiali-istio-system.apps.atsocppa.dmz</a></strong></p>
</li>
<li>
<p><strong>Log in with your OpenShift credentials</strong>.</p>
</li>
<li>
<p><strong>View the overview of your mesh in the Overview page that appears immediately after you log in</strong>.</p>
<p>The Overview page displays all the namespaces that have services in the mesh.</p>
<p><img alt="kiali-overiview" src="../images/kiali-overview.png" /></p>
<p>The namespaces shown are those previously discussed in the Service Mesh Member Roll.</p>
</li>
</ol>
<h2 id="lab010-lab010-8-validating-istio-configuration-with-kiali">Validating Istio Configuration with Kiali<a class="headerlink" href="#lab010-lab010-8-validating-istio-configuration-with-kiali" title="Permanent link">&para;</a></h2>
<p>Kiali can validate your Istio resources to ensure they follow proper conventions and semantics. Any problems detected in the configuration of your Istio resources can be flagged as errors or warnings depending on the severity of the incorrect configuration.</p>
<p>You might have noticed on the Kiali overview page that there is an error somewhere within your userNN-project Istio configuration.</p>
<p><img alt="kiali-overiview-2" src="../images/kiali-overview-2.png" /></p>
<ol>
<li>
<p>Let’s find and fix that error. <strong>In the left-side menu of the Kiali console, navigate to Istio Config, and then filter to your userNN-project namespace</strong>.</p>
<p><img alt="istio-config" src="../images/istio-config.png" /></p>
<p>You should notice an error icon in the Configuration column for your details microservice.</p>
</li>
<li>
<p><strong>Click the details hyperlink to drill down into the microservice and scroll to the bottom of the YAML file</strong>.</p>
<p><img alt="yaml-error" src="../images/yaml-error.png" /></p>
</li>
<li>
<p>There is an error with v2 of the details DestinationRule. <strong>For more specificity, hover over the red X to the left of the YAML file</strong>.</p>
<p><img alt="yaml-error-more" src="../images/yaml-error-more.png" /></p>
<p>You will see the error code KIA0203 This subset’s labels are not found in any matching host. Essentially, the details DestinationRule is failing to find the v2 host for the details microservice.</p>
<p>If you look back at our Bookinfo application architecture, there is only supposed to be one version of the details microservice - v1 is the only version that exists.</p>
</li>
<li>
<p><strong>Correct the error by deleting lines 30-32 of the YAML file, resulting in the following</strong>:</p>
<p><img alt="yaml-error-fixed" src="../images/yaml-error-fixed.png" /></p>
</li>
<li>
<p><strong>Click on the Istio Config tab in the left-side menu again to navigate back to the configuration page for your project</strong>.</p>
<p>You should no longer have any configuration issues in your Istio configuration.</p>
<p><img alt="istio-config-noerror" src="../images/istio-config-noerror.png" /></p>
</li>
</ol>
<h2 id="lab010-lab010-8-validating-service-mesh-application-configuration-with-kiali">Validating Service Mesh Application Configuration with Kiali<a class="headerlink" href="#lab010-lab010-8-validating-service-mesh-application-configuration-with-kiali" title="Permanent link">&para;</a></h2>
<p>Along with identifying issues with the Istio configuration, Kiali can also identify issues with the <em>applications running on the Service Mesh</em>.</p>
<ol>
<li>
<p><strong>In your terminal session, introduce an invalid configuration of a service port name with the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc patch service details --type json -p \
&#39;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/ports/0/name&quot;, &quot;value&quot;:&quot;foo&quot;}]&#39;
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ oc patch service details --type json -p \
&gt; &#39;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/ports/0/name&quot;, &quot;value&quot;:&quot;foo&quot;}]&#39;
service/details patched
</code></pre></div>
</details>
<p>If you see the <code>service/details patched</code> message as in the image above, your patch was successful.</p>
</li>
<li>
<p>Back in the Kiali web console, <strong>navigate to the Services page from the left side menu</strong>.</p>
<p><img alt="kiali-service" src="../images/kiali-service.png" /></p>
<p>You will notice that you now have an error icon under the Configuration column for the details service.</p>
</li>
<li>
<p><strong>Click the details hyperlink and then click the Network option under Service Info</strong>.</p>
<p><img alt="service-info-error" src="../images/service-info-error.png" /></p>
</li>
<li>
<p>Hover over the error icon to display a tool tip describing the error.</p>
<p><img alt="service-info-error-2" src="../images/service-info-error-2.png" /></p>
<p>This error is telling you that your port name does not follow the correct syntax.</p>
</li>
<li>
<p><strong>Back in your terminal session, run the following command to correct the port name</strong>:</p>
<div class="highlight"><pre><span></span><code>    oc patch service details--type json -p \
    &#39;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/ports/0/name&quot;, &quot;value&quot;:&quot;http&quot;}]&#39;
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ oc patch service details --type json -p \
&gt; &#39;[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/ports/0/name&quot;, &quot;value&quot;:&quot;http&quot;}]&#39;
service/details patched
</code></pre></div>
</details>
</li>
<li>
<p><strong>Back in the Kiali console, click the blue refresh button and see that your error has been fixed</strong>.</p>
<p><img alt="service-info-noerror" src="../images/service-info-noerror.png" /></p>
</li>
</ol>
<p>Now that our Service Mesh is configured correctly, along with the application running on top of it, let’s explore some of the other features that Kiali provides.</p>
<h2 id="lab010-lab010-8-viewing-your-service-mesh-applications-with-kiali">Viewing Your Service Mesh Applications with Kiali<a class="headerlink" href="#lab010-lab010-8-viewing-your-service-mesh-applications-with-kiali" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p>To view your namespace graph, <strong>Select the Graph option in the left side menu and select your project in the namespace dropdown</strong>. The page looks similar to:</p>
<p><img alt="kiali-graph" src="../images/kiali-graph.png" /></p>
<p>The graph represents traffic flowing through the service mesh for a period of time, generated using Istio telemetry.</p>
</li>
<li>
<p>Let’s generate some traffic into our Bookinfo application. <strong>In your terminal session, run the following command to continually send http request to the productpage</strong>:</p>
<div class="highlight"><pre><span></span><code>watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage
Every 1.0s: curl -o /dev/null -s -w %{http_code} user01-project.istio.apps.atsocppa.dmz/productpage

200
</code></pre></div>
</details>
<p>If you see a 200 return code as in the image above, you’re now sending requests to the productpage every second.</p>
<div class="admonition important">
<p class="admonition-title"><img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /> Important <img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /></p>
<p>Note: you will want to leave this watch command running until otherwise directed.</p>
</div>
</li>
<li>
<p><strong>Open a second terminal session and connect to the environment as directed previously in this lab</strong>.</p>
</li>
<li>
<p><strong>To view a summary of metrics, select any node or edge in the graph to display its metric details in the summary details panel on the right</strong>.</p>
<p>For example, <strong>click on the triangle representing the productpage service</strong>, and you should see a 100% success rate for both inbound and outbound traffic.</p>
<p><img alt="productpage-node" src="../images/productpage-node.png" /></p>
</li>
<li>
<p><strong>On this same page, click the Display dropdown and select Requests Percentage and Service Nodes, if they aren’t already selected</strong>.</p>
<p><img alt="display-dropdown" src="../images/display-dropdown.png" /></p>
<p>This will let you view the percentage of traffic to each workload in near real-time. For example, since there are three versions of the reviews microservice and traffic is being distributed in a round-robin fashion, you should see close to 33% traffic going from the productpage to each version of reviews.</p>
<p><img alt="reviews-flow" src="../images/reviews-flow.png" /></p>
<p>Your percentages are likely not exactly 33.3%, as in the image above. More often, they will vary between 20% and 40%.</p>
</li>
</ol>
<h2 id="lab010-lab010-8-managing-service-mesh-applications-with-kiali">Managing Service Mesh Applications with Kiali<a class="headerlink" href="#lab010-lab010-8-managing-service-mesh-applications-with-kiali" title="Permanent link">&para;</a></h2>
<p>Kiali does not only allow visibility into your service mesh application, Kiali can also be used to directly interact with the application.</p>
<ol>
<li>
<p><strong>Click the reviews service represented by a triangle. In the right-side menu that pops up, click on the hyperlink for the reviews service</strong>.</p>
<p><img alt="reviews-service" src="../images/reviews-service.png" /></p>
<p>You are taken to the Services page (instead of Graph, where you previously were). This shows expanded information about the reviews service, its properties and versions, metrics about its traffic, and more.</p>
</li>
<li>
<p><strong>Click the Actions dropdown in the top right of the page to see the traffic management options you have - Request Routing, Fault Injection, Traffic Shifting, and Request Timeouts</strong>.</p>
<p>These should all be grayed out right now because these are managed through DestinationRules, and we already created one for the reviews service in a previous step.</p>
</li>
<li>
<p><strong>Delete the Traffic Routing from this Actions dropdown and confirm that you want to delete the DestinationRule: ‘reviews’</strong>.</p>
<p><img alt="delete-traffic-routing" src="../images/delete-traffic-routing.png" /></p>
</li>
<li>
<p><strong>Click the Actions dropdown again</strong> and you will notice that the options can now be selected and <strong>click the Traffic Shifting option</strong>.</p>
<p><img alt="traffic-shifting" src="../images/traffic-shifting.png" /></p>
<p>From this page, you can create a new Traffic Shifting rule to manage how much traffic should be directed to each version of the reviews microservice.</p>
</li>
<li>
<p><strong>Slide the slider for reviews-v3 all the way to the left for 0%, and then make reviews-v1 and reviews-v2 50% and click create</strong>. Your page should look like the image below.</p>
<p><img alt="create-traffic-shifting" src="../images/create-traffic-shifting.png" /></p>
</li>
<li>
<p><strong>Navigate back to the Graph page from the left side menu</strong>.</p>
<p>Over the next few minutes (and depending on the graph’s refresh rate that you can edit in the top right of the Graph page) you will see that the percentage of traffic going to v3 of reviews will decrease towards 0%, while the traffic going to v1 and v2 will increase towards 50%.</p>
<p><img alt="shifted-traffic" src="../images/shifted-traffic.png" /></p>
</li>
<li>
<p><strong>In your web browser, navigate back to your bookinfo productpage and refresh the page a few times</strong>.</p>
<p>No matter how many times you refresh, you will not see the red stars again. That is because no traffic can reach v3 of the reviews microservice, which is the version that provides red stars.</p>
</li>
</ol>
<p>As you can tell from the past few sections, you can control Service Mesh applications either from the Command Line by creating VirtualServices and DestinationRules, or by using the Kiali GUI console. Using the Kiali console simply generates the VirtualServices and DestinationRules for you, however the Command Line offers greater flexibility, more control, and the ability to automate the creation of these rules.</p>
<p>We will now move on from Kiali to Jaeger, the tool that OSSM uses for distributed tracing. At this point, feel free to explore Kiali and the other functions it provides. There are many things Kiali can do that we will not be covering in this lab. You can find more information in the Kiali documentation here: <a href="https://kiali.io/documentation/latest/features/">https://kiali.io/documentation/latest/features/</a></p></section><section class="print-page" id="lab010-lab010-9"><h1 id="lab010-lab010-9-distributed-tracing-with-jaeger">Distributed Tracing with Jaeger<a class="headerlink" href="#lab010-lab010-9-distributed-tracing-with-jaeger" title="Permanent link">&para;</a></h1>
<p><em>Distributed Tracing</em> is the process of tracking the performance of individual microservices in an application by tracing the path of the service calls in the application. Each time a user takes action in an application, a request is executed that might require many microservices to interact to produce a response.</p>
<p><em>Jaeger</em> is an open source distributed tracing system used by OpenShift Service Mesh. With Jaeger, you can perform a trace that follows the path of a request through various microservices which make up an application.</p>
<p>For our Bookinfo application, traces are generated when HTTP requests are made to the productpage microservice. This starts a cascade of requests to the other microservices in the Bookinfo mesh.</p>
<ol>
<li>
<p>Before we start looking at traces, <strong>check that your watch command is still running in your terminal session</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>Every 1.0s: curl -o /dev/null -s -w %{http_code} user01-project.istio.apps.atsocppa.dmz/productpage

200
</code></pre></div>
</details>
<p>If you see the 200 status code return and the time in the top right is current, you are still sending requests to your productpage and will be able to generate traces.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If your watch command has been stopped for whatever reason, start it again with the command:</p>
<div class="highlight"><pre><span></span><code>watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage
</code></pre></div>
</div>
<ol>
<li><strong>Navigate to the Jaeger console located at: <a href="https://jaeger-istio-system.apps.atsocppa.dmz">https://jaeger-istio-system.apps.atsocppa.dmz</a></strong></li>
</ol>
</li>
<li>
<p><strong>Login with your OpenShift credentials</strong>.</p>
<p><img alt="jaeger-dash" src="../images/jaeger-dash.png" /></p>
</li>
<li>
<p><strong>In the Service dropdown, select</strong> <code>productpage.userNN-project</code>.</p>
<p>If others are doing this same lab, it might be easier to search for your userNN-project.</p>
<p><img alt="service-dropdown" src="../images/service-dropdown.png" /></p>
</li>
<li>
<p><strong>Scroll to the bottom-left of the page and click the Find Traces button</strong>.</p>
<p><img alt="traces-1" src="../images/traces-1.png" /></p>
<p>The rest of the page will be populated by a graph of your traces over time, and the traces that meet your search criteria. You will notice that some of your traces have much longer durations than others - don’t worry about these. This is due to hard-coded http timeouts in the productpage python application code.</p>
<p>The graph displayed at the top of the page has circles representing traces, with time on the x-axis and duration of the trace on the y-axis. The size of the circle represents how many spans make up the trace. A span is the logical unit of work associated with one microservice in the mesh.</p>
<p><img alt="trace-diagram" src="../images/trace-diagram.png" /></p>
</li>
<li>
<p><strong>Click one of the traces in the list below the graph. Select a trace that has 8 spans, as described in the left side of the box</strong>.</p>
<p><img alt="trace-2" src="../images/trace-2.png" /></p>
<p>You will be taken to a page that looks like the following:</p>
<p><img alt="selected-trace-1" src="../lab010/images/selected-trace-1" /></p>
<p>This graph shows how long each microservice took, when it started, when it ended, and includes detailed information about each span.</p>
</li>
<li>
<p><strong>Expand the span for your productpage, and then expand the Tags row</strong>.</p>
<p><img alt="selected-trace-2" src="../lab010/images/selected-trace-2" /></p>
<p>Here you will find more information about the productpage span that may be helpful with debugging an application issue or latency.</p>
</li>
<li>
<p><strong>Click the back arrow in the top left of the page to navigate back to your project traces</strong>.</p>
<p><img alt="go-back" src="../images/go-back.png" /></p>
<p>Jaeger includes a feature to compare two traces to one another.</p>
</li>
<li>
<p>In your list of traces, <strong>find one trace that has 8 spans, and another that has 6 spans</strong>.</p>
<p><img alt="compare-trace" src="../images/compare-trace.png" /></p>
</li>
<li>
<p><strong>Click the checkboxes next to the names of the traces, and then Compare Traces to the top right of the traces list</strong>.</p>
<p><img alt="compare-traces-2" src="../images/compare-traces-2.png" /></p>
<p>The resulting page shows the microservices that exist in trace A, trace B, and both traces you selected.</p>
<p><img alt="compared-traces" src="../images/compared-traces.png" /></p>
<p>Your comparison will likely look different. Any gray nodes are microservices that exist in both traces at the same version. Any red nodes exist in Trace A, but not Trace B. Any green nodes exist in Trace B, but not Trace A. If you’re comparing the Traces in the screenshot above, you can discern that the ratings microservice is not being called in Trace B, and that the versions of the reviews microservices are different in the two traces.</p>
</li>
</ol>
<details class="note" open="open"><summary>Note</summary><p>The distributed tracing sampling rate is set to sample 100% of traces in your Service Mesh by default. A high sampling rate consumes cluster resources and performance but is useful when debugging issues. Before you deploy Red Hat OpenShift Service Mesh in production, you would want to set the value to a smaller proportion of traces.</p>
</details></section><section class="print-page" id="lab010-lab010-10"><h1 id="lab010-lab010-10-wrap-up-clean-up">Wrap Up &amp; Clean Up<a class="headerlink" href="#lab010-lab010-10-wrap-up-clean-up" title="Permanent link">&para;</a></h1>
<p>In this lab, you have explored many of the features that come as part of OpenShift Service Mesh. OSSM is an extremely powerful OpenShift add-on, and we were not able to fit all of its features into this lab. You can find more information &amp; tutorials at the following documentation links:</p>
<ul>
<li>OpenShift Service Mesh: <a href="https://docs.openshift.com/container-platform/4.7/service_mesh/v2x/ossm-about.html">https://docs.openshift.com/container-platform/4.7/service_mesh/v2x/ossm-about.html</a></li>
<li>Istio: <a href="https://istio.io/latest/docs/">https://istio.io/latest/docs/</a></li>
<li>Kiali: <a href="https://kiali.io/documentation/">https://kiali.io/documentation/</a></li>
<li>
<p>Jaeger: <a href="https://www.jaegertracing.io/docs/1.24/">https://www.jaegertracing.io/docs/1.24/</a></p>
</li>
<li>
<p><strong>When you’re ready to clean up and finish this lab, run the following script to delete all the resources from your OpenShift project</strong>.</p>
<div class="highlight"><pre><span></span><code>./cleanup.sh
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/istio-s390x$ ./cleanup.sh 
serviceaccount &quot;bookinfo-details&quot; deleted
serviceaccount &quot;bookinfo-productpage&quot; deleted
serviceaccount &quot;bookinfo-ratings&quot; deleted
serviceaccount &quot;bookinfo-reviews&quot; deleted
deployment.apps &quot;details-v1&quot; deleted
deployment.apps &quot;productpage-v1&quot; deleted
deployment.apps &quot;ratings-v1&quot; deleted
deployment.apps &quot;reviews-v1&quot; deleted
deployment.apps &quot;reviews-v2&quot; deleted
deployment.apps &quot;reviews-v3&quot; deleted
service &quot;details&quot; deleted
service &quot;productpage&quot; deleted
service &quot;ratings&quot; deleted
service &quot;reviews&quot; deleted
gateway.networking.istio.io &quot;bookinfo-gateway&quot; deleted
virtualservice.networking.istio.io &quot;bookinfo&quot; deleted
destinationrule.networking.istio.io &quot;details&quot; deleted
destinationrule.networking.istio.io &quot;productpage&quot; deleted
destinationrule.networking.istio.io &quot;ratings&quot; deleted
destinationrule.networking.istio.io &quot;reviews&quot; deleted
Cleanup Complete
</code></pre></div>
</details>
</li>
</ul></section><h1 class='nav-section-title-end'>Ended: OpenShift Service Mesh</h1>
                        <h2 class='nav-section-title' id='section-monitoring-metering-and-metrics'>
                            Monitoring, Metering, and Metrics <a class='headerlink' href='#section-monitoring-metering-and-metrics' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab005-lab005-1"><h1 id="lab005-lab005-1-monitoring-metering-and-metrics">Monitoring, Metering, and Metrics<a class="headerlink" href="#lab005-lab005-1-monitoring-metering-and-metrics" title="Permanent link">&para;</a></h1>
<p>A significant architectural shift toward containers is underway and, as with any architectural shift, this brings new operational challenges. It can be challenging for many of the legacy monitoring tools to monitor container platforms in fast moving, often ephemeral environments. The good news is newer cloud-based offerings can ensure monitoring solutions are as scalable as the services being built and monitored. These new solutions have evolved to address the growing need to monitor your stack from the bottom to the top.</p>
<p>From an operations point of view, infrastructure monitoring tools collect metrics about the host or container, such as CPU load, available memory and network I/O.</p>
<p>The default monitoring stack is the 3-pronged open source approach of, Grafana, Alertmanager, and Prometheus.</p>
<p><strong><em>Prometheus</em></strong> gives you finely grained metrics at a huge scale. With the right configuration, Prometheus can handle millions of time series.</p>
<p><strong><em>Grafana</em></strong> can visualize the data being scraped by Prometheus. Grafana comes with pre-built dashboards for typical use cases, or you can create your own custom ones.</p>
<p><strong><em>Alertmanager</em></strong> forwards alerts to a service such as Slack or another webhook . Alertmanager can use metadata to classify alerts into groups such as errors, notifications, etc.</p>
<p>The Grafana-Alertmanager-Prometheus monitoring stack provides a highly configurable, open source option to monitor Kubernetes workloads.</p>
<p><img alt="monitoring-arch" src="../images/monitoring-arch.png" /></p></section><section class="print-page" id="lab005-lab005-2"><h1 id="lab005-lab005-2-connect-to-ocp-and-authenticate">Connect to OCP and Authenticate<a class="headerlink" href="#lab005-lab005-2-connect-to-ocp-and-authenticate" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>In your virtual machine desktop, <strong>open a Firefox web browser</strong>.</p>
</li>
<li>
<p>In the browser, <strong>navigate to the OpenShift on IBM Z console</strong> at the following address: <a href="https://console-openshift-console.apps.atsocppa.dmz/">https://console-openshift-console.apps.atsocppa.dmz/</a>.</p>
<details class="note" open="open"><summary>Note</summary><p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being “self-signed” and not yet recognized.</p>
<p>Accept the challenge to continue by <strong>clicking Advanced</strong> and then <strong>clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe)</strong>.</p>
<p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p>
</details>
<details><summary>Expand for screenshot</summary><p><img alt="security-accept.png" src="../images/security-accept.png" />
<img alt="security-accept-2.png" src="../images/security-accept-2.png" /></p>
</details>
<p>You will now see the OpenShift console login page.</p>
<p><img alt="openshift-console-login" src="../images/openshift-console-login.png" /></p>
</li>
<li>
<p><strong>Log in with the OpenShift credentials</strong> provided to you on the <a href="#lab-assignments">Lab Assignments</a> page.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Your OpenShift credentials will be something like the following:</p>
<ul>
<li>
<p>Username: userNN (where NN is your user number)</p>
</li>
<li>
<p>Password: p@ssw0rd</p>
</li>
</ul>
</div>
</li>
</ol></section><section class="print-page" id="lab005-lab005-3"><h1 id="lab005-lab005-3-using-openshifft-metrics-prometheus">Using OpenShifft Metrics (Prometheus)<a class="headerlink" href="#lab005-lab005-3-using-openshifft-metrics-prometheus" title="Permanent link">&para;</a></h1>
<p>OpenShift provides a web interface to <em>Prometheus</em>, which enables you to run Prometheus Query Language (PromQL) queries and visualize the metrics on a plot. This functionality provides an extensive overview of the cluster state and helps to troubleshoot problems.</p>
<ol>
<li>
<p>In the OpenShift console, <strong>switch to the Administrator perspective</strong> if you are not already on it.</p>
<p><img alt="administrator-perspective" src="../images/administrator-perspective.png" /></p>
</li>
<li>
<p>In the menu bar on the left side of the page, <strong>click Monitoring and then Metrics</strong>.</p>
<p><img alt="menu-metrics" src="../images/menu-metrics.png" /></p>
<p>You will be taken to a Prometheus interface within the OpenShift console.</p>
<p><img alt="empty-metrics" src="../images/empty-metrics.png" /></p>
<p>Once you enter a query, the graph will populate.</p>
</li>
<li>
<p><strong>Click the Insert Metric at Cursor dropdown and enter the following string in the new query bar</strong>:</p>
<div class="highlight"><pre><span></span><code>namespace:container_memory_usage_bytes:sum
</code></pre></div>
<p><img alt="insert-metric-1" src="../images/insert-metric-1.png" /></p>
</li>
<li>
<p><strong>Click the associated query result that is returned</strong>.</p>
<p><img alt="insert-metric-2" src="../images/insert-metric-2.png" /></p>
<p>The string will populate the query text box.</p>
</li>
<li>
<p><strong>Click the blue "Run Queries" button</strong>.</p>
<p><img alt="memory-usage" src="../images/memory-usage.png" /></p>
<p>The graph should now display the memory usage over time for each namespace.</p>
</li>
<li>
<p><strong>Scroll down the page</strong> to the table displaying each namespace and its memory usage in bytes.</p>
<p><img alt="memory-table" src="../images/memory-table.png" /></p>
<details class="note" open="open"><summary>Note</summary><p>You table will look different depending on what work is being done in the OpenShift cluster at the time.</p>
</details>
<p>OpenShift passes around a massive amount of data to run itself and the applications running on top of it. Prometheus is an extremely powerful data source that can return results for millions of time strings with extremely granular precision.</p>
<p>Because of OpenShift’s vast data production and Prometheus’ ability to process it, certain queries can produce simply too much data to be useful. Because Prometheus makes use of labels, we can use these labels to filter data to make better sense of it.</p>
</li>
<li>
<p><strong>Modify your query to the following</strong>:</p>
<div class="highlight"><pre><span></span><code>namespace:container_memory_usage_bytes:sum{namespace=&quot;userNN-project&quot;}
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Make sure you change the one instance of <code>NN</code> to your user number.</p>
<p>Also, notice that they are squiggly brackets <code>{}</code> in the query, not regular parentheses.</p>
</div>
</li>
<li>
<p><strong>Click Run Queries</strong></p>
<p><img alt="memory-namespaced" src="../images/memory-namespaced.png" /></p>
<p>Your graph is now displaying the memory usage over time for your own project. If you see a “No datapoints found” message, select a longer timeframe using the dropdown menu in the top left of the graph.</p>
<details class="note" open="open"><summary>Note</summary><p>If you skipped ahead to this lab without completing the others, it’s possible that your project has not had workload deployed in it for more than the maximum time frame. If this is the case, run a simple application in your project, and you will see the data start to populate (refer to <a href="#lab001-lab001-1">Exploring the OpenShift Console</a> for help with this.)</p>
</details>
</li>
</ol>
<p>As you might have noticed, working directly with Prometheus can be tedious and requires specific PromQL queries that aren’t the easiest to work with. That’s why people typically use Prometheus for its <em>data source</em> functionality, and then move to Grafana for the <em>data visualization</em>.</p></section><section class="print-page" id="lab005-lab005-4"><h1 id="lab005-lab005-4-using-the-in-browser-grafana-dashboards">Using the In-Browser Grafana Dashboards<a class="headerlink" href="#lab005-lab005-4-using-the-in-browser-grafana-dashboards" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p><strong>From the OpenShift menu, navigate to Monitoring -&gt; Dashboards</strong>.</p>
<p><img alt="menu-dashboards" src="../images/menu-dashboards.png" /></p>
<p>This takes you to an in-browser user interface for the Grafana monitoring solution. By default, there are various preconfigured dashboards for common use cases.</p>
<p><img alt="default-in-browser" src="../images/default-in-browser.png" /></p>
</li>
<li>
<p><strong>Click the "Dashboard" dropdown in the top-left of the page, and select another that is of interest to you</strong>.</p>
</li>
</ol></section><section class="print-page" id="lab005-lab005-5"><h1 id="lab005-lab005-5-connect-to-grafana">Connect to Grafana<a class="headerlink" href="#lab005-lab005-5-connect-to-grafana" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>To utilize further Grafana functions, <strong>navigate to the Grafana UI at the following address</strong>.</p>
<p><a href="https://grafana-openshift-monitoring.apps.atsocppa.dmz/">https://grafana-openshift-monitoring.apps.atsocppa.dmz/</a></p>
<details class="information"><summary>Expand for More Information</summary><p>Where is this URL coming from? It is exposed service (or route) for the Grafana service. You could open a terminal and run the following command to find the URLs to Prometheus, Grafana, and Alertmanager:</p>
<div class="highlight"><pre><span></span><code>root # ===&gt; oc -n openshift-monitoring get routes
NAME                HOST/PORT                              
alertmanager-main   alertmanager-main-openshift-monitoring.apps.atsocppa.dmz
grafana             grafana-openshift-monitoring.apps.atsocppa.dmz
prometheus-k8s      prometheus-k8s-openshift-monitoring.apps.atsocppa.dmz
</code></pre></div>
</details>
<details class="information" open="open"><summary>Information</summary><p>You might see a security challenge if the cluster has not yet been accessed from your workstation. Accept the challenge to continue.</p>
</details>
<p>You should now see login page prompting your OpenShift credentials.</p>
</li>
<li>
<p><strong>Log into Grafana using the your OpenShift credentials</strong>.</p>
<ul>
<li>Username: userNN</li>
<li>Password: p@ssw0rd</li>
</ul>
<p>Notice that the credentials you use to log into Grafana are the same as those you use to log into OpenShift itself. OpenShift’s role-bases access control (RBAC) functionality extends to the monitoring stack, so administrators can control who can see this part of the environment.</p>
</li>
</ol></section><section class="print-page" id="lab005-lab005-6"><h1 id="lab005-lab005-6-using-grafana-dashboards">Using Grafana Dashboards<a class="headerlink" href="#lab005-lab005-6-using-grafana-dashboards" title="Permanent link">&para;</a></h1>
<p>Once logged into Grafana, you’ll be taken to the Home Dashboard from which you can navigate to your starred or recently viewed dashboards. You can also install various types of plugins from the official Grafana list and also from third-party sources from this page.</p>
<p><img alt="grafana-home" src="../images/grafana-home.png" /></p>
<ol>
<li>
<p><strong>Click the Home dropdown in the top-left corner, and expand the Default dashboards if they aren’t already visible</strong>.</p>
<p><img alt="dashboard-dropdown" src="../images/dashboard-dropdown.png" /></p>
<p>A list of the recent and pre-installed dashboards will pop up. You may or may not see any recent dashboards, depending on previous usage of your userNN credentials. Notice that you can search dashboards by keyword in the search bar up top, or filter by labels on the right side.</p>
</li>
<li>
<p><strong>Click the Kubernetes / Compute Resources / Cluster link</strong> in the General tab.</p>
<p><img alt="kubernetes-graph" src="../images/kubernetes-graph.png" /></p>
<p>You will see a dashboard populated with information related to the cluster’s compute resources such as CPU and memory utilization. This dashboard displays CPU usage and CPU quota/memory requests by namespace.</p>
<p><img alt="grafana-options" src="../images/grafana-options.png" /></p>
</li>
<li>
<p><strong>Click the CPU Usage dropdown above the first graph in this dashboard and click View</strong>.</p>
<details class="information" open="open"><summary>Information</summary><p>Alternatively, you can click on the CPU Usage graph to activate it, and hit the <code>V</code> key on your keyboard.</p>
</details>
<p>This will bring up a full screen view of the graph to more easily see details.</p>
<p><img alt="graph-details-1" src="../images/graph-details-1.png" /></p>
</li>
<li>
<p><strong>Hover your cursor over the graph various points to see details for a certain namespace at a specific point in time</strong>.</p>
<p><img alt="graph-details-2" src="../images/graph-details-2.png" /></p>
<p>This still might be difficult to target a specific namespace, especially for the namespaces that aren’t using much CPU.</p>
</li>
<li>
<p><strong>Click a namespaces in the chart’s legend</strong>, such as <code>atg-zoscb</code> or <code>openshift-apiserver</code>.</p>
<p><img alt="legend" src="../images/legend.png" /></p>
</li>
<li>
<p><strong>Hold the Shift key and click a few more namespaces</strong>.</p>
<p><img alt="legend-2" src="../images/legend-2.png" /></p>
<p>This will display only the CPU usage for the few namespaces you selected.</p>
</li>
<li>
<p><strong>Click the Share dashboard button in the top right of the page</strong>.</p>
<p><img alt="share-link" src="../images/share-link.png" /></p>
<p>From here, you can share a snapshot of the graph either internally or externally. When creating a snapshot to share externally, sensitive data will be stripped.</p>
<details class="note" open="open"><summary>Note</summary><p>If you try to share or export a graph here, you will find that it’s unsuccessful.</p>
</details>
<p>The userNN profiles have administrator-viewer credentials, so you are limited to the features you can actually change. A profile with full cluster administrator authority would have more access to Grafana functions such as sharing graphs and snapshots, creating their own custom dashboards, editing and saving pre-built dashboards, and installing various plugins and other tools that extend Grafana’s built-in features.</p>
</li>
<li>
<p><strong>Close this browser window when you are ready to move on</strong>.</p>
</li>
</ol></section><section class="print-page" id="lab005-lab005-7"><h1 id="lab005-lab005-7-using-openshift-alerts-with-alertmanager">Using OpenShift Alerts with Alertmanager<a class="headerlink" href="#lab005-lab005-7-using-openshift-alerts-with-alertmanager" title="Permanent link">&para;</a></h1>
<p>Alerting with Prometheus is separated into two parts. Alerting rules in <em>Prometheus</em> send alerts to <em>Alertmanager</em>. Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email or chat platforms like Slack.</p>
<p><img alt="alert-stack" src="../images/alert-stack.png" /></p>
<p>An example rules file with an alert would be:</p>
<div class="highlight"><pre><span></span><code><span class="nt">groups</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">example</span>
  <span class="nt">rules</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">alert</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">HighRequestLatency</span>
    <span class="nt">expr</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">job:request_latency_seconds:mean5m{job=&quot;myjob&quot;} &gt; 0.5</span>
    <span class="nt">for</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10m</span>
    <span class="nt">labels</span><span class="p">:</span>
      <span class="nt">severity</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">page</span>
    <span class="nt">annotations</span><span class="p">:</span>
      <span class="nt">summary</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">High request latency</span>
</code></pre></div>
<p>The optional <code>for</code> clause causes Prometheus to wait for a certain duration between first encountering a new expression output vector element and counting an alert as firing for this element. In this case, Prometheus will check that the alert continues to be active during each evaluation for 10 minutes before firing the alert. Elements that are active, but not firing yet, are in the pending state.</p>
<p>The <code>labels</code> clause allows specifying a set of additional labels to be attached to the alert. Any existing conflicting labels will be overwritten.</p>
<p>The <code>annotations</code> clause specifies a set of informational labels that can be used to store longer additional information such as alert descriptions or runbook links.</p>
<ol>
<li>
<p>In the menu bar on the left side of the OpenShift console, <strong>click Monitoring and then Alerting</strong>.</p>
<p>You will be taken to an Alertmanager interface within the OpenShift console.</p>
<p><img alt="alertmanager" src="../images/alertmanager.png" /></p>
</li>
<li>
<p><strong>Click the Alerting Rules tab</strong> to see the 100+ alerts that are not currently firing (hopefully!)</p>
<p><img alt="alerting-rules" src="../images/alerting-rules.png" /></p>
<p>These alerts come pre-built with the monitoring stack, and they will start firing if triggered. This list includes alerts for critical operators going down, pods crash-looping, nodes being unreachable, and many more. Feel free to look through them.</p>
</li>
</ol></section><h1 class='nav-section-title-end'>Ended: Monitoring, Metering, and Metrics</h1>
                        <h2 class='nav-section-title' id='section-using-persistent-storage-with-mongodb-and-nodejs'>
                            Using Persistent Storage with MongoDB and NodeJS <a class='headerlink' href='#section-using-persistent-storage-with-mongodb-and-nodejs' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab006-lab006-1"><h1 id="lab006-lab006-1-using-persistent-storage-mongodb-and-nodejs">Using Persistent Storage - MongoDB and NodeJS<a class="headerlink" href="#lab006-lab006-1-using-persistent-storage-mongodb-and-nodejs" title="Permanent link">&para;</a></h1>
<p>In production Kubernetes clusters, applications need to write data to storage where it will persist even if the application pods go down. In this lab, we’ll see how that’s done using Persistent Volumes and Persistent Volume Claims.</p>
<p>OpenShift on IBM Z supports various types of persistent storage, including Spectrum Scale, OpenShift Container Storage, and NFS, which is what this cluster uses. Before the start of the workshop, persistent volumes were defined in OpenShift, more than enough for one per user. Each persistent volume definition maps to the NFS server.</p>
<p><img alt="nfs-arch" src="../images/nfs-arch.png" /></p>
<p>In this lab, you will deploy an application consisting of two components, a containerized <em>Node.js web application</em> and a containerized <em>MongoDB</em> instance, which you will back with persistent storage. Using the Node.js web application, you will be able to query the database, as well as insert new data into it.</p>
<p><img alt="app-arch" src="../images/app-arch.png" /></p>
<p>To deploy the <em>Node.js application</em>, you will build and run the container from a Dockerfile residing in a GitHub repository.</p>
<p>To deploy <em>MongoDB</em>, you will pull a MongoDB image from <quay.io> and run it. The image in quay.io is the official MongoDB container image pulled from Docker Hub and moved to the Quay registry. This was done simply because Dockerhub has rate limits on pull requests from their public repository.</p></section><section class="print-page" id="lab006-lab006-2"><h1 id="lab006-lab006-2-connect-to-ocp-and-authenticate">Connect to OCP and Authenticate<a class="headerlink" href="#lab006-lab006-2-connect-to-ocp-and-authenticate" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>In your virtual machine desktop, <strong>open a Firefox web browser</strong>.</p>
</li>
<li>
<p>In the browser, <strong>navigate to the OpenShift on IBM Z console</strong> at the following address: <a href="https://console-openshift-console.apps.atsocppa.dmz/">https://console-openshift-console.apps.atsocppa.dmz/</a>.</p>
<details class="note" open="open"><summary>Note</summary><p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being “self-signed” and not yet recognized.</p>
<p>Accept the challenge to continue by <strong>clicking Advanced</strong> and then <strong>clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe)</strong>.</p>
<p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p>
</details>
<details><summary>Expand for screenshot</summary><p><img alt="security-accept.png" src="../images/security-accept.png" />
<img alt="security-accept-2.png" src="../images/security-accept-2.png" /></p>
</details>
<p>You will now see the OpenShift console login page.</p>
<p><img alt="openshift-console-login" src="../images/openshift-console-login.png" /></p>
</li>
<li>
<p>Log in with the OpenShift credentials provided to you on the <a href="#lab-assignments">Lab Assignments</a> page.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Your OpenShift credentials will be something like the following:</p>
<ul>
<li>
<p>Username: userNN (where NN is your user number)</p>
</li>
<li>
<p>Password: p@ssw0rd</p>
</li>
</ul>
</div>
</li>
</ol></section><section class="print-page" id="lab006-lab006-3"><h1 id="lab006-lab006-3-create-a-persistentvolumeclaim">Create a PersistentVolumeClaim<a class="headerlink" href="#lab006-lab006-3-create-a-persistentvolumeclaim" title="Permanent link">&para;</a></h1>
<p>As described in a previous section, a PersistentVolume has been already been predefined for each lab user. Next you will create a PersistentVolumeClaim that will bind to one of the available PersistentVolumes.</p>
<ol>
<li>
<p><strong>Change to the Administrator perspective</strong>, if not already there.</p>
</li>
<li>
<p><strong>Navigate to the Projects page</strong>. You can find it in the Menu, under Home -&gt; Projects.</p>
<p><img alt="home-projects" src="../images/home-projects.png" /></p>
</li>
<li>
<p><strong>Find and click on your userNN-project</strong>.</p>
</li>
<li>
<p>Under the menu button, <strong>click Storage -&gt; Persistent Volume Claims</strong>.</p>
</li>
<li>
<p><strong>Click “Create Persistent Volume Claim”</strong>.</p>
<p><img alt="create-pvc" src="../images/create-pvc.png" /></p>
<p>The Create Persistent Volume Claim form has four fields, and you’ll need to manually change each.</p>
<ul>
<li>
<p><strong>For Storage Class, select rootsquash-nfs</strong></p>
</li>
<li>
<p><strong>For Persistent Volume Claim Name, change the value to pvc-userNN</strong> (Replacing NN with your user #).</p>
</li>
<li>
<p><strong>For Access Mode, select Shared Access (RWX</strong>).</p>
</li>
<li>
<p><strong>For Size, change the value to 2 Gi</strong>.</p>
</li>
</ul>
<p>Your form should look like the following:</p>
<p><img alt="create-pvc-2" src="../images/create-pvc-2.png" /></p>
</li>
<li>
<p><strong>Click the create button</strong>.</p>
<p>You’ll be brought to the Overview for your newly created Persistent Volume Claim. The status of your claim should be <em>Bound</em>.</p>
<p><img alt="bound-pvc" src="../images/bound-pvc.png" /></p>
<details class="note" open="open"><summary>Note</summary><p>If your PersistentVolumeClaim does not bind almost immediately to a PersistentVolume, you likely did not fill out the fields as described above. You can delete your persistent volume claim and try again by clicking on the Actions dropdown and selecting Delete Persistent Volume Claim.</p>
</details>
</li>
</ol></section><section class="print-page" id="lab006-lab006-4"><h1 id="lab006-lab006-4-deploy-mongodb-from-a-container-image">Deploy MongoDB from a Container Image<a class="headerlink" href="#lab006-lab006-4-deploy-mongodb-from-a-container-image" title="Permanent link">&para;</a></h1>
<p>In this section, you will be deploying a container using a MongoDB container image from quay.io. A container image holds a set of software that is ready to run, while a container is a running instance of a container image. Images can be hosted in registries, such as the quay.io registry, Docker Hub, or a private registry of your own.</p>
<ol>
<li>
<p><strong>Toggle to the Developer Perspective and ensure you are in the correct userNN-project</strong>.</p>
<p><img alt="developer-userNN" src="../images/developer-userNN.png" /></p>
</li>
<li>
<p><strong>Click Add+ from the left-hand menu</strong>.</p>
</li>
<li>
<p><strong>Click the Container Images option</strong>.</p>
<p>This brings up a new page which prompts you for an image name and further configurable parameters further down the page. Only the image name is required, and the rest will automatically populate for you.</p>
</li>
<li>
<p><strong>In the search bar for Image Name from external registry, type</strong> <code>quay.io/mmondics/mongo</code>.</p>
<p><img alt="mongo-search" src="../images/mongo-search.png" /></p>
<p>A green check mark and a “validated” message will appear in the search bar, indicating that the MongoDB image has been found and validated in Quay.</p>
<details class="important" open="open"><summary><img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /> Important <img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /></summary><p>The fields below will automatically populate, but <strong><em>it is imperative that you change the Name field for your MongoDB service</em></strong>, or else the Node.js app will not be able to find and connect to the database.</p>
</details>
</li>
<li>
<p><strong>For Application Name, leave the default value</strong>.</p>
</li>
<li>
<p><strong><em>Replace the value of the Name field with mongodb</em></strong></p>
</li>
<li>
<p><strong>Leave Deployment and Create a Route to the Application checked</strong>.</p>
<p><img alt="mongo-fields" src="../images/mongo-fields.png" /></p>
</li>
<li>
<p><strong>Click the Create Button</strong>.</p>
<p>You will now be taken to the Topology view, where you will see an icon for your new MongoDB deployment.</p>
</li>
<li>
<p><strong>Click the icon for the mongodb deployment</strong>.</p>
<p>This will bring up a window on the right-hand side of the screen with information about your deployment.</p>
</li>
<li>
<p><strong>Select the Details tab</strong>, if not already selected.</p>
<p><img alt="mongo-details" src="../images/mongo-details.png" /></p>
<p>Depending on how quickly you clicked the icon, it will display either 1 pod, or 0 scaling to 1. If it has not scaled up to 1 pod yet, it will after a few seconds. However, we’re going to be adding and removing storage, so we will kill the pod once it comes up.</p>
</li>
<li>
<p><strong>Click the Down Arrow</strong> to reduce the pod count to zero.</p>
<p><img alt="mongo-down" src="../images/mongo-down.png" /></p>
<p>When a MongoDB pod is created, two storage volumes are attached to it. Let’s examine those.</p>
</li>
<li>
<p><strong>Click the mongodb deployment in the right-side window</strong>.</p>
<p><img alt="mongodb-deployment" src="../images/mongodb-deployment.png" /></p>
</li>
<li>
<p><strong>Once on the Deployment Details page, scroll down Volumes section</strong>.</p>
<p>Upon its creation, the pod created two volumes, mongodb-1 and mongodb-2. By default, MongoDB stores data in the <code>/data/db</code> directory, which is where the mongodb-2 volume has been mounted. This volume is not persistent. If the pod gets deleted, all of the stored data will be lost.</p>
<p>To make your MongoDB data persistent, you are going to delete mongodb-2 and instead mount your persistent volume claim at /data/db.</p>
</li>
<li>
<p><strong>Click the three dots for the mongodb-2 volume</strong>.</p>
</li>
<li>
<p><strong>Click Remove Volume</strong>.</p>
<p><img alt="remove-volume" src="../images/remove-volume.png" /></p>
<p>Now, you’ll add persistent storage, mounting the volume at /data/db.</p>
</li>
<li>
<p>Still on the Deployment Details page, <strong>scroll back up to the top and click the Actions dropdown</strong>.</p>
</li>
<li>
<p><strong>Click Add Storage</strong>.</p>
<p><img alt="add-storage" src="../images/add-storage.png" /></p>
<p>To add storage to your MongoDB deployment, you will need to fill out a couple of fields.</p>
</li>
<li>
<p><em>Use Existing Claim should already be selected</em>.</p>
</li>
<li>
<p><strong>From the Select Claim dropdown menu, select pvc-userNN</strong>.</p>
</li>
<li>
<p><strong>For Mount Path, enter</strong> <code>/data/db</code>.</p>
</li>
<li>
<p><strong>For Subpath, enter your userNN</strong>, where NN is your user number.</p>
<p><img alt="add-storage-1" src="../images/add-storage-1.png" /></p>
<p><img alt="add-storage-2" src="../images/add-storage-2.png" /></p>
</li>
<li>
<p><strong>Click the Save button</strong>.</p>
<p>You will now be returned to the Deployment Details page. In the same way that you reduced the pod count to zero, you will now bring it back up to one.</p>
</li>
<li>
<p><strong>Click the Up Arrow to increase the pod count to one</strong>.</p>
<p><img alt="scale-mongo-up" src="../images/scale-mongo-up.png" /></p>
</li>
<li>
<p><strong>Scroll Down to the section labeled Volumes</strong>, and you’ll see the persistent volume now mounted at /data/db</p>
<p><img alt="mounted-pvc1" src="../images/mounted-pvc.png" /></p>
<p>Now you’re ready to deploy the Node.js web application.</p>
</li>
</ol></section><section class="print-page" id="lab006-lab006-5"><h1 id="lab006-lab006-5-deploy-nodejs-application-from-a-dockerfile">Deploy Node.js Application from a Dockerfile<a class="headerlink" href="#lab006-lab006-5-deploy-nodejs-application-from-a-dockerfile" title="Permanent link">&para;</a></h1>
<p>In this portion of the lab, you will deploy a Node.js web application created by the ATG. You will be building your deployment from a Dockerfile residing in a GitHub repository. Through the web application, you will be able to insert data into and query the MongoDB database you just created. For this lab the database will store sample name and email pairs displayed as a list of user information in the web UI.</p>
<p>First, you need to deploy the application.</p>
<ol>
<li>
<p>Staying in the Developer Perspective, <strong>click +Add from the left-side menu</strong>.</p>
</li>
<li>
<p><strong>Click the From Dockerfile tile</strong>.</p>
<p>Fill out the form as follows:</p>
</li>
<li>
<p><strong>In the Git Repo URL Field, enter</strong>:</p>
<div class="highlight"><pre><span></span><code>https://github.com/mmondics/mongodb-app
</code></pre></div>
<p>You should see a “Validated” message below the URL field.</p>
</li>
<li>
<p><strong>Ensure that the value in the Application field is mongo-app</strong>.</p>
</li>
<li>
<p><strong>Replace the value in the Name field with nodejs-app</strong>.</p>
</li>
<li>
<p><strong>Leave Deployment checked</strong>.</p>
<p><img alt="import-dockerfile" src="../images/import-dockerfile.png" /></p>
<p><img alt="import-dockerfile-2" src="../images/import-dockerfile-2.png" /></p>
</li>
<li>
<p><strong>Click the Create button</strong>.  </p>
<p>Your Node.js application will now pull the Dockerfile from GitHub and begin its build. You will be returned to the Topology view. You should see mongodb and nodejs-app grouped together in mongo-app. When the build is complete, you will see a blue ring form around nodejs-app. You can also check its status by clicking on the nodejs-app icon and examining the Details panel.</p>
</li>
<li>
<p>Once the build is complete, <strong>click the Open URL button at the top right of the nodejs-app icon</strong>.</p>
<p><img alt="open-url" src="../images/open-url.png" /></p>
<p>This button is simply a shortcut to the route that was created as part of the deployment.</p>
<p>You are brought to the following landing page for your Node.js application:</p>
<p><img alt="node-landing" src="../images/node-landing.png" /></p>
</li>
</ol>
<p>In the next section, you will insert data into and query your MongoDB database.</p></section><section class="print-page" id="lab006-lab006-6"><h1 id="lab006-lab006-6-interacting-with-mongodb-from-nodejs-web-application">Interacting with MongoDB from Node.js Web Application<a class="headerlink" href="#lab006-lab006-6-interacting-with-mongodb-from-nodejs-web-application" title="Permanent link">&para;</a></h1>
<p>You should be on the “Hello World” landing page of your Node.js web application. If you have moved off of this screen, refer to the previous section for instructions on how to access the application.</p>
<p>Since your application has not yet been used, the MongoDB database of user data will be empty. We can use the Node.js frontend application to insert data into the linked MongoDB pod and the persistent storage backing it.</p>
<ol>
<li>
<p><strong>Click the Add a New User button</strong>.</p>
<p><img alt="node-landing-2" src="../images/node-landing-2.png" /></p>
<p>You will be brought to a new page titled Add New User.</p>
</li>
<li>
<p><strong>Enter a sample username and email and click Submit</strong>.</p>
<p><img alt="add-new-user" src="../images/add-new-user.png" /></p>
<p>You will be brought to a page titled User List which displays the entire contents of your database. Feel free to add additional users.</p>
<p><img alt="user-list" src="../images/user-list.png" /></p>
<p>The data you just entered through the NodeJS web application is now stored in a MongoDB database backed by persistent storage on our NFS server. Now, let’s test that our data will persist if we simulate a database crash by deleting our MongoDB pod.</p>
</li>
<li>
<p><strong>Return to the OpenShift Console Developer Perspective and navigate to the Topology View</strong>.</p>
</li>
<li>
<p><strong>Click the mongodb deployment</strong>.</p>
</li>
<li>
<p><strong>Click the down arrow to reduce the pod count to zero, terminating the MongoDB pod</strong>.</p>
<p><img alt="mongo-down" src="../images/mongo-down.png" /></p>
</li>
<li>
<p><strong>Return to the web application and refresh the page</strong>.</p>
<p>The page will not connect, and if you wait long enough you will get a 504 Gateway Time-out error as the database no longer exists and no connection can be made.</p>
</li>
<li>
<p><strong>Back in OpenShift, click the up arrow to increase the pod count back to 1</strong>.</p>
</li>
<li>
<p><strong>Return to the web application and refresh the page again</strong>.</p>
<p>Your data still exists, even though the MongoDB pod was terminated and replaced by a completely new one.</p>
<p>In this section, a new MongoDB pod was created. Since you mounted NFS persistent storage at /data/db in the original MongoDB pod, your data persisted even when the original MongoDB pod was deleted and replaced with a new one. Without persistent storage, the new MongoDB pod would have contained an empty database.</p>
</li>
</ol></section><section class="print-page" id="lab006-lab006-7"><h1 id="lab006-lab006-7-cleaning-up">Cleaning Up<a class="headerlink" href="#lab006-lab006-7-cleaning-up" title="Permanent link">&para;</a></h1>
<p>There is no easy way to delete all of these objects from the OpenShift console. This is a much easier task in the OpenShift command line.</p>
<ol>
<li>
<p>In the OpenShift CLI, <strong>make sure you are in your own project</strong> (i.e. userNN-project) <strong>and run the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc delete all --all
</code></pre></div>
<details class="note" open="open"><summary>Note</summary><p>If you are not connected to the OpenShift command line, refer to <a href="#lab002-lab002-1">Using the OpenShift Command Line</a>.</p>
</details>
</li>
</ol>
<p>This will delete most of the objects in your project, but not the Persistent Volume Claim you created.</p>
<ol>
<li>
<p><strong>To delete the PVC, run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc delete pvc/pvc-userNN
</code></pre></div>
<p>Where <code>NN</code> is your user number.</p>
</li>
</ol></section><h1 class='nav-section-title-end'>Ended: Using Persistent Storage with MongoDB and NodeJS</h1>
                        <h2 class='nav-section-title' id='section-using-the-z-os-cloud-broker'>
                            Using the z/OS Cloud Broker <a class='headerlink' href='#section-using-the-z-os-cloud-broker' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab003-lab003-1"><h1 id="lab003-lab003-1-the-zos-cloud-broker">The z/OS Cloud Broker<a class="headerlink" href="#lab003-lab003-1-the-zos-cloud-broker" title="Permanent link">&para;</a></h1>
<p>The <em>IBM z/OS Cloud Broker</em> is an IBM offering that connects z/OS services running on an IBM Z backend to a frontend container platform providing self-service access and consumption of these services to developers. This allows developers to provision their own z/OS resources directly from the OpenShift console – without the need for z/OS skills or direct access.</p>
<p><img alt="zoscb-arch.png" src="../images/zoscb-arch.png" /></p>
<p>The services available for the z/OS Cloud Broker to expose into OpenShift are:</p>
<table>
<thead>
<tr>
<th>z/OS Connect EE</th>
<th>Db2</th>
<th>CICS</th>
<th>IMS</th>
<th>MQ</th>
<th>WLP</th>
</tr>
</thead>
<tbody>
<tr>
<td>Provision / deprovision z/OS Connect   Servers.     Start/Stop z/OS Connect Servers</td>
<td>Provision / deprovision Db2   subsystems, schemas, and databases +     snapshot / restore</td>
<td>Provision / deprovision CICS regions</td>
<td>Provision / deprovision IMS TM/DB   systems</td>
<td>Provision / deprovision MQ Queue   Manager subsystem</td>
<td>WebSphere Liberty Profile server provisioning, start/stop   server</td>
</tr>
</tbody>
</table>
<p>In this lab, you will be provisioning a WebSphere Liberty Profile (WLP) server on z/OS using the z/OS Cloud Broker on OpenShift. </p></section><section class="print-page" id="lab003-lab003-2"><h1 id="lab003-lab003-2-connect-to-openshift-and-authenticate">Connect to OpenShift and Authenticate<a class="headerlink" href="#lab003-lab003-2-connect-to-openshift-and-authenticate" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p>In your virtual machine desktop, <strong>open a Firefox web browser</strong>.</p>
</li>
<li>
<p>In the browser, <strong>navigate to the OpenShift on IBM Z console</strong> at the following address: <a href="https://console-openshift-console.apps.atsocppa.dmz/">https://console-openshift-console.apps.atsocppa.dmz/</a>.</p>
<details class="note" open="open"><summary>Note</summary><p>You will receive a security challenge if the cluster has not yet been accessed from your browser. This is due to the default SSL certificate being “self-signed” and not yet recognized.</p>
<p>Accept the challenge to continue by <strong>clicking Advanced</strong> and then <strong>clicking Proceed to console-openshift-console.apps.atsocppa.dmz (unsafe)</strong>.</p>
<p>You will likely need to do this twice due to how OpenShift reroutes Oauth requests. </p>
</details>
<details><summary>Expand for screenshot</summary><p><img alt="security-accept.png" src="../images/security-accept.png" />
<img alt="security-accept-2.png" src="../images/security-accept-2.png" /></p>
</details>
<p>You will now see the OpenShift console login page.</p>
<p><img alt="openshift-console-login" src="../images/openshift-console-login.png" /></p>
</li>
<li>
<p><strong>Log in with the OpenShift credentials provided to you</strong> on the <a href="#lab-assignments">Lab Assignments</a> page.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Your OpenShift credentials will be something like the following:</p>
<ul>
<li>
<p>Username: userNN (where NN is your user number)</p>
</li>
<li>
<p>Password: p@ssw0rd</p>
</li>
</ul>
</div>
</li>
</ol></section><section class="print-page" id="lab003-lab003-3"><h1 id="lab003-lab003-3-deploy-liberty-for-zos-using-the-zos-cloud-broker">Deploy Liberty for z/OS Using the z/OS Cloud Broker<a class="headerlink" href="#lab003-lab003-3-deploy-liberty-for-zos-using-the-zos-cloud-broker" title="Permanent link">&para;</a></h1>
<p>With the z/OS Cloud Broker and OpenShift, provisioning z/OS resources is as easy as clicking on a tile in the OpenShift Developer Catalog.</p>
<ol>
<li>
<p><strong>Enter the Developer Perspective</strong>, if you aren’t there already.</p>
</li>
<li>
<p><strong>Make sure that you’re working under the z/OS Cloud Broker project</strong> <code>atg-zoscb</code>.</p>
<div class="admonition important">
<p class="admonition-title"><img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /> Important <img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /></p>
<p><strong><em>Unlike other labs, this lab uses a shared project for all lab attendees. Please pay close attention to naming conventions so you do not end up deleting other attendees’ provisioned services</em></strong>.</p>
</div>
<p><img alt="zoscb-project" src="../images/zoscb-project.png" /></p>
</li>
<li>
<p><strong>Click the +Add button in the left-side menu</strong>.</p>
</li>
<li>
<p><strong>Click the All Services option under the Developer Catalog section</strong>.</p>
<p><img alt="from-catalog" src="../images/from-catalog.png" /></p>
</li>
<li>
<p><strong>Search the catalog for Liberty for z/OS</strong> and <strong>click on it</strong>.</p>
</li>
<li>
<p><strong>Click the Create button</strong> at the bottom of the page.</p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>This Liberty service does not live inside this OpenShift cluster. It is, in fact, a template for a z/OS Liberty instance that z/OSMF has found and displayed. When you provision an instance, it will spin up the service in a completely different z/OS LPAR separate from the Linux LPAR where this OpenShift cluster is running.</p>
</div>
</li>
<li>
<p>All of the required fields will automatically populate for you, but <strong>rename the wlp service to</strong> <code>userNN-wlp</code> where <code>NN</code> is your user number.</p>
<div class="admonition important">
<p class="admonition-title"><img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /> Important <img alt="⚠" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/26a0.svg" title=":warning:" /></p>
<p>Please double check that you have correctly typed your user number for userNN. Remember that you are using a shared project for this lab, and nothing is stopping you from interfering with another lab participant's provisioned service if you use the wrong name.</p>
</div>
<p><img alt="create-wlp" src="../images/create-wlp.png" /></p>
</li>
<li>
<p><strong>Click the create button</strong>.</p>
<p>You will be brought to the <em>topology</em> page. After you click create, you will need to navigate to the <em>service instance</em> page:</p>
</li>
<li>
<p><strong>Switch to the Administrator Perspective -&gt; Operators in the menu bar -&gt; Installed Operators -&gt; Liberty for z/OS -&gt; Liberty for z/OS tab -&gt; Click on the instance with your user number NN</strong>.</p>
<p>You will end up on a screen that looks like the following:</p>
<p><img alt="wlp-deployed" src="../images/wlp-deployed.png" /></p>
<p>Notice two things:</p>
<ul>
<li>
<p>Depending on how quickly you navigated to this page and how long the WLP instance takes to provision, your status will be either <em>Pending</em> or <em>Succeeded</em>.</p>
</li>
<li>
<p>Once it’s <em>Succeeded</em>, you will have a link to your Dashboard.</p>
</li>
</ul>
<p>OpenShift is telling you that the service is either provisioned in z/OS, or in the process of being provisioned. While you don’t have access to z/OSMF, the following is what you would see over in the z/OSMF console:</p>
<p><img alt="wlp-zosmf" src="../images/wlp-zosmf.png" /></p>
<p>If you want to look at the z/OSMF console, ask an instructor and they will give you a tour.</p>
<p>This service will take a minute or two to provision. Wait until you see the following messages on the service instance page:</p>
<p><img alt="green-succeeded" src="../images/green-succeeded.png" /></p>
<p><img alt="provisioned-successfully" src="../images/provisioned-successfully.png" /></p>
<p>Over on z/OSMF page again, this is what one would see:</p>
<p><img alt="zosmf-success" src="../images/zosmf-success.png" /></p>
<p>And in z/OS itself, the following task is started:</p>
<p><img alt="zos-started" src="../images/zos-started.png" /></p>
<p>You have just successfully provisioned a Liberty instance on z/OS, without leaving the OpenShift console.</p>
</li>
<li>
<p>From the OpenShift WLP instance page, <strong>click the Dashboard URL hyperlink</strong>.</p>
</li>
<li>
<p><strong>Click the Log in with OpenShift button</strong>.</p>
</li>
<li>
<p>You might get a security challenge here. If you do, <strong>make sure that both of the two checkboxes are checked, and click Allow Selected Permissions</strong>.</p>
<p>You will be taken to the dashboard for your z/OS Liberty instance. This page will be referred to as the <em>Dashboard</em> tab.</p>
<p><img alt="dashboard-tab" src="../images/dashboard-tab.png" /></p>
<p>The right side of the page contains information about your WLP service and the z/OS system it’s running on.</p>
<p>The left side of the page contains buttons you can use to perform various actions. You will use a few of them shortly.</p>
</li>
<li>
<p><strong>Scroll to the bottom of the right-hand column, and locate the</strong> <code>IP_ADDRESS</code> <strong>variable</strong>.</p>
<details class="hint"><summary>Hint</summary><p>It's <code>192.168.176.154</code>. That’s the IP address of the z/OS system on which z/OSMF and Liberty for z/OS are hosted.</p>
</details>
</li>
<li>
<p><strong>Scroll up a bit and locate the</strong> <code>HTTP_PORT</code> <strong>variable</strong>. It’s just about in the middle of the column.</p>
<details class="hint"><summary>Hint</summary><p>It's something like <code>9XXX</code>, where <code>XXX</code> will be unique for each user.</p>
</details>
</li>
<li>
<p><strong>Keeping the Dashboard tab open, open a new browser tab</strong>.</p>
</li>
<li>
<p><strong>In the new tab, navigate to</strong> <code>&lt;IP_ADDRESS&gt;:&lt;HTTP_PORT&gt;</code></p>
<details class="hint"><summary>Hint</summary><p>It will look something like <code>192.168.176.154:9XXX</code>, where the <code>XXX</code> is unique for each user.</p>
</details>
<p>You should see the default Liberty homepage. This is the Liberty service you just provisioned on z/OS.</p>
<p><img alt="liberty-homepage" src="../images/liberty-homepage.png" /></p>
</li>
<li>
<p>Staying in this “Welcome to Liberty” tab, <strong>add the following string to the end of the URL</strong>: <code>/CloudTestServlet</code></p>
</li>
<li>
<p><strong>Press enter</strong>.</p>
<p>That will take you to a sample application that was deployed into the Liberty z/OS instance you provisioned.  You will see something like this:</p>
<p><img alt="servlet" src="../images/servlet.png" /></p>
<p>Note the date and timestamp. It should be the current time (in U.S. Eastern time format).</p>
</li>
<li>
<p><strong>Reload the browser tab</strong>.  You should see the time-stamp change.</p>
<p><strong>Do not close this tab</strong>.</p>
</li>
<li>
<p><strong>Return to the Dashboard tab</strong>, which had all the information about the provisioned instance in it.</p>
</li>
</ol></section><section class="print-page" id="lab003-lab003-4"><h1 id="lab003-lab003-4-stop-and-restart-liberty-for-zos-from-ocp">Stop and Restart Liberty for z/OS from OCP<a class="headerlink" href="#lab003-lab003-4-stop-and-restart-liberty-for-zos-from-ocp" title="Permanent link">&para;</a></h1>
<p>In the <em>Dashboard</em> tab, you should see the following on the left side of the screen:</p>
<p><img alt="stop-wlp" src="../images/stop-wlp.png" /></p>
<ol>
<li>
<p><strong>Click the "Run" button that's associated with "Stop"</strong>.</p>
</li>
<li>
<p><strong>Click the “Action History” button above</strong>.</p>
<p>Depending how quickly you click on this button, you’ll see either:</p>
<p><img alt="stop-in-progress" src="../images/stop-in-progress.png" /></p>
<p>If the stop is in progress, or:</p>
<p><img alt="stop-complete" src="../images/stop-complete.png" /></p>
<p>if the stop completed before you looked at the Action History.</p>
<p>Over in z/OSMF, one would see:</p>
<p><img alt="zosmf-stopped" src="../images/zosmf-stopped.png" /></p>
</li>
<li>
<p>Once you see in the Action History that the stop has completed, <strong>go back to the tab where the timestamp application was</strong> (<code>192.168.176.154:9XXX</code>, if you accidentally closed it).</p>
</li>
<li>
<p><strong>Reload this page</strong>.</p>
<p>You will see the following:</p>
<p><img alt="wlp-stopped" src="../images/wlp-stopped.png" /></p>
</li>
<li>
<p><strong>Go back to the Dashboard tab and click the “Run” button that’s associated with “Start”</strong>.</p>
<p><img alt="start-wlp" src="../images/start-wlp.png" /></p>
<p>This will trigger a workflow over in z/OSMF to start the server.</p>
</li>
<li>
<p><strong>Click “Action History” and refresh until you see that the Start workflow is complete</strong>.</p>
<p><img alt="start-complete" src="../images/start-complete.png" /></p>
</li>
<li>
<p><strong>Go back to the tab with the timestamp application and reload the page</strong>.</p>
<p>You should see the time and date with the current time shown:</p>
<p><img alt="wlp-restarted" src="../images/wlp-restarted.png" /></p>
<p>This indicates that the server is back up and serving pages</p>
</li>
</ol></section><section class="print-page" id="lab003-lab003-5"><h1 id="lab003-lab003-5-cleaning-up">Cleaning Up<a class="headerlink" href="#lab003-lab003-5-cleaning-up" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p><strong>Close the tab with the timestamp application</strong>.</p>
</li>
<li>
<p><strong>Close the Dashboard tab</strong>.</p>
</li>
<li>
<p><strong>Navigate back to your userNN-wlp instance</strong></p>
<details class="hint" open="open"><summary>Hint</summary><p>Administrator -&gt; Operators -&gt; Installed Operators -&gt; Liberty for z/OS -&gt; Liberty for z/OS tab</p>
</details>
</li>
<li>
<p><strong>Click the three dots to the far right of your provisioned service and click Delete WLP</strong>.</p>
<p><img alt="delete-wlp" src="../images/delete-wlp.png" /></p>
<p>Over in z/OSMF, that will trigger a de-provision operation:</p>
<p><img alt="deprovision-wlp" src="../images/deprovision-wlp.png" /></p>
</li>
</ol>
<p>When the operation is complete, you will see</p>
<p><img alt="no-operands" src="../images/no-operands.png" /></p>
<p>On z/OSMF, the Liberty z/OS server instance has been de-provisioned, which means it was stopped and the file system location for the server instance removed.</p></section><h1 class='nav-section-title-end'>Ended: Using the z/OS Cloud Broker</h1>
                        <h2 class='nav-section-title' id='section-deploying-an-application-with-the-open-liberty-operator'>
                            Deploying an Application with the Open Liberty Operator <a class='headerlink' href='#section-deploying-an-application-with-the-open-liberty-operator' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab007-lab007-1"><h1 id="lab007-lab007-1-deploying-an-application-with-the-open-liberty-operator">Deploying an Application with the Open Liberty Operator<a class="headerlink" href="#lab007-lab007-1-deploying-an-application-with-the-open-liberty-operator" title="Permanent link">&para;</a></h1>
<p><img alt="open-liberty-logo" src="../images/open-liberty-logo.png" /></p>
<p>Note: this lab is a modified version of the GitHub repository here:</p>
<p><a href="https://github.com/OpenShift-Z/openliberty-operator-ocpz">https://github.com/OpenShift-Z/openliberty-operator-ocpz</a></p>
<p>Open Liberty:</p>
<ul>
<li>is a lightweight, open framework for building fast and efficient cloud-native Java microservices</li>
<li>is fast to start up with low memory footprint and live reload for quick iteration.</li>
<li>is simple to add and remove features from the latest versions of MicroProfile and Java EE.</li>
<li>requires zero migration lets you focus on what's important, not the APIs changing under you.</li>
</ul>
<p>The <a href="https://github.com/OpenLiberty/open-liberty-operator">Open Liberty Operator</a> can be used to deploy and manage <a href="https://github.com/OpenLiberty">Open Liberty</a> applications into OpenShift clusters. You can also perform Day-2 operations such as gathering traces and dumps using the operator.</p>
<p>Because the Open Liberty Operator watches all namespaces in the OpenShift cluster, workshop users are not required to deploy the Operator itself. It has already been deployed in the openshift-operators project.</p></section><section class="print-page" id="lab007-lab007-2"><h1 id="lab007-lab007-2-log-into-openshift-using-the-cli">Log into OpenShift Using the CLI<a class="headerlink" href="#lab007-lab007-2-log-into-openshift-using-the-cli" title="Permanent link">&para;</a></h1>
<p>In this section, you will be connecting to a “Linux Guest” server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don’t have to install it on your RHEL VM terminal.</p>
<ol>
<li>
<p><strong>Open a Terminal session</strong></p>
</li>
<li>
<p><strong>ssh into the Linux Guest server</strong>:</p>
<div class="highlight"><pre><span></span><code>ssh userNN@192.168.176.61
</code></pre></div>
<p>Where <code>NN</code> is your user number.</p>
</li>
<li>
<p>When prompted, <strong>enter your password:</strong> <code>p@ssw0rd</code> <strong>and hit enter</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><p><img alt="ascii-wsc.png" src="../images/ascii-wsc.png" /></p>
</details>
</li>
<li>
<p>In Firefox, <strong>navigate to the following URL</strong> to request an API token:</p>
<p><a href="https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request">https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</a></p>
</li>
<li>
<p><strong>Enter your OpenShift credentials when prompted</strong>.</p>
<ul>
<li>
<p>Username: <code>userNN</code></p>
</li>
<li>
<p>Password: <code>p@ssw0rd</code></p>
</li>
</ul>
</li>
<li>
<p><strong>Click the “Display Token” hyperlink</strong>.</p>
<p><img alt="display-token.png" src="../images/display-token.png" /></p>
</li>
<li>
<p><strong>Copy the contents of the first text box</strong> beginning with <code>oc login</code> and ending with <code>6443</code>.</p>
<p><img alt="oc-login-token.png" src="../images/oc-login-token.png" /></p>
</li>
<li>
<p><strong>Paste this command back in your terminal session and press enter</strong>.</p>
<div class="highlight"><pre><span></span><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you’re prompted to use an insecure connection, type Y and hit enter.</p>
</div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443
Logged into &quot;https://api.atsocppa.dmz:6443&quot; as &quot;user01&quot; using the token provided.

You have access to 161 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;user01-project&quot;.
</code></pre></div>
</details>
<p>You are now logged into the cluster via the command line, and you are told which project you are using.</p>
<p>If you’re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p>
</li>
</ol></section><section class="print-page" id="lab007-lab007-3"><h1 id="lab007-lab007-3-cloning-the-github-repository-and-reviewing-its-contents">Cloning the GitHub Repository and Reviewing its Contents<a class="headerlink" href="#lab007-lab007-3-cloning-the-github-repository-and-reviewing-its-contents" title="Permanent link">&para;</a></h1>
<p>In the terminal session, you should have been automatically placed in your home directory <code>/home/userNN</code> (where NN is your user number).</p>
<ol>
<li>
<p><strong>Run the command</strong> <code>pwd</code> <strong>to check your current working directory</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ pwd
/home/user01
</code></pre></div>
</details>
</li>
<li>
<p>If you are in any other directory, <strong>change into the correct home directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd /home/userNN
</code></pre></div>
<p>Where <code>NN</code> is your user number.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ cd /home/user01
user01@lab061:~$ pwd
/home/user01
</code></pre></div>
</details>
</li>
<li>
<p>In your home directory, <strong>clone the Open Liberty Operator repository using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>git clone https://github.com/mmondics/openliberty-operator-ocpz
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ git clone https://github.com/mmondics/openliberty-operator-ocpz
Cloning into &#39;openliberty-operator-ocpz&#39;...
remote: Enumerating objects: 70, done.
remote: Counting objects: 100% (70/70), done.
remote: Compressing objects: 100% (68/68), done.
remote: Total 70 (delta 30), reused 2 (delta 1), pack-reused 0
Unpacking objects: 100% (70/70), done.
Checking connectivity... done.
</code></pre></div>
</details>
</li>
<li>
<p>This will create a new directory called <code>openliberty-operator-ocpz</code>. <strong>Change into this directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd openliberty-operator-ocpz
</code></pre></div>
</li>
<li>
<p><strong>Then list its contents using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>ls -l
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ cd openliberty-operator-ocpz
user01@lab061:~/openliberty-operator-ocpz$ ls -l
total 24
-rw-r--r-- 6 user01 users 4096 Sep  8 14:33 README.md
drwxr-xr-x 6 user01 users 4096 Sep  8 14:33 admin-ol-operator-install
drwxr-xr-x 6 user01 users 4096 Sep  8 14:33 images
drwxr-xr-x 6 user01 users 4096 Sep  8 14:33 ol-app-install
</code></pre></div>
</details>
<details class="information"><summary>Expand for More Information</summary><p>If you navigate to the GitHub in a web browser (<a href="https://github.com/mmondics/openliberty-operator-ocpz">https://github.com/mmondics/openliberty-operator-ocpz</a>), you will notice that the sub-directories in your Linux session reflect the folders contained in the repository.</p>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>README.md</td>
<td>Contains the content displayed on the GitHub page for this   repository. You can read through this README file if you want to get more   information about this lab.</td>
</tr>
<tr>
<td>admin-ol-operator-install</td>
<td>Directory used to install the Open Liberty Operator onto   the OpenShift cluster. Since this has been done ahead of time, you won’t be   using this directory.</td>
</tr>
<tr>
<td>images</td>
<td>Contains the images referenced in the README.md file and   displayed on the GitHub page for this repository.</td>
</tr>
<tr>
<td>ol-app-install</td>
<td>Contains all of the files needed to build, push, and   deploy the Mod Resorts sample application. This is where we will be doing our   work for this lab.</td>
</tr>
</tbody>
</table>
</details>
</li>
<li>
<p><strong>Change into the</strong> <code>ol-app-install</code> <strong>directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd ol-app-install
</code></pre></div>
</li>
<li>
<p><strong>List its contents using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>ls -l
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ cd ol-app-install
user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ls -l
total 8416
-rwxr--r—1 user01 users      845 Sep  8 14:33 1-build.sh
-rwxr--r—1 user01 users      367 Sep  8 14:33 2-deploy.sh
-rwxr--r—1 user01 users      612 Sep  8 14:33 3-cleanup.sh
-rwxr--r—1 user01 users      142 Sep  8 14:33 Dockerfile
-rwxr--r—1 user01 users      291 Sep  8 14:33 app-mod-withroute_cr.yaml
-rwxr--r—1 user01 users      636 Sep  8 14:33 env
-rwxr--r—1 user01 users   858364 Sep  8 14:33 modresorts-1.0.war
-rwxr--r—1 user01 users      687 Sep  8 14:33 server.xml
</code></pre></div>
</details>
<p>This directory contains 8 files that you will use to install the Mod Resorts sample application.</p>
<details class="information"><summary>Expand for More Information</summary><table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>1-build.sh</code></td>
<td>shell script that contains commands to log into the   OpenShift Cluster and the internal registry, create a new project, build the   container image and then push it into the registry.</td>
</tr>
<tr>
<td><code>2-deploy.sh</code></td>
<td>shell script that contains commands to create the   OpenLibertyApplication custom resource based off of the image build and   pushed by 1-build.sh.</td>
</tr>
<tr>
<td><code>3-cleanup.sh</code></td>
<td>shell script that will clean up the OpenLibertyApplication   created by the previous scripts and delete the new project created by 1-build.sh.</td>
</tr>
<tr>
<td><code>Dockerfile</code></td>
<td>referenced by 1-build.sh to build the container image.</td>
</tr>
<tr>
<td><code>app-mod-withroute_cr.yaml</code></td>
<td>referenced by 2-deploy.sh to create the   OpenLibertyApplication custom resource.</td>
</tr>
<tr>
<td><code>env</code></td>
<td>environment variables sourced by the shell scripts for   various commands.</td>
</tr>
<tr>
<td><code>modresorts-1.0.war</code></td>
<td>a collection of JAR-files, JavaServer Pages, Java   Servlets, Java classes, etc… that together constitute the sample Mod Resorts   application.</td>
</tr>
<tr>
<td><code>server.xml</code></td>
<td>used in conjunction with the .war file to create the web   application.</td>
</tr>
</tbody>
</table>
</details>
</li>
</ol></section><section class="print-page" id="lab007-lab007-4"><h1 id="lab007-lab007-4-using-the-open-liberty-operator-to-install-an-application">Using the Open Liberty Operator to Install an Application<a class="headerlink" href="#lab007-lab007-4-using-the-open-liberty-operator-to-install-an-application" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p><strong>Edit the environment variables file to match your user number (NN)</strong>.</p>
<div class="highlight"><pre><span></span><code>sed -i &#39;s/NN/YOUR_USER_NUMBER/g&#39; env
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Make sure that you replace YOUR_USER_NUMBER in the command above.</p>
</div>
</li>
<li>
<p><strong>Run the command</strong> <code>cat env</code> <strong>to check that the two instances of</strong> <code>NN</code> <strong>were properly replaced with your user number</strong>.</p>
<p>With your modified environment variables file env, you’re ready to run the shell script <code>1-build.sh</code> that will use <em>Buildah</em> to build a container image from the Dockerfile, and <em>Podman</em> to push the image into OpenShift’s internal registry.</p>
<p>Before running this script, take a look at the steps it will go through.</p>
</li>
<li>
<p><strong>View the script contents using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat 1-build.sh
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>#!/bin/text

unset KUBECONFIG

. ./env

echo &quot;Logging into OpenShift&quot;
oc login $OPENSHIFT_API_URL \
    --username=$OPENSHIFT_USERNAME \
    --password=$OPENSHIFT_PASSWORD \
    --insecure-skip-tls-verify=true

echo &quot;Logging into OpenShift image registry&quot;
podman login \
    --username $OPENSHIFT_USERNAME \
    --password $(oc whoami -t) \
    --tls-verify=false \
    $OPENSHIFT_REGISTRY_URL

echo &quot;Switch to $OPENSHIFT_PROJECT&quot;
oc project $OPENSHIFT_PROJECT

echo &quot;Building the container image&quot;
buildah build-using-dockerfile \
-t ${OPENSHIFT_REGISTRY_URL}/$OPENSHIFT_PROJECT/app-modernization:v1.0.0 \
    .

echo &quot;Pushing the container image to the OpenShift image registry&quot;
podman push --tls-verify=false \
${OPENSHIFT_REGISTRY_URL}/${OPENSHIFT_PROJECT}/app-modernization:v1.0.0
</code></pre></div>
</details>
<p>This shell script:</p>
<ul>
<li>Logs you into the OpenShift cluster.</li>
<li>Logs you into the OpenShift cluster’s internal image registry.</li>
<li>Switches to your project, if not currently working in it.</li>
<li>Builds the container image using the Dockerfile contained in your working directory.</li>
<li>Pushes the new container image from step 4 to the cluster’s internal image registry.</li>
</ul>
<details class="note" open="open"><summary>Note</summary><p>Note that you are welcome to enter each command manually and individually, but the scripting is there to minimize the opportunity for typos and other errors. If you do enter each command manually, make sure to replace each variable in BLUE with the actual value itself. Also, notice that the forward slash  simply breaks a single command into multiple lines.</p>
</details>
</li>
<li>
<p>You might notice that the <code>Dockerfile</code> is doing the brunt of the work in this script to build the container image itself. <strong>Take a look at this too, using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat Dockerfile
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code><span class="k">FROM</span> <span class="s">quay.io/mmondics/open-liberty:latest</span>
<span class="k">COPY</span> --chown<span class="o">=</span><span class="m">1001</span>:0 modresorts-1.0.war /config/dropins
<span class="k">COPY</span> --chown<span class="o">=</span><span class="m">1001</span>:0 server.xml /config/
</code></pre></div>
</details>
<p>This <code>Dockerfile</code> pulls the Open Liberty Java EE 8 image from Quay.io then adds the <code>modresorts-1.0.war</code> binary and <code>server.xml</code> configuration file to the base image.</p>
<p>Back in the <code>ol-app-install</code> working directory, you can now run the script that brings all of these pieces together.</p>
</li>
<li>
<p><strong>Run the script using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>./1-build.sh
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./1-build.sh
Logging into Openshift
Login successful.

You have access to 170 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;user01-project&quot;.
Logging into Openshift image registry
Login Succeeded!
Switch to user01-project
Already on project &quot;user01-project&quot; on server &quot;https://api.atsocppa.dmz:6443&quot;.
Building the container image
STEP 1: FROM quay.io/mmondics/open-liberty:javaee8-ubi-min
STEP 2: COPY --chown=1001:0 modresorts-1.0.war /config/dropins
STEP 3: COPY --chown=1001:0 server.xml /config/
STEP 4: COMMIT default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization:v1.0.0
Getting image source signatures
Copying blob d20db2e30c33 skipped: already exists
Copying blob 4ec5f0a55d74 skipped: already exists

... cut from screenshot ...

Copying config c6e6f3bf6b done
Writing manifest to image destination
Copying config c6e6f3bf6b done
Writing manifest to image destination
Storing signatures
user01@lab061:~/openliberty-operator-ocpz/ol-app-install$
</code></pre></div>
</details>
<p>Your container image is now built and pushed into OpenShift’s internal registry.</p>
</li>
<li>
<p><strong>View your new image in the registry using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>podman images
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ podman images
REPOSITORY                                                                                 TAG     IMAGE ID       CREATED          SIZE
default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization  v1.0.0  5970fc63cb46   16 seconds ago   472 MB
</code></pre></div>
</details>
<p>With the app-modernization image in the OpenShift internal registry, it can now be used to deploy an application into the cluster.</p>
<p>You have one more file to edit before deploying the Mod Resorts sample application.</p>
</li>
<li>
<p><strong>Edit the custom resource file using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>sed -i &#39;s/NN/YOUR_USER_NUMBER/g&#39; app-mod-withroute_cr.yaml
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Make sure that you replace YOUR_USER_NUMBER in the command above.</p>
</div>
</li>
<li>
<p><strong>Run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat app-mod-withroute_cr.yaml
</code></pre></div>
<p>And make sure that the two instances of <code>NN</code> were properly replaced with your user number.</p>
<details class="information"><summary>Expand for More Information</summary><p>What exactly is this YAML file?</p>
<p>A <em>Custom Resource Definition (CRD)</em> object defines a new, unique object in the cluster and lets the Kubernetes API server handle its entire lifecycle. <em>Custom Resource (CR)</em> objects are created from CRDs that have been added to the cluster by a cluster administrator, allowing all cluster users to add the new resource type into projects.</p>
<p>So in this case, a CRD was created ahead of time of the kind: OpenLibertyApplication, and you are creating a CR from that CRD. </p>
</details>
<p>With the modified app-mod-withroute_cr.yaml file, you’re ready to run the second script, <code>2-deploy.sh</code>. Before running it, take a look at what all it will do.</p>
</li>
<li>
<p><strong>Run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat 2-deploy.sh
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>#!/bin/text

unset KUBECONFIG

. ./env

echo &quot;Logging into OpenShift&quot;
oc login $OPENSHIFT_API_URL \
    --username=$OPENSHIFT_USERNAME \
    --password=$OPENSHIFT_PASSWORD \
    --insecure-skip-tls-verify=true

echo &quot;Creating OpenLiberty Custom Resource&quot;
oc -n $OPENSHIFT_PROJECT create -f app-mod-withroute_cr.yaml
</code></pre></div>
</details>
<p>This shell script:</p>
<ul>
<li>Logs you into the OpenShift cluster.</li>
<li>Creates an object from the Custom Resource (CR) YAML file you just edited.</li>
</ul>
</li>
<li>
<p><strong>Run this shell script with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>./2-deploy.sh
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./2-deploy.sh
Logging into Openshift
Login successful.

You have access to 170 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;user01-project&quot;.
Creating Openliberty Custom Resource Definition
openlibertyapplication.openliberty.io/appmod created
</code></pre></div>
</details>
<p>Your Custom Resource named <code>appmod</code> of kind <code>OpenLibertyApplication</code> has been <code>created</code> in your project, <code>userNN-project</code>.  The creation of this CR resulted in the Open Liberty Operator deploying a pod, deployment, replicaset, and a service that is exposed as a route.</p>
</li>
<li>
<p><strong>View all of the created objects using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get all
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ oc get all
NAME                          READY   STATUS    RESTARTS   AGE
pod/appmod-5959fb64b5-fqkvg   1/1     Running   0          16s

NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/appmod   ClusterIP   172.30.26.44   &lt;none&gt;        9080/TCP   17s

NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/appmod   1/1     1            1           16s

NAME                                DESIRED   CURRENT   READY   AGE
replicaset.apps/appmod-5959fb64b5   1         1         1       16s

NAME                                           IMAGE REPOSITORY                    TAGS     UPDATED
imagestream.image.openshift.io/app-modernization   default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/app-modernization   v1.0.0   29 seconds ago

NAME                              HOST/PORT                 PATH  SERVICES   PORT  TERMINATION WILDCARD
route.route.openshift.io/appmod   modresort-user01.apps.atsocppa.dmz   /resorts   appmod     9080-tcp     None
</code></pre></div>
</details>
</li>
</ol></section><section class="print-page" id="lab007-lab007-5"><h1 id="lab007-lab007-5-access-the-application-in-a-browser">Access the Application in a Browser<a class="headerlink" href="#lab007-lab007-5-access-the-application-in-a-browser" title="Permanent link">&para;</a></h1>
<p>Your application is running and accessible via the exposed route.</p>
<ol>
<li>
<p><strong>In a web browser, navigate to the route</strong>:</p>
<p><a href="http://modresort-userNN.apps.atsocppa.dmz/resorts/">http://modresort-userNN.apps.atsocppa.dmz/resorts/</a></p>
<p>Where NN is your user number.</p>
<p><img alt="mod-resorts" src="../images/mod-resorts.png" /></p>
</li>
</ol>
<p>The demo application used (mod resorts) is admittedly a simple use-case with no dependencies on external resources, so you will notice that most of the links do not function. In real-world applications, there will be dependencies on external resources which can be integrated using various OpenShift and Kubernetes objects such as ConfigMaps, Volume mounts, secrets, etc.</p></section><section class="print-page" id="lab007-lab007-6"><h1 id="lab007-lab007-6-cleaning-up">Cleaning Up<a class="headerlink" href="#lab007-lab007-6-cleaning-up" title="Permanent link">&para;</a></h1>
<p>When you’re ready to wrap up this lab, return to your terminal to run the last shell script.</p>
<ol>
<li>
<p><strong>Run the cleanup script with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>./3-cleanup.sh
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openliberty-operator-ocpz/ol-app-install$ ./3-cleanup.sh
Logging into OpenShift
Login successful.

You have access to 169 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;user01-project&quot;.
Logging into OpenShift image registry
Login Succeeded!
Deleting Openliberty app
openlibertyapplication.openliberty.io &quot;appmod&quot; deleted
Deleting imagestream
imagestream.image.openshift.io &quot;app-modernization&quot; deleted
</code></pre></div>
</details>
</li>
</ol>
<p>Your OpenLiberty app consisting of a pod, deployment, service, route, and the imagestream used to create the pod have all been deleted.</p></section><h1 class='nav-section-title-end'>Ended: Deploying an Application with the Open Liberty Operator</h1>
                        <h2 class='nav-section-title' id='section-deploying-an-application-with-quarkus-red-hat-runtime'>
                            Deploying an Application with Quarkus Red Hat Runtime <a class='headerlink' href='#section-deploying-an-application-with-quarkus-red-hat-runtime' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="lab008-lab008-1"><h1 id="lab008-lab008-1-deploying-an-application-with-quarkus-red-hat-runtime">Deploying an Application with Quarkus Red Hat Runtime<a class="headerlink" href="#lab008-lab008-1-deploying-an-application-with-quarkus-red-hat-runtime" title="Permanent link">&para;</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more Quarkus guides, see the official site here: <a href="https://quarkus.io/guides/">https://quarkus.io/guides/</a></p>
</div>
<p><img alt="quarkus-logo" src="../images/quarkus-logo.png" /></p>
<p><strong><em>Quarkus</em></strong> is a full-stack, Kubernetes-native Java framework optimized specifically for containers and Kubernetes environments. It is designed to work with popular Java standards, frameworks, and libraries like Eclipse MicroProfile, Spring Apache Kafka, RESTEasy, and many more.</p>
<p>Quarkus was designed for developers with the intent to be <em>easy to use</em> with features that work well with <em>little to no configuration</em>. It includes many features for developers, such as live coding so you can immediately check the effect of code changes and quickly troubleshoot them.</p>
<p>Quarkus was built around a <em>container-first</em> philosophy, meaning it’s optimized for lower memory usage and faster startup times. Quarkus builds applications to consume 1/10<sup>th</sup> the memory when compared to traditional Java, and has a much faster startup time (as much as 300 times faster), both of which greatly reduce the cost of cloud resources.</p>
<p>In this lab, you will explore these features by:</p>
<ul>
<li>Creating a new Quarkus project</li>
<li>Configuring the Quarkus application for OpenShift</li>
<li>Deploying the Quarkus application to OpenShift</li>
</ul></section><section class="print-page" id="lab008-lab008-2"><h1 id="lab008-lab008-2-log-into-openshift-using-the-cli">Log into OpenShift Using the ClI<a class="headerlink" href="#lab008-lab008-2-log-into-openshift-using-the-cli" title="Permanent link">&para;</a></h1>
<p>In this section, you will be connecting to a “Linux Guest” server which has a few things set up to make your life a little easier. Most notably, it has the OpenShift command line <code>oc</code> installed, so you don’t have to install it on your RHEL VM terminal.</p>
<ol>
<li>
<p><strong>Open a Terminal session</strong></p>
</li>
<li>
<p><strong>ssh into the Linux Guest server</strong>:</p>
<div class="highlight"><pre><span></span><code>ssh userNN@192.168.176.61
</code></pre></div>
<p>Where <code>NN</code> is your user number.</p>
</li>
<li>
<p>When prompted, <strong>enter your password:</strong> <code>p@ssw0rd</code> <strong>and hit enter</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><p><img alt="ascii-wsc.png" src="../images/ascii-wsc.png" /></p>
</details>
</li>
<li>
<p>In Firefox, <strong>navigate to the following URL</strong> to request an API token:</p>
<p><a href="https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request">https://oauth-openshift.apps.atsocppa.dmz/oauth/token/request</a></p>
</li>
<li>
<p><strong>Enter your OpenShift credentials when prompted</strong>.</p>
<ul>
<li>
<p>Username: <code>userNN</code></p>
</li>
<li>
<p>Password: <code>p@ssw0rd</code></p>
</li>
</ul>
</li>
<li>
<p><strong>Click the “Display Token” hyperlink</strong>.</p>
<p><img alt="display-token.png" src="../images/display-token.png" /></p>
</li>
<li>
<p><strong>Copy the contents of the first text box</strong> beginning with “oc login” and ending with “6443”.</p>
<p><img alt="oc-login-token.png" src="../images/oc-login-token.png" /></p>
</li>
<li>
<p><strong>Paste this command back in your terminal session and press enter</strong>.</p>
<div class="highlight"><pre><span></span><code>oc login --token=&lt;YOUR_TOKEN_HERE&gt; --server=https://api.atsocppa.dmz:6443
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you’re prompted to use an insecure connection, type Y and hit enter.</p>
</div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ oc login --token=uL3fHEPSGH3io0htdGRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.atsocppa.dmz:6443
Logged into &quot;https://api.atsocppa.dmz:6443&quot; as &quot;user01&quot; using the token provided.

You have access to 161 projects, the list has been suppressed. You can list all projects with &#39;oc projects&#39;

Using project &quot;user01-project&quot;.
</code></pre></div>
</details>
<p>You are now logged into the cluster via the command line, and you are told which project you are using.</p>
<p>If you’re in a project other than userNN-project, use the following command to move into it: <code>oc project userNN-project</code>, where NN is your user number.</p>
</li>
</ol></section><section class="print-page" id="lab008-lab008-3"><h1 id="lab008-lab008-3-creating-and-reviewing-the-quarkus-project">Creating and Reviewing the Quarkus Project<a class="headerlink" href="#lab008-lab008-3-creating-and-reviewing-the-quarkus-project" title="Permanent link">&para;</a></h1>
<p>In the terminal session, you should have been automatically placed in your home directory <code>/home/userNN</code> (where <code>NN</code> is your user number).</p>
<ol>
<li>
<p><strong>Run the command</strong> <code>pwd</code> <strong>to check your current working directory</strong>.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ pwd
/home/user01
</code></pre></div>
</details>
</li>
<li>
<p>If you are in any other directory, <strong>change into the correct home directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd /home/userNN
</code></pre></div>
<p>Where NN is your user number.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ cd /home/user01
user01@lab061:~$ pwd
/home/user01
</code></pre></div>
</details>
<p>We will start off by creating a Maven Project. <em>Maven</em> is a powerful project management tool based on POM (Project Object Model). It is a tool used by Java developers to simplify and add structure to their day-to-day work by implementing dependency and documentation into their Java applications.</p>
<p>You can read more about Maven on their official site here: <a href="https://maven.apache.org/index.html">https://maven.apache.org/index.html</a>.</p>
<p>The following command uses the <em>Maven Quarkus Plugin</em> to create a basic Maven project for you in the <code>openshift-quickstart</code> subdirectory. It generates:</p>
<ul>
<li>
<p>The Maven structure including the <code>pom.xml</code></p>
</li>
<li>
<p>An org.acme.rest.GreetingResource resource exposed on <code>/greeting</code></p>
</li>
<li>
<p>An associated unit test</p>
</li>
<li>
<p>A landing page that is accessible on <a href="http://localhost:8080">http://localhost:8080</a> after starting the application</p>
</li>
<li>
<p>Example Dockerfiles for both native and jvm modes</p>
</li>
<li>
<p>The application configuration file</p>
</li>
</ul>
<p>Note that the forward slash  simply breaks the command into multiple lines for readability. Also note that if you do not specify the variables for projectGroupId, projectArtifactId, etc., the Maven installer will prompt you for them.</p>
</li>
<li>
<p><strong>In your home directory, run the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>mvn io.quarkus:quarkus-maven-plugin:1.8.3.Final:create \
-DprojectGroupId=org.acme \
-DprojectArtifactId=openshift-quickstart \
-DclassName=&quot;org.acme.rest.GreetingResource&quot; \
-Dpath=&quot;/greeting&quot; \
-Dextensions=&quot;resteasy&quot;
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~$ user01@lab061:~$ mvn io.quarkus:quarkus-maven-plugin:1.8.3.Final:create \
&gt;     -DprojectGroupId=org.acme \
&gt;     -DprojectArtifactId=openshift-quickstart \
&gt;     -DclassName=&quot;org.acme.rest.GreetingResource&quot; \
&gt;     -Dpath=&quot;/greeting&quot; \
&gt;     -Dextensions=”resteasy”

[INFO] Scanning for projects...
[INFO] 
[INFO] ------------------&lt; org.apache.maven:standalone-pom &gt;-------------------
[INFO] Building Maven Stub Project (No POM) 1
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- quarkus-maven-plugin:1.8.3.Final:create (default-cli) @ standalone-pom ---
[INFO] 
[INFO] Maven Wrapper version 0.5.6 has been successfully set up for your project.
[INFO] Using Apache Maven: 3.6.3
[INFO] Repo URL in properties file: https://repo.maven.apache.org/maven2
[INFO] 
[INFO] ========================================================================================
[INFO] Your new application has been created in /home/user01/openshift-quickstart
[INFO] Navigate into this directory and launch your application with mvn quarkus:dev
[INFO] Your application will be accessible on http://localhost:8080
[INFO] ========================================================================================
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  12.644 s
[INFO] Finished at: 2020-10-19T11:35:57-04:00
[INFO] ------------------------------------------------------------------------
</code></pre></div>
</details>
</li>
<li>
<p>As the installation says, an application has been created under the <code>openshift-quickstart</code> sub-directory. <strong>Change into this directory using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cd openshift-quickstart
</code></pre></div>
</li>
<li>
<p><strong>Then view its structure using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>tree
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>    user01@lab061:~$ cd openshift-quickstart
    user01@lab061:~/openshift-quickstart$ tree
    total 24
    .
    |-- README.md
    |-- mvnw
    |-- mvnw.cmd
    |-- pom.xml
    `-- src
        |-- main
        |   |-- docker
        |   |   |-- Dockerfile.fast-jar
        |   |   |-- Dockerfile.jvm
        |   |   `-- Dockerfile.native
        |   |-- java
        |   |   `-- org
        |   |       `-- acme
        |   |           `-- rest
        |   |               `-- GreetingResource.java
        |   `-- resources
        |       |-- META-INF
        |       |   `-- resources
        |       |       `-- index.html
        |       `-- application.properties
        `-- test
            `-- java
                `-- org
                    `-- acme
                        `-- rest
                            |-- GreetingResourceTest.java
                            `-- NativeGreetingResourceIT.java

    15 directories, 12 files
</code></pre></div>
</details>
<p>You can see that the command created the <code>pom.xml</code> file, Dockerfiles for both JVM and native modes, your GreetingResource.java file exposed at <code>/greeting</code>, and the associated test resources.</p>
</li>
<li>
<p>Let’s take a look at the <code>pom.xml</code> file that was created as a part of the installation. <strong>View the file using the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat pom.xml
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-quickstart$ cat pom.xml

...omitted...
<span class="nt">&lt;dependencyManagement&gt;</span>
    <span class="nt">&lt;dependencies&gt;</span>
    <span class="nt">&lt;dependency&gt;</span>
        <span class="nt">&lt;groupId&gt;</span>${quarkus.platform.group-id}<span class="nt">&lt;/groupId&gt;</span>
        <span class="nt">&lt;artifactId&gt;</span>${quarkus.platform.artifact-id}<span class="nt">&lt;/artifactId&gt;</span>
        <span class="nt">&lt;version&gt;</span>${quarkus.platform.version}<span class="nt">&lt;/version&gt;</span>
        <span class="nt">&lt;type&gt;</span>pom<span class="nt">&lt;/type&gt;</span>
        <span class="nt">&lt;scope&gt;</span>import<span class="nt">&lt;/scope&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
    <span class="nt">&lt;/dependencies&gt;</span>
<span class="nt">&lt;/dependencyManagement&gt;</span>
...omitted...

<span class="nt">&lt;build&gt;</span>
    <span class="nt">&lt;plugins&gt;</span>
    <span class="nt">&lt;plugin&gt;</span>
        <span class="nt">&lt;groupId&gt;</span>io.quarkus<span class="nt">&lt;/groupId&gt;</span>
        <span class="nt">&lt;artifactId&gt;</span>quarkus-maven-plugin<span class="nt">&lt;/artifactId&gt;</span>
        <span class="nt">&lt;version&gt;</span>${quarkus-plugin.version}<span class="nt">&lt;/version&gt;</span>
        <span class="nt">&lt;executions&gt;</span>
        <span class="nt">&lt;execution&gt;</span>
            <span class="nt">&lt;goals&gt;</span>
            <span class="nt">&lt;goal&gt;</span>generate-code<span class="nt">&lt;/goal&gt;</span>
            <span class="nt">&lt;goal&gt;</span>generate-code-tests<span class="nt">&lt;/goal&gt;</span>
            <span class="nt">&lt;goal&gt;</span>build<span class="nt">&lt;/goal&gt;</span>
            <span class="nt">&lt;/goals&gt;</span>
        <span class="nt">&lt;/execution&gt;</span>
        <span class="nt">&lt;/executions&gt;</span>
    <span class="nt">&lt;/plugin&gt;</span>
    <span class="nt">&lt;plugin&gt;</span>
        <span class="nt">&lt;artifactId&gt;</span>maven-compiler-plugin<span class="nt">&lt;/artifactId&gt;</span>
        <span class="nt">&lt;version&gt;</span>${compiler-plugin.version}<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/plugin&gt;</span>
    <span class="nt">&lt;plugin&gt;</span>
        <span class="nt">&lt;artifactId&gt;</span>maven-surefire-plugin<span class="nt">&lt;/artifactId&gt;</span>
        <span class="nt">&lt;version&gt;</span>${surefire-plugin.version}<span class="nt">&lt;/version&gt;</span>
        <span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;systemPropertyVariables&gt;</span>
            <span class="nt">&lt;java.util.logging.manager&gt;</span>org.jboss.logmanager.LogManager<span class="nt">&lt;/java.util.logging.manager&gt;</span>
            <span class="nt">&lt;maven.home&gt;</span>${maven.home}<span class="nt">&lt;/maven.home&gt;</span>
        <span class="nt">&lt;/systemPropertyVariables&gt;</span>
        <span class="nt">&lt;/configuration&gt;</span>
    <span class="nt">&lt;/plugin&gt;</span>
    <span class="nt">&lt;/plugins&gt;</span>
<span class="nt">&lt;/build&gt;</span>
</code></pre></div>
</details>
<p>The snippets above show the import of the Quarkus BOM, which allows you to omit the version on the different Quarkus dependencies. In addition, you can see the quarkus-maven-plugin responsible for the packaging of the application and providing the development mode.</p>
<p>Next look at the dependencies section.</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code><span class="nt">&lt;dependencies&gt;</span>
    <span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>io.quarkus<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>quarkus-resteasy<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
    <span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>io.quarkus<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>quarkus-junit5<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
    <span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>io.rest-assured<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>rest-assured<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;scope&gt;</span>test<span class="nt">&lt;/scope&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
</code></pre></div>
</details>
<p>You can see we are using <a href="https://code.quarkus.io/">Quarkus extensions</a> which allow the development and testing of REST applications:</p>
<p>During the installation, the <code>openshift-quickstart/src/main/java/org/acme/rest/GreetingResource.java</code>
file was created. This is a simple REST endpoint, returning “hello” to requests at <code>/greeting</code>.</p>
</li>
<li>
<p><strong>From the</strong> <code>openshift-quickstart</code> <strong>directory, view this file with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>cat src/main/java/org/acme/rest/GreetingResource.java
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code><span class="kn">package</span> <span class="nn">org.acme.rest</span><span class="p">;</span>

<span class="kn">import</span> <span class="nn">javax.ws.rs.GET</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">javax.ws.rs.Path</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">javax.ws.rs.Produces</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">javax.ws.rs.core.MediaType</span><span class="p">;</span>

<span class="nd">@Path</span><span class="p">(</span><span class="s">&quot;/greeting&quot;</span><span class="p">)</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">GreetingResource</span> <span class="p">{</span>

    <span class="nd">@GET</span>
    <span class="nd">@Produces</span><span class="p">(</span><span class="n">MediaType</span><span class="p">.</span><span class="na">TEXT_PLAIN</span><span class="p">)</span>
    <span class="kd">public</span> <span class="n">String</span> <span class="nf">hello</span><span class="p">()</span> <span class="p">{</span>
        <span class="k">return</span> <span class="s">&quot;hello&quot;</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<p>You can see that this file is simply telling the application to return “hello” at the <code>/greeting</code> path.</p>
</li>
</ol></section><section class="print-page" id="lab008-lab008-4"><h1 id="lab008-lab008-4-configure-the-application-for-openshift">Configure the Application for OpenShift<a class="headerlink" href="#lab008-lab008-4-configure-the-application-for-openshift" title="Permanent link">&para;</a></h1>
<p>One of the great things about Quarkus is the plethora of <em>extensions</em> it provides out of the box. Quarkus extensions are comparable to Maven dependencies that allow for much easier use and integration into 3<sup>rd</sup> party projects.</p>
<p>We will be using the <em>Quarkus OpenShift extension</em>. The OpenShift extension is actually a wrapper that brings together the <a href="https://quarkus.io/guides/deploying-to-kubernetes">kubernetes</a> and <a href="https://quarkus.io/guides/container-image#s2i">container-image-s2i</a> extensions with defaults specific to OpenShift.</p>
<ol>
<li>
<p><strong>In your terminal session, add the OpenShift extension to you application with the command</strong>:</p>
<div class="highlight"><pre><span></span><code>./mvnw quarkus:add-extension -Dextensions=&quot;openshift&quot;
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-quickstart$ ./mvnw quarkus:add-extension -Dextensions=&quot;openshift&quot;
[INFO] Scanning for projects...
[INFO] 
[INFO] -------------------&lt; org.acme:openshift-quickstart &gt;--------------------
[INFO] Building openshift-quickstart 1.0-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- quarkus-maven-plugin:1.9.0.CR1:add-extension (default-cli) @ openshift-quickstart ---
? Extension io.quarkus:quarkus-openshift has been installed
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  11.356 s
[INFO] Finished at: 2020-10-23T15:42:02-04:00
[INFO] ------------------------------------------------------------------------
</code></pre></div>
</details>
<p>This command added the following dependency to your <code>pom.xml</code> file:</p>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>    <span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>io.quarkus<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>quarkus-openshift<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
</code></pre></div>
</details>
<p>This dependency is the generic quarkus.openshift extension, but it can be further customized with essentially any further nested dependency for OpenShift or Kubernetes objects you need. Some examples are in the table below, and the full list is here: <a href="https://quarkus.io/guides/kubernetes#openshift">https://quarkus.io/guides/kubernetes#openshift</a>.</p>
<details class="information"><summary>Expand for more Information</summary><table>
<thead>
<tr>
<th>Property</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>quarkus.openshift.version</td>
<td>String</td>
</tr>
<tr>
<td>quarkus.openshift.env-vars</td>
<td>Map<String, Env></td>
</tr>
<tr>
<td>quarkus.openshift.replicas</td>
<td>int</td>
</tr>
<tr>
<td>quarkus.openshift.service-account</td>
<td>String</td>
</tr>
<tr>
<td>quarkus.openshift.host</td>
<td>String</td>
</tr>
<tr>
<td>quarkus.openshift.ports</td>
<td>Map<String, Port></td>
</tr>
<tr>
<td>quarkus.openshift.pvc-volumes</td>
<td>Map<String, PersistentVolumeClaimVolume></td>
</tr>
<tr>
<td>quarkus.openshift.image-pull-policy</td>
<td>ImagePullPolicy</td>
</tr>
<tr>
<td>quarkus.openshift.image-pull-secrets</td>
<td>String[]</td>
</tr>
<tr>
<td>quarkus.openshift.liveness-probe</td>
<td>Probe</td>
</tr>
<tr>
<td>quarkus.openshift.readiness-probe</td>
<td>Probe</td>
</tr>
<tr>
<td>quarkus.openshift.expose</td>
<td>boolean</td>
</tr>
</tbody>
</table>
</details>
<p>You will be using a few of these customizations when you deploy the application to OpenShift in the next step.</p>
</li>
</ol></section><section class="print-page" id="lab008-lab008-5"><h1 id="lab008-lab008-5-deploy-the-application-onto-openshift">Deploy the Application onto OpenShift<a class="headerlink" href="#lab008-lab008-5-deploy-the-application-onto-openshift" title="Permanent link">&para;</a></h1>
<p>Let’s now take our local application and use the Quarkus extension we just added to build and deploy a containerized application onto OpenShift.</p>
<ol>
<li>
<p><strong>In the openshift-quickstart directory, run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>./mvnw clean package \
-Dquarkus.kubernetes.deploy=true \
-Dquarkus.kubernetes-client.trust-certs=true \
-Dquarkus.openshift.expose=true
</code></pre></div>
<p>The <code>-Dquarkus</code> flags in this command are telling Maven to deploy the application into the Kubernetes (OpenShift) cluster, trust the certificates, and expose the application service as a route, eliminating the need to run an <code>oc expose svc</code> to make the service endpoint accessible outside of the cluster.</p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>This command may take a few minutes to complete.</p>
</div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-quickstart$ ./mvnw clean package \
    -Dquarkus.kubernetes.deploy=true \
    -Dquarkus.kubernetes-client.trust-certs=true \
    -Dquarkus.openshift.expose=true
[INFO] Scanning for projects...
[INFO] 
[INFO] -------------------&lt; org.acme:openshift-quickstart &gt;--------------------
[INFO] Building openshift-quickstart 1.0.0-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
...omitted...

[INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] The deployed application can be accessed at: http://openshift-quickstart-user01-project.apps.atsocppa.dmz
[INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 94655ms
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  03:19 min
[INFO] Finished at: 2021-03-09T12:12:14-05:00
[INFO] ------------------------------------------------------------------------
</code></pre></div>
</details>
<p>The previous command builds a jar file locally, connects to the OpenShift cluster you previously logged into, triggers a container image build, pushes that container image into the OpenShift internal registry, generates OpenShift/Kubernetes resources including a Service, Route, DeploymentConfig, and your running application pod.</p>
</li>
<li>
<p><strong>View all of the created objects with the following command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get all
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061: ~/openshift-quickstart$ oc get all
NAME                                READY   STATUS      RESTARTS   AGE
pod/openshift-quickstart-1-build    0/1     Completed   0          9m10s
pod/openshift-quickstart-1-deploy   0/1     Completed   0          8m13s
pod/openshift-quickstart-1-ndp8h    1/1     Running     0          8m10s

NAME                                           DESIRED   CURRENT   READY   AGE
replicationcontroller/openshift-quickstart-1   1         1         1       8m13s

NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/openshift-quickstart   ClusterIP   172.30.28.241   &lt;none&gt;        8080/TCP   8m14s

NAME                                                  REVISION   DESIRED   CURRENT   TRIGGERED BY
deploymentconfig.apps.openshift.io/openshift-quickstart   1          1         1         image(openshift-quickstart:1.0-SNAPSHOT)

NAME                                                  TYPE     FROM     LATEST
buildconfig.build.openshift.io/openshift-quickstart   Source   Binary   1

NAME                                              TYPE     FROM     STATUS     STARTED         DURATION
build.build.openshift.io/openshift-quickstart-1   Source   Binary   Complete   9 minutes ago   55s

NAME                    IMAGE REPOSITORY                                 TAGS                                      UPDATED
imagestream.image.openshift.io/openjdk-11             default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/openjdk-11             1.3,1.3-3,1.3-3.1591609340 + 18 more...   9 minutes ago
imagestream.image.openshift.io/openshift-quickstart   default-route-openshift-image-registry.apps.atsocppa.dmz/user01-project/openshift-quickstart   1.0-SNAPSHOT                              8 minutes ago

NAME           HOST/PORT                        PATH      SERVICES               PORT   TERMINATION  
route.route.openshift.io/openshift-quickstart  openshift-quickstart-user01-project.apps.atsocppa.dmz   /      openshift-quickstart   8080
</code></pre></div>
</details>
<p>Each of these objects were created because of the <code>-Dquarkus.kubernetes.deploy=true</code> and <code>-Dquarkus.openshift.expose=true</code> flags provided in the previous command. There are many more OpenShift objects and object properties that can be created by passing different flags, such as liveliness probes, environment variables, secrets, persistent storage, and more.</p>
<p>If you have one running pod, your application has successfully deployed and is accessible at the route.</p>
</li>
<li>
<p><strong>In a web browser, navigate to your route</strong>:</p>
<details class="hint"><summary>Hint</summary><p>It will be <a href="http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/">http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/</a> where NN is your user number.</p>
</details>
<p><img alt="quarkus-main" src="../images/quarkus-main.png" /></p>
<p>Your Quarkus application is now deployed as a container in OpenShift.</p>
<p>Earlier we looked at the GreetingResource.java REST endpoint and its return of “hello” in the command line. We can do the same thing in the web browser.</p>
</li>
<li>
<p><strong>Add</strong> <code>/greeting</code> <strong>to the end of your route</strong>.</p>
<details class="hint"><summary>Hint</summary><p><a href="http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/greeting">http://openshift-quickstart-userNN-project.apps.atsocppa.dmz/greeting</a> where NN is your user number.</p>
</details>
<p><img alt="hello" src="../images/hello.png" /></p>
<p>In this lab, you have created a Quarkus application locally, containerized the application and deployed it onto an OpenShift cluster running on IBM Z, and accessed it from a public route.</p>
<p>The speed, agility, and ease with which we’re able to edit and redeploy applications using the Quarkus runtime creates tremendous value in time savings, allowing developers and operations staff to minimize downtime and keep applications up to date. Further, the simplicity of integration using Quarkus extensions creates great opportunity for customization and implementations tailored to fit a variety of needs.</p>
</li>
</ol></section><section class="print-page" id="lab008-lab008-6"><h1 id="lab008-lab008-6-cleaning-up">Cleaning Up<a class="headerlink" href="#lab008-lab008-6-cleaning-up" title="Permanent link">&para;</a></h1>
<ol>
<li>
<p><strong>Double check that you are in your own userNN-project by issuing the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc project
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061 ~/openshift-quickstart $ oc project
Using project &quot;user01-project&quot; on server &quot;https://api.atsocppa.dmz:6443&quot;.
</code></pre></div>
</details>
</li>
<li>
<p><strong>Once you’re sure you’re in your own project, issue the following command to delete all objects</strong> associated with your application labeled app.kubernetes.io/name=openshift-quickstart.  </p>
<div class="highlight"><pre><span></span><code>oc delete all --selector app.kubernetes.io/name=openshift-quickstart
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-quickstart $ oc delete all --selector app.kubernetes.io/name=openshift-quickstart
pod &quot;openshift-quickstart-1-ztfq9&quot; deleted
replicationcontroller &quot;openshift-quickstart-1&quot; deleted
service &quot;openshift-quickstart&quot; deleted
deploymentconfig.apps.openshift.io &quot;openshift-quickstart&quot; deleted
buildconfig.build.openshift.io &quot;openshift-quickstart&quot; deleted
build.build.openshift.io &quot;openshift-quickstart-1&quot; deleted
imagestream.image.openshift.io &quot;openjdk-11&quot; deleted
imagestream.image.openshift.io &quot;openshift-quickstart&quot; deleted
route.route.openshift.io &quot;openshift-quickstart&quot; deleted
</code></pre></div>
</details>
</li>
<li>
<p><strong>To check that all of your mongo application resources were deleted, run the command</strong>:</p>
<div class="highlight"><pre><span></span><code>oc get all
</code></pre></div>
<details class="example" open="open"><summary>Example Output</summary><div class="highlight"><pre><span></span><code>user01@lab061:~/openshift-quickstart $ oc get all
No resources found.
user00@lab061:~$
</code></pre></div>
</details>
</li>
</ol>
<details class="note"><summary>Note</summary><p>If there are leftover resources from other labs that you would like to delete, run the command:</p>
<div class="highlight"><pre><span></span><code>oc delete all --all 
</code></pre></div>
</details></section><h1 class='nav-section-title-end'>Ended: Deploying an Application with Quarkus Red Hat Runtime</h1><h1 class='nav-section-title-end'>Ended: Labs</h1></div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright © 2021 IBM Z Washington Systems Center
          </div>
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
    
  </body>
</html>